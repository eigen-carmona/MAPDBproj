{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e73b3fd",
   "metadata": {},
   "source": [
    "# Streaming with Kafka and Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827b85a1",
   "metadata": {},
   "source": [
    "Here I try to implement a basic pipeline for the project conecting kafka with spark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632ee96d",
   "metadata": {},
   "source": [
    "### Computer setting\n",
    "I downloaded and located in my home the spark file **spark-3.1.2-bin-hadoop3.2** and also the kafka file **kafka_2.13-2.7.0**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ea3a6e",
   "metadata": {},
   "source": [
    "## Get Kafka and Spark ready"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e149a414",
   "metadata": {},
   "source": [
    "### Standalone cluster deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88481fb2",
   "metadata": {},
   "source": [
    "We can now initialize all the required variables with `findspark.init()` by passing the path to the spark folder we downloaded previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39d3a5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/usr/local/spark/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f5eaee",
   "metadata": {},
   "source": [
    "First we need to start the master, This will spin up the spark master with address spark://localhost:7077 and a cluster dashboark at localhost:8080.\n",
    "\n",
    "We can now create a worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0736488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting org.apache.spark.deploy.master.Master, logging to /usr/local/spark//logs/spark-saverio-org.apache.spark.deploy.master.Master-1-saverio-PU301LA.out\n",
      "starting org.apache.spark.deploy.worker.Worker, logging to /usr/local/spark//logs/spark-saverio-org.apache.spark.deploy.worker.Worker-1-saverio-PU301LA.out\n"
     ]
    }
   ],
   "source": [
    "%%script bash --no-raise-error\n",
    "\n",
    "# start master \n",
    "$SPARK_HOME/sbin/start-master.sh --host localhost \\\n",
    "    --port 7077 --webui-port 8080\n",
    "    \n",
    "# start worker\n",
    "$SPARK_HOME/sbin/start-worker.sh spark://localhost:7077 \\\n",
    "    --cores 2 --memory 1g\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d18b2d7",
   "metadata": {},
   "source": [
    "## Create the spark session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24d00a0",
   "metadata": {},
   "source": [
    "We can now create the spark session. With the following command we are asking to the master (and resource manager) to create an application with required resources and configurations. In this case we are using all the default options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40e94454",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/08/30 22:53:13 WARN Utils: Your hostname, saverio-PU301LA resolves to a loopback address: 127.0.1.1; using 192.168.1.24 instead (on interface wlp3s0)\n",
      "21/08/30 22:53:13 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/spark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/saverio/.ivy2/cache\n",
      "The jars for the packages stored in: /home/saverio/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-a8508cb9-806d-4149-8abb-58fdd72f6bee;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.1.1 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.1.1 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.6.0 in central\n",
      "\tfound com.github.luben#zstd-jni;1.4.8-1 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.2 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      ":: resolution report :: resolve 628ms :: artifacts dl 15ms\n",
      "\t:: modules in use:\n",
      "\tcom.github.luben#zstd-jni;1.4.8-1 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.6.0 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.1.1 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.1.1 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.2 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   9   |   0   |   0   |   0   ||   9   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-a8508cb9-806d-4149-8abb-58fdd72f6bee\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 9 already retrieved (0kB/13ms)\n",
      "21/08/30 22:53:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "KAFKA_BOOTSTRAP_SERVERS = ''\n",
    "\n",
    "    \n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"spark://localhost:7077\")\\\n",
    "    .appName(\"Spark Streaming\")\\\n",
    "    .config(\"spark.jars.packages\",\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.1\")\\\n",
    "    .getOrCreate()\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c00f187",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.24:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://localhost:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark Streaming</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fabdce57340>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d515875",
   "metadata": {},
   "source": [
    "## KAFKA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1b41e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "KAFKA_HOME = '/usr/local/kafka'\n",
    "KAFKA_BOOTSTRAP_SERVERS = 'localhost:9092'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "536dd111",
   "metadata": {},
   "outputs": [],
   "source": [
    "#By some reason I can't launch this from here using OS, so i open the terminals in the KAFKA_HOME folder\n",
    "# and launch the zookeper and the kafka server comands manually\n",
    "\n",
    "\n",
    "# Start Zookeeper\n",
    "# bin/zookeeper-server-start.sh config/zookeeper.properties \n",
    "#os.system('{0}/bin/zookeeper-server-start.sh {0}/config/zookeeper.properties'.format(KAFKA_HOME)) \n",
    "    \n",
    "# Start one Kafka Broker\n",
    "#bin/kafka-server-start.sh config/server.properties\n",
    "#os.system('{0}/bin/kafka-server-start.sh {0}/config/server.properties'.format(KAFKA_HOME)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a8fcbf",
   "metadata": {},
   "source": [
    "### Create the topics for kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0c7471f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CreateTopicsResponse_v3(throttle_time_ms=0, topic_errors=[(topic='Experiment_measurements', error_code=0, error_message=None), (topic='results', error_code=0, error_message=None)])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from kafka.admin import KafkaAdminClient, NewTopic\n",
    "kafka_admin = KafkaAdminClient(\n",
    "        bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,\n",
    "    )\n",
    "\n",
    "#Here we will inject the data\n",
    "new_topic_a = NewTopic(name='Experiment_measurements', \n",
    "                       num_partitions=1, \n",
    "                       replication_factor=1)\n",
    "\n",
    "#Here we inject the number of processed hits, post cleaning\n",
    "new_topic_b = NewTopic(name='results', \n",
    "                       num_partitions=1, \n",
    "                       replication_factor=1)\n",
    "\n",
    "kafka_admin.create_topics(new_topics=[new_topic_a,new_topic_b])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ecf70ea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['results', 'Experiment_measurements']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kafka_admin.list_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f58e90",
   "metadata": {},
   "source": [
    "## KAFKA - SPARK INTEGRATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c5f9f0",
   "metadata": {},
   "source": [
    "### Read the data from the kafka topic (define the consumer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "581fc2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDF = spark\\\n",
    "    .readStream\\\n",
    "    .format(\"kafka\")\\\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP_SERVERS)\\\n",
    "    .option('subscribe', 'Experiment_measurements')\\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8445674",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inputDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92c9e3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json, col, when\n",
    "from pyspark.sql.types import StructField, StructType, DoubleType, IntegerType\n",
    "\n",
    "## the schema of the json data format used to create the messages\n",
    "schema = StructType(\n",
    "        [\n",
    "                StructField(\"HEAD\",        IntegerType()),\n",
    "                StructField(\"FPGA\",        IntegerType()),\n",
    "                StructField(\"TDC_CHANNEL\", IntegerType()),\n",
    "                StructField(\"ORBIT_CNT\",   DoubleType()),\n",
    "                StructField(\"BX_COUNTER\",  IntegerType()),\n",
    "                StructField(\"TDC_MEAS\",    DoubleType())\n",
    "        ]  \n",
    ")\n",
    "\n",
    "## a new DF can be created from the previous by using the pyspark.sql functions\n",
    "jsonDF = inputDF.select(from_json(col(\"value\").alias('value').cast(\"string\"), schema).alias('value'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e51f3af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: struct (nullable = true)\n",
      " |    |-- HEAD: integer (nullable = true)\n",
      " |    |-- FPGA: integer (nullable = true)\n",
      " |    |-- TDC_CHANNEL: integer (nullable = true)\n",
      " |    |-- ORBIT_CNT: double (nullable = true)\n",
      " |    |-- BX_COUNTER: integer (nullable = true)\n",
      " |    |-- TDC_MEAS: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jsonDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e85552bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#jsonDF.writeStream\\\n",
    "#   .outputMode(\"append\")\\\n",
    "#   .format(\"console\")\\\n",
    "#   .start()\\\n",
    "#   .awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "194ce688",
   "metadata": {},
   "outputs": [],
   "source": [
    "flatDF = jsonDF.selectExpr(\"value.HEAD\", \n",
    "                           \"value.FPGA\", \n",
    "                           \"value.TDC_CHANNEL\",\n",
    "                           \"value.ORBIT_CNT\",\n",
    "                           \"value.BX_COUNTER\",\n",
    "                           \"value.TDC_MEAS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c998e0a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- HEAD: integer (nullable = true)\n",
      " |-- FPGA: integer (nullable = true)\n",
      " |-- TDC_CHANNEL: integer (nullable = true)\n",
      " |-- ORBIT_CNT: double (nullable = true)\n",
      " |-- BX_COUNTER: integer (nullable = true)\n",
      " |-- TDC_MEAS: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flatDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e780d855",
   "metadata": {},
   "outputs": [],
   "source": [
    "#flatDF.writeStream\\\n",
    "#   .outputMode(\"append\")\\\n",
    "#   .format(\"console\")\\\n",
    "#   .start()\\\n",
    "#   .awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4850e120",
   "metadata": {},
   "source": [
    "### SPARK processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dcf3f256",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "#Keep the events where \"HEAD\"=2\n",
    "cleanDF = flatDF.where(col('HEAD')==2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "addb605f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computations(DF, epoch):\n",
    "    #This function perform the whole operations on the received batch,\n",
    "    #\n",
    "\n",
    "    #As the 4 calculations that we have to perform are done foe each chamber we set 4 dataframes\n",
    "    chamber_1 = DF.filter((col(\"FPGA\") == 0) & (col(\"TDC_CHANNEL\")>=0) & (col(\"TDC_CHANNEL\")<=63))\n",
    "    chamber_2 = DF.filter((col(\"FPGA\") == 0) & (col(\"TDC_CHANNEL\")>=64) & (col(\"TDC_CHANNEL\")<=127))\n",
    "    chamber_3 = DF.filter((col(\"FPGA\") == 1) & (col(\"TDC_CHANNEL\")>=0) & (col(\"TDC_CHANNEL\")<=63))\n",
    "    chamber_4 = DF.filter((col(\"FPGA\") == 1) & (col(\"TDC_CHANNEL\")>=64) & (col(\"TDC_CHANNEL\")<=127))\n",
    "\n",
    "    #Now we can count the number of events in each chamber\n",
    "    n_c1 = chamber_1.count()\n",
    "    n_c2 = chamber_2.count()\n",
    "    n_c3 = chamber_3.count()\n",
    "    n_c4 = chamber_4.count()\n",
    "\n",
    "    #Total number of events\n",
    "    n = n_c1 + n_c2 + n_c3 + n_c4\n",
    "\n",
    "\n",
    "    #Histograms    \n",
    "    h_c1 = chamber_1.groupBy('TDC_CHANNEL').count().collect()\n",
    "    h_c2 = chamber_2.groupBy('TDC_CHANNEL').count().collect()\n",
    "    h_c3 = chamber_3.groupBy('TDC_CHANNEL').count().collect()\n",
    "    h_c4 = chamber_4.groupBy('TDC_CHANNEL').count().collect()\n",
    "\n",
    "    h_active_1 = chamber_1.groupBy('TDC_CHANNEL','ORBIT_CNT').count().collect()\n",
    "    h_active_2 = chamber_2.groupBy('TDC_CHANNEL','ORBIT_CNT').count().collect()\n",
    "    h_active_3 = chamber_3.groupBy('TDC_CHANNEL','ORBIT_CNT').count().collect()\n",
    "    h_active_4 = chamber_4.groupBy('TDC_CHANNEL','ORBIT_CNT').count().collect()\n",
    "    \n",
    "    \n",
    "    #Organise the results to send them to one topic as a dictionary\n",
    "    results = {'Total_events': n,\n",
    "              'Events_per_chamber': [n_c1,n_c2,n_c3,n_c4],\n",
    "              'Histogram_1': [h_c1, h_c2, h_c3, h_c4],\n",
    "              'Histogram_2': [h_active_1,h_active_2,h_active_3,h_active_4]}\n",
    "    \n",
    "    #publish the results in the \"results\" topic for further usage\n",
    "    producer.send(topic='results', value=json.dumps(results).encode('utf-8'))\n",
    "    producer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f1e5d3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computations_2(DF, epoch):\n",
    "    start=time.time()\n",
    "    #This function perform the whole operations on the received batch,\n",
    "    #\n",
    "\n",
    "    #As the 4 calculations that we have to perform are done foe each chamber we set 4 dataframes\n",
    "    chamber_1 = DF.filter((col(\"FPGA\") == 0) & (col(\"TDC_CHANNEL\")>=0) & (col(\"TDC_CHANNEL\")<=63))\n",
    "    chamber_2 = DF.filter((col(\"FPGA\") == 0) & (col(\"TDC_CHANNEL\")>=64) & (col(\"TDC_CHANNEL\")<=127))\n",
    "    chamber_3 = DF.filter((col(\"FPGA\") == 1) & (col(\"TDC_CHANNEL\")>=0) & (col(\"TDC_CHANNEL\")<=63))\n",
    "    chamber_4 = DF.filter((col(\"FPGA\") == 1) & (col(\"TDC_CHANNEL\")>=64) & (col(\"TDC_CHANNEL\")<=127))\n",
    "\n",
    "    #Initialize results dictionary\n",
    "    results = {}\n",
    "    results[\"Total Count\"] = {}\n",
    "    chamber_name = [\"Chamber_1\", \"Chamber_2\", \"Chamber_3\", \"Chamber_4\"]\n",
    "    for chamber in chamber_name:\n",
    "        results[chamber] = {}\n",
    "        results[chamber][\"Count\"] = {}\n",
    "        \n",
    "    #Now we can count the number of events in each chamber\n",
    "    results[\"Chamber_1\"][\"Count\"] = chamber_1.count()\n",
    "    results[\"Chamber_2\"][\"Count\"] = chamber_2.count()\n",
    "    results[\"Chamber_3\"][\"Count\"] = chamber_3.count()\n",
    "    results[\"Chamber_4\"][\"Count\"] = chamber_4.count()\n",
    "\n",
    "    #Total number of events\n",
    "    results[\"Total Count\"] = results[\"Chamber_1\"][\"Count\"] + results[\"Chamber_2\"][\"Count\"] + \\\n",
    "                             results[\"Chamber_3\"][\"Count\"] + results[\"Chamber_4\"][\"Count\"]\n",
    "    \n",
    "\n",
    "    # Compute histograms for each chamber   \n",
    "    i=0    \n",
    "    for chamber in [chamber_1, chamber_2, chamber_3, chamber_4]:\n",
    "        #Initialize dictionary partitions to save the results\n",
    "        for hist in [\"Hist_1\",\"Hist_2\"]:\n",
    "            results[chamber_name[i]][hist] = {}\n",
    "            results[chamber_name[i]][hist][\"Bins\"] = {}\n",
    "            results[chamber_name[i]][hist][\"Counts\"] = {}\n",
    "        \n",
    "        if(chamber.count()!=0): \n",
    "            #Histogram 1\n",
    "            bins, counts = (\n",
    "                chamber.select(\"TDC_CHANNEL\")\n",
    "                .rdd.map(lambda x: x.TDC_CHANNEL)\n",
    "                .histogram(list(np.arange(0,170,5)))\n",
    "            )\n",
    "            \n",
    "            results[chamber_name[i]][\"Hist_1\"][\"Bins\"] = bins\n",
    "            results[chamber_name[i]][\"Hist_1\"][\"Counts\"] = counts\n",
    "            \n",
    "            #Histogram 2\n",
    "            bins, counts = (\n",
    "            chamber.groupBy(\"TDC_CHANNEL\",\"ORBIT_CNT\")\n",
    "            .count()\n",
    "            .select(\"ORBIT_CNT\")\n",
    "            .rdd.map(lambda x: x.ORBIT_CNT)\n",
    "            .histogram(list(np.arange(6.e5,1.e7,0.5e6)))\n",
    "            )\n",
    "            \n",
    "            results[chamber_name[i]][\"Hist_2\"][\"Bins\"] = bins\n",
    "            results[chamber_name[i]][\"Hist_2\"][\"Counts\"] = counts            \n",
    "            \n",
    "        else:\n",
    "            #Histogram 1\n",
    "            results[chamber_name[i]][\"Hist_1\"][\"Bins\"] = list(np.arange(0,170,5))\n",
    "            counts = list(np.arange(0,170,5)* 0) \n",
    "            results[chamber_name[i]][\"Hist_1\"][\"Counts\"] = counts\n",
    "            \n",
    "            #Histogram 2\n",
    "            results[chamber_name[i]][\"Hist_2\"][\"Bins\"] = list(np.arange(6.e5,1.e7,0.5e6))\n",
    "            counts = list(np.arange(6.e5,1.e7,0.5e6)* 0) \n",
    "            results[chamber_name[i]][\"Hist_2\"][\"Counts\"] = counts\n",
    "        i +=1\n",
    "    \n",
    "    end =time.time()\n",
    "    print(\"Time =\",end-start)\n",
    "    producer.send(topic=\"results\", value= str(results).encode('utf-8'))\n",
    "    producer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "162d6953",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computations_3(DF, epoch):\n",
    "    start=time.time()\n",
    "    #This function perform the whole operations on the received batch,\n",
    "    \n",
    "    #Add a column with the chamber number\n",
    "    DF_new = DF.withColumn('chamber',when((col(\"FPGA\") == 0) & (col(\"TDC_CHANNEL\")<=63),1).\n",
    "                                 when((col(\"FPGA\") == 0) & (col(\"TDC_CHANNEL\")>=64),2).\n",
    "                                 when((col(\"FPGA\") == 1) & (col(\"TDC_CHANNEL\")<=63),3).\n",
    "                                 when((col(\"FPGA\") == 1) & (col(\"TDC_CHANNEL\")>=64),4)).\\\n",
    "                                 select([ col('TDC_CHANNEL'), col('ORBIT_CNT'),\n",
    "                                    col('BX_COUNTER'),col('TDC_MEAS'),\n",
    "                                    col('chamber')])\n",
    "    DF_new.persist()\n",
    "    #DF_new.show()\n",
    "    #Initialize results dictionary\n",
    "    results = {}\n",
    "    results[\"Total Count\"] = {}\n",
    "    results[\"Index\"] = time.time()\n",
    "    chamber_name = [\"Chamber_1\", \"Chamber_2\", \"Chamber_3\", \"Chamber_4\"]\n",
    "    for chamber in chamber_name:\n",
    "        results[chamber] = {}\n",
    "        results[chamber][\"Count\"] = {}\n",
    "        for hist in [\"Hist_1\",\"Hist_2\"]:\n",
    "            results[chamber][hist] = {}\n",
    "            results[chamber][hist][\"Bins\"] = {}\n",
    "            results[chamber][hist][\"Counts\"] = {}\n",
    "        \n",
    "    # Compute histograms for each chamber   \n",
    "    for i in [1,2,3,4]:\n",
    "        #Now we can count the number of events in each chamber\n",
    "        chamber = DF_new.filter(col(\"chamber\") == i).persist()\n",
    "        results[f\"Chamber_{i}\"][\"Count\"] = chamber.count()\n",
    "        \n",
    "        if(results[f\"Chamber_{i}\"][\"Count\"]!=0):\n",
    "            \n",
    "            #Histogram 1\n",
    "            bins, counts = (\n",
    "            chamber.select(\"TDC_CHANNEL\")\n",
    "                 .rdd.map(lambda x: x.TDC_CHANNEL)\n",
    "                 .histogram(list(np.arange(0,170,5)))\n",
    "            )\n",
    "            \n",
    "            results[f\"Chamber_{i}\"][\"Hist_1\"][\"Bins\"] = bins\n",
    "            results[f\"Chamber_{i}\"][\"Hist_1\"][\"Counts\"] = counts\n",
    "            \n",
    "            #Histogram 2\n",
    "            bins, counts = (\n",
    "            chamber.groupBy(\"TDC_CHANNEL\",\"ORBIT_CNT\")\n",
    "            .count()\n",
    "            .select(\"ORBIT_CNT\")\n",
    "            .rdd.map(lambda x: x.ORBIT_CNT)\n",
    "            .histogram(list(np.arange(6.e5,1.e7,0.5e6)))\n",
    "            )\n",
    "            \n",
    "            results[f\"Chamber_{i}\"][\"Hist_2\"][\"Bins\"] = bins\n",
    "            results[f\"Chamber_{i}\"][\"Hist_2\"][\"Counts\"] = counts            \n",
    "                \n",
    "        else:\n",
    "            #Histogram 1\n",
    "            results[f\"Chamber_{i}\"][\"Hist_1\"][\"Bins\"] = list(np.arange(0,170,5))\n",
    "            counts = list(np.arange(0,170,5)* 0) \n",
    "            results[f\"Chamber_{i}\"][\"Hist_1\"][\"Counts\"] = counts\n",
    "            \n",
    "             #Histogram 2\n",
    "            results[f\"Chamber_{i}\"][\"Hist_2\"][\"Bins\"] = list(np.arange(6.e5,1.e7,0.5e6))\n",
    "            counts = list(np.arange(6.e5,1.e7,0.5e6)* 0) \n",
    "            results[f\"Chamber_{i}\"][\"Hist_2\"][\"Counts\"] = counts\n",
    "        chamber = DF_new.filter(col(\"chamber\") == i).unpersist()  \n",
    "        \n",
    "    results[\"Total Count\"] = results[\"Chamber_1\"][\"Count\"] + results[\"Chamber_2\"][\"Count\"] + \\\n",
    "                             results[\"Chamber_3\"][\"Count\"] + results[\"Chamber_4\"][\"Count\"]\n",
    "    end =time.time()\n",
    "    print(\"Time =\",end-start)\n",
    "       \n",
    "    \n",
    "    producer.send(topic=\"results\", value= str(results).encode('utf-8'))\n",
    "    producer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3f0b4b67-95f8-40f0-9b48-20d200283607",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computations_7(DF, epoch):\n",
    "    start=time.time()\n",
    "    #This function perform the whole operations on the received batch,\n",
    "    \n",
    "    #Add a column with the chamber number\n",
    "    #DF_clean     = DF.filter(col(\"HEAD\")==2)\n",
    "    DF_hit = DF.withColumn('CHAMBER',when(col(\"FPGA\") == 0, \n",
    "                                                when(col(\"TDC_CHANNEL\")<=63,1).\\\n",
    "                                                otherwise(2)).\\\n",
    "                                           otherwise(when(col(\"TDC_CHANNEL\")<=63,3).\\\n",
    "                                                     otherwise(when(col(\"TDC_CHANNEL\")<128,4))\n",
    "                                           )).\\\n",
    "                                           select([ col('TDC_CHANNEL'), col('ORBIT_CNT'),\n",
    "                                           col('BX_COUNTER'),col('TDC_MEAS'),\n",
    "                                           col('CHAMBER')])\n",
    "    \n",
    "    #Initialize results dictionary\n",
    "    results = {}\n",
    "    results[\"Total Count\"] = {}\n",
    "    results[\"Index\"] = time.time()\n",
    "    chamber_name = [\"Chamber_1\", \"Chamber_2\", \"Chamber_3\", \"Chamber_4\"]\n",
    "    for chamber in chamber_name:\n",
    "        results[chamber] = {}\n",
    "        results[chamber][\"Count\"] = {}\n",
    "        for hist in [\"Hist_1\",\"Hist_2\",\"Hist_3\",\"Hist_4\"]:\n",
    "            results[chamber][hist] = {}\n",
    "            results[chamber][hist][\"Bins\"] = {}\n",
    "            results[chamber][hist][\"Counts\"] = {}\n",
    "    \n",
    "    \n",
    "   \n",
    "    #We prepare the scilantor data\n",
    "    #First we filter the events encoding the passage time,\n",
    "    #then we add the PASSAGE time for each event \n",
    "    #Finally if we have two scilantor hits within the same orbit we keep \n",
    "    #the one with the smallertime\n",
    "    DF_scintillator = DF_hit.filter((col(\"CHAMBER\").isNull())).\\\n",
    "                          withColumn(\"PASSAGETIME\", 25 * (col(\"ORBIT_CNT\") * 3564 + col(\"BX_COUNTER\") + col(\"TDC_MEAS\")/30)).\\\n",
    "                          drop(\"TDC_CHANNEL\").drop(\"BX_COUNTER\").\\\n",
    "                          drop(\"TDC_MEAS\").drop(\"CHAMBER\").\\\n",
    "                          groupBy(\"ORBIT_CNT\").min(\"PASSAGETIME\").\\\n",
    "                          withColumnRenamed(\"ORBIT_CNT\",\"ORBIT_CNT_sci\").\\\n",
    "                          withColumnRenamed(\"min(PASSAGETIME)\",\"PASSAGETIME\")\n",
    "    \n",
    "    #Drop the columns with null values from DF_hit\n",
    "    DF_hit.na.drop(subset=[\"CHAMBER\"])\n",
    "        \n",
    "    # scintillator time offset by Chamber\n",
    "    time_offset_by_chamber = {\n",
    "    1: 95.0 - 1.1, # Ch 0\n",
    "    2: 95.0 + 6.4, # Ch 1\n",
    "    3: 95.0 + 0.5, # Ch 2\n",
    "    4: 95.0 - 2.6, # Ch 3\n",
    "    }\n",
    "\n",
    "    # Compute histograms for each chamber   \n",
    "    for i in [1,2,3,4]:     \n",
    "        #Now we can count the number of events in each chamber\n",
    "        chamber = DF_hit.filter(col(\"CHAMBER\") == i).persist()\n",
    "        results[f\"Chamber_{i}\"][\"Count\"] = chamber.count()\n",
    "        \n",
    "        if(results[f\"Chamber_{i}\"][\"Count\"]!=0):\n",
    "            \n",
    "            #Histogram 1\n",
    "            bins_1, counts_1 = (\n",
    "            chamber.select(\"TDC_CHANNEL\")\n",
    "                 .rdd.map(lambda x: x.TDC_CHANNEL)\n",
    "                 .histogram(list(np.arange(0,170,5)))\n",
    "            )\n",
    "            \n",
    "            results[f\"Chamber_{i}\"][\"Hist_1\"][\"Bins\"] = bins_1\n",
    "            results[f\"Chamber_{i}\"][\"Hist_1\"][\"Counts\"] = counts_1\n",
    "            \n",
    "            #Histogram 2\n",
    "            bins_2, counts_2 = (\n",
    "            chamber.groupBy(\"TDC_CHANNEL\",\"ORBIT_CNT\")\n",
    "            .count()\n",
    "            .select(\"ORBIT_CNT\")\n",
    "            .rdd.map(lambda x: x.ORBIT_CNT)\n",
    "            .histogram(list(np.arange(6.e5,1.e7,0.5e6)))\n",
    "            )\n",
    "            \n",
    "            results[f\"Chamber_{i}\"][\"Hist_2\"][\"Bins\"] = bins_2\n",
    "            results[f\"Chamber_{i}\"][\"Hist_2\"][\"Counts\"] = counts_2            \n",
    "            \n",
    "            \n",
    "            #keep only the hits with a scintillator signal within the same orbit\n",
    "            chamber_sci = chamber.join(DF_scintillator,chamber.ORBIT_CNT ==  DF_scintillator.ORBIT_CNT_sci,\"inner\")\n",
    "#Add the ABSSOLUTETIME \n",
    "            chamber_sci = chamber_sci.withColumn(\"ABSOLUTETIME\",\n",
    "                             25 * (col(\"ORBIT_CNT\") * 3564 + col(\"BX_COUNTER\") + col(\"TDC_MEAS\")/30))\n",
    "            #Drop useless data\n",
    "#            chamber_sci = chamber_sci.drop(\"HEAD\").drop(\"FPGA\").drop(\"BX_COUNTER\").drop(\"ORBIT_CNT_sci\")\n",
    "  \n",
    "            #Add DRIFTIME\n",
    "            chamber_sci = chamber_sci.withColumn(\"DRIFTIME\",col(\"ABSOLUTETIME\")-col(\"PASSAGETIME\") + time_offset_by_chamber[i])#.show()\n",
    "\n",
    "        \n",
    "            #Histogram 3\n",
    "            bins_3, counts_3 = (\n",
    "            chamber_sci.select(\"TDC_CHANNEL\")\n",
    "                 .rdd.map(lambda x: x.TDC_CHANNEL)\n",
    "                 .histogram(list(np.arange(0,170,5)))\n",
    "            )    \n",
    "            results[f\"Chamber_{i}\"][\"Hist_3\"][\"Bins\"] = bins_3\n",
    "            results[f\"Chamber_{i}\"][\"Hist_3\"][\"Counts\"] = counts_3\n",
    "            \n",
    "            #Histogram 4\n",
    "            bins_4, counts_4 = (\n",
    "            chamber_sci.select(\"DRIFTIME\")\n",
    "                 .rdd.map(lambda x: x.DRIFTIME)\n",
    "                 .histogram(list(np.arange(0,1000,10)))\n",
    "            )\n",
    "            results[f\"Chamber_{i}\"][\"Hist_4\"][\"Bins\"] = bins_4\n",
    "            results[f\"Chamber_{i}\"][\"Hist_4\"][\"Counts\"] = counts_4\n",
    "            \n",
    "            \n",
    "            \n",
    "        else:\n",
    "            #Histogram 1\n",
    "            results[f\"Chamber_{i}\"][\"Hist_1\"][\"Bins\"] = list(np.arange(0,170,5))\n",
    "            counts = list(np.arange(0,170,5)* 0) \n",
    "            results[f\"Chamber_{i}\"][\"Hist_1\"][\"Counts\"] = counts\n",
    "            \n",
    "             #Histogram 2\n",
    "            results[f\"Chamber_{i}\"][\"Hist_2\"][\"Bins\"] = list(np.arange(6.e5,1.e7,0.5e6))\n",
    "            counts = list(np.arange(6.e5,1.e7,0.5e6)* 0) \n",
    "            results[f\"Chamber_{i}\"][\"Hist_2\"][\"Counts\"] = counts\n",
    "            \n",
    "            #Histogram 3\n",
    "            results[f\"Chamber_{i}\"][\"Hist_3\"][\"Bins\"] = list(np.arange(0,170,5))\n",
    "            counts = list(np.arange(0,170,5)* 0) \n",
    "            results[f\"Chamber_{i}\"][\"Hist_3\"][\"Counts\"] = counts\n",
    "            \n",
    "             #Histogram 4\n",
    "            results[f\"Chamber_{i}\"][\"Hist_4\"][\"Bins\"] = list(np.arange(-100,1000,10))\n",
    "            counts = list(np.arange(-100,1000,10)* 0) \n",
    "            results[f\"Chamber_{i}\"][\"Hist_4\"][\"Counts\"] = counts\n",
    "        chamber.unpersist()\n",
    "        \n",
    "    results[\"Total Count\"] = results[\"Chamber_1\"][\"Count\"] + results[\"Chamber_2\"][\"Count\"] + \\\n",
    "                             results[\"Chamber_3\"][\"Count\"] + results[\"Chamber_4\"][\"Count\"]\n",
    "    end =time.time()\n",
    "    end =time.time()\n",
    "    print(\"Time =\",end-start)\n",
    "       \n",
    "    producer.send(topic=\"results\", value= str(results).encode('utf-8'))\n",
    "    #producer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d94126a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer\n",
    "\n",
    "#Send the results to the kafka topic\n",
    "#Initialize the producer\n",
    "producer = KafkaProducer(bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253b9d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/08/30 22:53:47 WARN StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-e47c420a-b227-4686-8d84-d2322f7b83c9. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time = 4.535778999328613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/08/30 22:53:55 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 6000 milliseconds, but spent 7581 milliseconds\n",
      "21/08/30 22:55:12 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 6000 milliseconds, but spent 72919 milliseconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time = 72.60547423362732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/08/30 22:56:23 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 6000 milliseconds, but spent 70863 milliseconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time = 70.62349343299866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time = 66.92310690879822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/08/30 22:57:30 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 6000 milliseconds, but spent 67134 milliseconds\n",
      "21/08/30 22:58:32 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 6000 milliseconds, but spent 61350 milliseconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time = 61.21691346168518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/08/30 22:59:29 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 6000 milliseconds, but spent 57697 milliseconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time = 57.57320857048035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/08/30 23:00:27 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 6000 milliseconds, but spent 57234 milliseconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time = 57.11191463470459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 309:======================================>              (146 + 2) / 200]\r"
     ]
    }
   ],
   "source": [
    "#Trigger the processing\n",
    "cleanDF.writeStream\\\n",
    "    .foreachBatch(computations_7)\\\n",
    "    .trigger(processingTime='6 second')\\\n",
    "    .start()\\\n",
    "    .awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fd73b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5484d6cf",
   "metadata": {},
   "source": [
    "If you also want to delete any data of your local Kafka environment including any events you have created along the way, run the command:\n",
    "\n",
    "`` $ rm -rf /tmp/kafka-logs /tmp/zookeeper `` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f22ff1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
