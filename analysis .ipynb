{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming with Kafka and Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I try to implement a basic pipeline for the project conecting kafka with spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computer setting\n",
    "I downloaded and located in my home the spark file **spark-3.1.2-bin-hadoop3.2** and also the kafka file **kafka_2.13-2.7.0**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Kafka and Spark ready"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standalone cluster deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now initialize all the required variables with `findspark.init()` by passing the path to the spark folder we downloaded previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/usr/local/spark')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to start the master, This will spin up the spark master with address spark://localhost:7077 and a cluster dashboark at localhost:8080.\n",
    "\n",
    "We can now create a worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting org.apache.spark.deploy.worker.Worker, logging to /usr/local/spark/logs/spark-root-org.apache.spark.deploy.worker.Worker-1-mapd-b-gr17-5.novalocal.out\n",
      "starting org.apache.spark.deploy.master.Master, logging to /usr/local/spark/logs/spark-root-org.apache.spark.deploy.master.Master-1-mapd-b-gr17-5.novalocal.out\n"
     ]
    }
   ],
   "source": [
    "%%script bash --no-raise-error\n",
    "\n",
    "# start master \n",
    "$SPARK_HOME/sbin/start-master.sh --host localhost \\\n",
    "    --port 7077 --webui-port 8080\n",
    "    \n",
    "# start worker\n",
    "$SPARK_HOME/sbin/start-worker.sh spark://localhost:7077 \\\n",
    "    --cores 4 --memory 2g\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the spark session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create the spark session. With the following command we are asking to the master (and resource manager) to create an application with required resources and configurations. In this case we are using all the default options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "KAFKA_BOOTSTRAP_SERVERS = ''\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"spark://master:7077\")\\\n",
    "    .appName(\"Spark Streaming\")\\\n",
    "    .config(\"spark.jars.packages\",\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.1\")\\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://master:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark Streaming</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f4fc1eeaa58>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KAFKA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "KAFKA_HOME = '/usr/local/kafka'\n",
    "KAFKA_BOOTSTRAP_SERVERS = 'slave01:9092'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#By some reason I can't launch this from here using OS, so i open the terminals in the KAFKA_HOME folder\n",
    "# and launch the zookeper and the kafka server comands manually\n",
    "\n",
    "\n",
    "# Start Zookeeper\n",
    "# bin/zookeeper-server-start.sh config/zookeeper.properties \n",
    "#os.system('{0}/bin/zookeeper-server-start.sh {0}/config/zookeeper.properties'.format(KAFKA_HOME)) \n",
    "    \n",
    "# Start one Kafka Broker\n",
    "#bin/kafka-server-start.sh config/server.properties\n",
    "#os.system('{0}/bin/kafka-server-start.sh {0}/config/server.properties'.format(KAFKA_HOME)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the topics for kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CreateTopicsResponse_v3(throttle_time_ms=0, topic_errors=[(topic='Experiment_measurements', error_code=0, error_message=None), (topic='results', error_code=0, error_message=None)])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from kafka.admin import KafkaAdminClient, NewTopic\n",
    "kafka_admin = KafkaAdminClient(\n",
    "        bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,\n",
    "    )\n",
    "\n",
    "#Here we will inject the data\n",
    "new_topic_a = NewTopic(name='Experiment_measurements', \n",
    "                       num_partitions=1, \n",
    "                       replication_factor=1)\n",
    "\n",
    "#Here we inject the number of processed hits, post cleaning\n",
    "new_topic_b = NewTopic(name='results', \n",
    "                       num_partitions=1, \n",
    "                       replication_factor=1)\n",
    "\n",
    "kafka_admin.create_topics(new_topics=[new_topic_a,new_topic_b])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['results', 'Experiment_measurements']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kafka_admin.list_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KAFKA - SPARK INTEGRATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data from the kafka topic (define the consumer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDF = spark\\\n",
    "    .readStream\\\n",
    "    .format(\"kafka\")\\\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP_SERVERS)\\\n",
    "    .option('subscribe', 'Experiment_measurements')\\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inputDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json, col, when\n",
    "from pyspark.sql.types import StructField, StructType, DoubleType, IntegerType\n",
    "\n",
    "## the schema of the json data format used to create the messages\n",
    "schema = StructType(\n",
    "        [\n",
    "                StructField(\"HEAD\",        IntegerType()),\n",
    "                StructField(\"FPGA\",        IntegerType()),\n",
    "                StructField(\"TDC_CHANNEL\", IntegerType()),\n",
    "                StructField(\"ORBIT_CNT\",   DoubleType()),\n",
    "                StructField(\"BX_COUNTER\",  IntegerType()),\n",
    "                StructField(\"TDC_MEAS\",    DoubleType())\n",
    "        ]  \n",
    ")\n",
    "\n",
    "## a new DF can be created from the previous by using the pyspark.sql functions\n",
    "jsonDF = inputDF.select(from_json(col(\"value\").alias('value').cast(\"string\"), schema).alias('value'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: struct (nullable = true)\n",
      " |    |-- HEAD: integer (nullable = true)\n",
      " |    |-- FPGA: integer (nullable = true)\n",
      " |    |-- TDC_CHANNEL: integer (nullable = true)\n",
      " |    |-- ORBIT_CNT: double (nullable = true)\n",
      " |    |-- BX_COUNTER: integer (nullable = true)\n",
      " |    |-- TDC_MEAS: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jsonDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#jsonDF.writeStream\\\n",
    "#   .outputMode(\"append\")\\\n",
    "#   .format(\"console\")\\\n",
    "#   .start()\\\n",
    "#   .awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatDF = jsonDF.selectExpr(\"value.HEAD\", \n",
    "                           \"value.FPGA\", \n",
    "                           \"value.TDC_CHANNEL\",\n",
    "                           \"value.ORBIT_CNT\",\n",
    "                           \"value.BX_COUNTER\",\n",
    "                           \"value.TDC_MEAS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- HEAD: integer (nullable = true)\n",
      " |-- FPGA: integer (nullable = true)\n",
      " |-- TDC_CHANNEL: integer (nullable = true)\n",
      " |-- ORBIT_CNT: double (nullable = true)\n",
      " |-- BX_COUNTER: integer (nullable = true)\n",
      " |-- TDC_MEAS: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flatDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flatDF.writeStream\\\n",
    "#   .outputMode(\"append\")\\\n",
    "#   .format(\"console\")\\\n",
    "#   .start()\\\n",
    "#   .awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SPARK processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "#Keep the events where \"HEAD\"=2\n",
    "cleanDF = flatDF.where(col('HEAD')==2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computations(DF, epoch):\n",
    "    #This function perform the whole operations on the received batch,\n",
    "    #\n",
    "\n",
    "    #As the 4 calculations that we have to perform are done foe each chamber we set 4 dataframes\n",
    "    chamber_1 = DF.filter((col(\"FPGA\") == 0) & (col(\"TDC_CHANNEL\")>=0) & (col(\"TDC_CHANNEL\")<=63))\n",
    "    chamber_2 = DF.filter((col(\"FPGA\") == 0) & (col(\"TDC_CHANNEL\")>=64) & (col(\"TDC_CHANNEL\")<=127))\n",
    "    chamber_3 = DF.filter((col(\"FPGA\") == 1) & (col(\"TDC_CHANNEL\")>=0) & (col(\"TDC_CHANNEL\")<=63))\n",
    "    chamber_4 = DF.filter((col(\"FPGA\") == 1) & (col(\"TDC_CHANNEL\")>=64) & (col(\"TDC_CHANNEL\")<=127))\n",
    "\n",
    "    #Now we can count the number of events in each chamber\n",
    "    n_c1 = chamber_1.count()\n",
    "    n_c2 = chamber_2.count()\n",
    "    n_c3 = chamber_3.count()\n",
    "    n_c4 = chamber_4.count()\n",
    "\n",
    "    #Total number of events\n",
    "    n = n_c1 + n_c2 + n_c3 + n_c4\n",
    "\n",
    "\n",
    "    #Histograms    \n",
    "    h_c1 = chamber_1.groupBy('TDC_CHANNEL').count().collect()\n",
    "    h_c2 = chamber_2.groupBy('TDC_CHANNEL').count().collect()\n",
    "    h_c3 = chamber_3.groupBy('TDC_CHANNEL').count().collect()\n",
    "    h_c4 = chamber_4.groupBy('TDC_CHANNEL').count().collect()\n",
    "\n",
    "    h_active_1 = chamber_1.groupBy('TDC_CHANNEL','ORBIT_CNT').count().collect()\n",
    "    h_active_2 = chamber_2.groupBy('TDC_CHANNEL','ORBIT_CNT').count().collect()\n",
    "    h_active_3 = chamber_3.groupBy('TDC_CHANNEL','ORBIT_CNT').count().collect()\n",
    "    h_active_4 = chamber_4.groupBy('TDC_CHANNEL','ORBIT_CNT').count().collect()\n",
    "    \n",
    "    \n",
    "    #Organise the results to send them to one topic as a dictionary\n",
    "    results = {'Total_events': n,\n",
    "              'Events_per_chamber': [n_c1,n_c2,n_c3,n_c4],\n",
    "              'Histogram_1': [h_c1, h_c2, h_c3, h_c4],\n",
    "              'Histogram_2': [h_active_1,h_active_2,h_active_3,h_active_4]}\n",
    "    \n",
    "    #publish the results in the \"results\" topic for further usage\n",
    "    producer.send(topic='results', value=json.dumps(results).encode('utf-8'))\n",
    "    producer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computations_2(DF, epoch):\n",
    "    start=time.time()\n",
    "    #This function perform the whole operations on the received batch,\n",
    "    #\n",
    "\n",
    "    #As the 4 calculations that we have to perform are done foe each chamber we set 4 dataframes\n",
    "    chamber_1 = DF.filter((col(\"FPGA\") == 0) & (col(\"TDC_CHANNEL\")>=0) & (col(\"TDC_CHANNEL\")<=63))\n",
    "    chamber_2 = DF.filter((col(\"FPGA\") == 0) & (col(\"TDC_CHANNEL\")>=64) & (col(\"TDC_CHANNEL\")<=127))\n",
    "    chamber_3 = DF.filter((col(\"FPGA\") == 1) & (col(\"TDC_CHANNEL\")>=0) & (col(\"TDC_CHANNEL\")<=63))\n",
    "    chamber_4 = DF.filter((col(\"FPGA\") == 1) & (col(\"TDC_CHANNEL\")>=64) & (col(\"TDC_CHANNEL\")<=127))\n",
    "\n",
    "    #Initialize results dictionary\n",
    "    results = {}\n",
    "    results[\"Total Count\"] = {}\n",
    "    chamber_name = [\"Chamber_1\", \"Chamber_2\", \"Chamber_3\", \"Chamber_4\"]\n",
    "    for chamber in chamber_name:\n",
    "        results[chamber] = {}\n",
    "        results[chamber][\"Count\"] = {}\n",
    "        \n",
    "    #Now we can count the number of events in each chamber\n",
    "    results[\"Chamber_1\"][\"Count\"] = chamber_1.count()\n",
    "    results[\"Chamber_2\"][\"Count\"] = chamber_2.count()\n",
    "    results[\"Chamber_3\"][\"Count\"] = chamber_3.count()\n",
    "    results[\"Chamber_4\"][\"Count\"] = chamber_4.count()\n",
    "\n",
    "    #Total number of events\n",
    "    results[\"Total Count\"] = results[\"Chamber_1\"][\"Count\"] + results[\"Chamber_2\"][\"Count\"] + \\\n",
    "                             results[\"Chamber_3\"][\"Count\"] + results[\"Chamber_4\"][\"Count\"]\n",
    "    \n",
    "\n",
    "    # Compute histograms for each chamber   \n",
    "    i=0    \n",
    "    for chamber in [chamber_1, chamber_2, chamber_3, chamber_4]:\n",
    "        #Initialize dictionary partitions to save the results\n",
    "        for hist in [\"Hist_1\",\"Hist_2\"]:\n",
    "            results[chamber_name[i]][hist] = {}\n",
    "            results[chamber_name[i]][hist][\"Bins\"] = {}\n",
    "            results[chamber_name[i]][hist][\"Counts\"] = {}\n",
    "        \n",
    "        if(chamber.count()!=0): \n",
    "            #Histogram 1\n",
    "            bins, counts = (\n",
    "                chamber.select(\"TDC_CHANNEL\")\n",
    "                .rdd.map(lambda x: x.TDC_CHANNEL)\n",
    "                .histogram(list(np.arange(0,170,5)))\n",
    "            )\n",
    "            \n",
    "            results[chamber_name[i]][\"Hist_1\"][\"Bins\"] = bins\n",
    "            results[chamber_name[i]][\"Hist_1\"][\"Counts\"] = counts\n",
    "            \n",
    "            #Histogram 2\n",
    "            bins, counts = (\n",
    "            chamber.groupBy(\"TDC_CHANNEL\",\"ORBIT_CNT\")\n",
    "            .count()\n",
    "            .select(\"ORBIT_CNT\")\n",
    "            .rdd.map(lambda x: x.ORBIT_CNT)\n",
    "            .histogram(list(np.arange(6.e5,1.e7,0.5e6)))\n",
    "            )\n",
    "            \n",
    "            results[chamber_name[i]][\"Hist_2\"][\"Bins\"] = bins\n",
    "            results[chamber_name[i]][\"Hist_2\"][\"Counts\"] = counts            \n",
    "            \n",
    "        else:\n",
    "            #Histogram 1\n",
    "            results[chamber_name[i]][\"Hist_1\"][\"Bins\"] = list(np.arange(0,170,5))\n",
    "            counts = list(np.arange(0,170,5)* 0) \n",
    "            results[chamber_name[i]][\"Hist_1\"][\"Counts\"] = counts\n",
    "            \n",
    "            #Histogram 2\n",
    "            results[chamber_name[i]][\"Hist_2\"][\"Bins\"] = list(np.arange(6.e5,1.e7,0.5e6))\n",
    "            counts = list(np.arange(6.e5,1.e7,0.5e6)* 0) \n",
    "            results[chamber_name[i]][\"Hist_2\"][\"Counts\"] = counts\n",
    "        i +=1\n",
    "    \n",
    "    end =time.time()\n",
    "    print(\"Time =\",end-start)\n",
    "    producer.send(topic=\"results\", value= str(results).encode('utf-8'))\n",
    "    producer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time = 12.103435754776001\n"
     ]
    }
   ],
   "source": [
    "def computations_3(DF, epoch):\n",
    "    start=time.time()\n",
    "    #This function perform the whole operations on the received batch,\n",
    "    \n",
    "    #Add a column with the chamber number\n",
    "    DF_new = DF.withColumn('chamber',when((col(\"FPGA\") == 0) & (col(\"TDC_CHANNEL\")<=63),1).\n",
    "                                 when((col(\"FPGA\") == 0) & (col(\"TDC_CHANNEL\")>=64),2).\n",
    "                                 when((col(\"FPGA\") == 1) & (col(\"TDC_CHANNEL\")<=63),3).\n",
    "                                 when((col(\"FPGA\") == 1) & (col(\"TDC_CHANNEL\")>=64),4)).\\\n",
    "                                 select([ col('TDC_CHANNEL'), col('ORBIT_CNT'),\n",
    "                                    col('BX_COUNTER'),col('TDC_MEAS'),\n",
    "                                    col('chamber')])\n",
    "    #DF_new.persist()\n",
    "    #DF_new.show()\n",
    "    #Initialize results dictionary\n",
    "    results = {}\n",
    "    results[\"Total Count\"] = {}\n",
    "    results[\"Index\"] = time.time()\n",
    "    chamber_name = [\"Chamber_1\", \"Chamber_2\", \"Chamber_3\", \"Chamber_4\"]\n",
    "    for chamber in chamber_name:\n",
    "        results[chamber] = {}\n",
    "        results[chamber][\"Count\"] = {}\n",
    "        for hist in [\"Hist_1\",\"Hist_2\"]:\n",
    "            results[chamber][hist] = {}\n",
    "            results[chamber][hist][\"Bins\"] = {}\n",
    "            results[chamber][hist][\"Counts\"] = {}\n",
    "        \n",
    "    # Compute histograms for each chamber   \n",
    "    for i in [1,2,3,4]:\n",
    "        #Now we can count the number of events in each chamber\n",
    "        chamber = DF_new.filter(col(\"chamber\") == i).persist()\n",
    "        results[f\"Chamber_{i}\"][\"Count\"] = chamber.count()\n",
    "        \n",
    "        if(results[f\"Chamber_{i}\"][\"Count\"]!=0):\n",
    "            \n",
    "            #Histogram 1\n",
    "            bins, counts = (\n",
    "            chamber.select(\"TDC_CHANNEL\")\n",
    "                 .rdd.map(lambda x: x.TDC_CHANNEL)\n",
    "                 .histogram(list(np.arange(0,170,5)))\n",
    "            )\n",
    "            \n",
    "            results[f\"Chamber_{i}\"][\"Hist_1\"][\"Bins\"] = bins\n",
    "            results[f\"Chamber_{i}\"][\"Hist_1\"][\"Counts\"] = counts\n",
    "            \n",
    "            #Histogram 2\n",
    "            bins, counts = (\n",
    "            chamber.groupBy(\"TDC_CHANNEL\",\"ORBIT_CNT\")\n",
    "            .count()\n",
    "            .select(\"ORBIT_CNT\")\n",
    "            .rdd.map(lambda x: x.ORBIT_CNT)\n",
    "            .histogram(list(np.arange(6.e5,1.e7,0.5e6)))\n",
    "            )\n",
    "            \n",
    "            results[f\"Chamber_{i}\"][\"Hist_2\"][\"Bins\"] = bins\n",
    "            results[f\"Chamber_{i}\"][\"Hist_2\"][\"Counts\"] = counts            \n",
    "                \n",
    "        else:\n",
    "            #Histogram 1\n",
    "            results[f\"Chamber_{i}\"][\"Hist_1\"][\"Bins\"] = list(np.arange(0,170,5))\n",
    "            counts = list(np.arange(0,170,5)* 0) \n",
    "            results[f\"Chamber_{i}\"][\"Hist_1\"][\"Counts\"] = counts\n",
    "            \n",
    "             #Histogram 2\n",
    "            results[f\"Chamber_{i}\"][\"Hist_2\"][\"Bins\"] = list(np.arange(6.e5,1.e7,0.5e6))\n",
    "            counts = list(np.arange(6.e5,1.e7,0.5e6)* 0) \n",
    "            results[f\"Chamber_{i}\"][\"Hist_2\"][\"Counts\"] = counts\n",
    "        chamber.unpersist()\n",
    "        \n",
    "    results[\"Total Count\"] = results[\"Chamber_1\"][\"Count\"] + results[\"Chamber_2\"][\"Count\"] + \\\n",
    "                             results[\"Chamber_3\"][\"Count\"] + results[\"Chamber_4\"][\"Count\"]\n",
    "    end =time.time()\n",
    "    print(\"Time =\",end-start)\n",
    "       \n",
    "    \n",
    "    producer.send(topic=\"results\", value= str(results).encode('utf-8'))\n",
    "    producer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer\n",
    "\n",
    "#Send the results to the kafka topic\n",
    "#Initialize the producer\n",
    "producer = KafkaProducer(bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time = 2.7600951194763184\n",
      "Time = 12.605599880218506\n",
      "Time = 10.716827154159546\n",
      "Time = 12.415242433547974\n",
      "Time = 12.52517819404602\n",
      "Time = 14.638943195343018\n",
      "Time = 15.040907144546509\n",
      "Time = 12.215430736541748\n",
      "Time = 13.009719133377075\n",
      "Time = 12.16805100440979\n",
      "Time = 12.722002744674683\n",
      "Time = 12.323020935058594\n",
      "Time = 12.71977972984314\n",
      "Time = 11.915708541870117\n",
      "Time = 12.621934175491333\n",
      "Time = 14.815287590026855\n",
      "Time = 12.395026683807373\n",
      "Time = 12.24831771850586\n",
      "Time = 12.11865520477295\n",
      "Time = 14.695249557495117\n",
      "Time = 14.9103102684021\n",
      "Time = 12.375563621520996\n",
      "Time = 12.625203847885132\n",
      "Time = 12.447861671447754\n",
      "Time = 12.698059797286987\n",
      "Time = 11.619881868362427\n",
      "Time = 12.63197946548462\n",
      "Time = 12.21250319480896\n",
      "Time = 12.536131620407104\n",
      "Time = 12.289501190185547\n",
      "Time = 12.498435974121094\n",
      "Time = 12.261037588119507\n",
      "Time = 12.562913417816162\n",
      "Time = 12.243067026138306\n",
      "Time = 15.31281042098999\n",
      "Time = 14.91160535812378\n",
      "Time = 15.279619216918945\n",
      "Time = 14.741284370422363\n",
      "Time = 12.763592720031738\n",
      "Time = 12.479793310165405\n",
      "Time = 12.365573644638062\n",
      "Time = 12.29942011833191\n",
      "Time = 12.421806812286377\n",
      "Time = 12.265027523040771\n",
      "Time = 12.56100845336914\n",
      "Time = 11.319085836410522\n",
      "Time = 12.602465391159058\n",
      "Time = 12.138750553131104\n",
      "Time = 12.510503053665161\n",
      "Time = 14.675140857696533\n",
      "Time = 12.564208507537842\n",
      "Time = 12.048040628433228\n",
      "Time = 12.457059860229492\n",
      "Time = 12.249204397201538\n",
      "Time = 12.617568731307983\n",
      "Time = 11.838872194290161\n",
      "Time = 12.566187381744385\n",
      "Time = 11.908719301223755\n",
      "Time = 11.876248598098755\n",
      "Time = 11.997444152832031\n",
      "Time = 12.031829595565796\n",
      "Time = 12.08504056930542\n",
      "Time = 12.527158975601196\n",
      "Time = 14.905204772949219\n",
      "Time = 14.935423851013184\n",
      "Time = 14.813812732696533\n",
      "Time = 12.424152374267578\n",
      "Time = 12.357396125793457\n",
      "Time = 12.396037340164185\n",
      "Time = 12.053164720535278\n",
      "Time = 12.25985336303711\n",
      "Time = 11.770104885101318\n",
      "Time = 12.704593658447266\n",
      "Time = 12.293052911758423\n",
      "Time = 12.415747165679932\n",
      "Time = 11.738380670547485\n",
      "Time = 12.583708763122559\n",
      "Time = 11.976415157318115\n",
      "Time = 14.660223007202148\n",
      "Time = 17.216691732406616\n",
      "Time = 12.474776029586792\n",
      "Time = 12.271549701690674\n",
      "Time = 12.632492542266846\n",
      "Time = 11.921008825302124\n",
      "Time = 12.664763927459717\n",
      "Time = 12.08461618423462\n",
      "Time = 12.668787717819214\n",
      "Time = 12.09824275970459\n",
      "Time = 12.739937782287598\n",
      "Time = 11.561544418334961\n",
      "Time = 12.349498748779297\n",
      "Time = 12.067757844924927\n",
      "Time = 12.404552698135376\n",
      "Time = 11.906771898269653\n",
      "Time = 12.024959564208984\n",
      "Time = 11.99906873703003\n",
      "Time = 12.520452737808228\n",
      "Time = 11.785099267959595\n",
      "Time = 11.785114765167236\n",
      "Time = 12.702261686325073\n",
      "Time = 14.906960010528564\n",
      "Time = 17.73229670524597\n",
      "Time = 12.01437520980835\n",
      "Time = 12.659854173660278\n",
      "Time = 12.166839122772217\n",
      "Time = 12.313518047332764\n",
      "Time = 12.113359212875366\n",
      "Time = 12.081574440002441\n",
      "Time = 11.870083570480347\n",
      "Time = 12.287307024002075\n",
      "Time = 12.121132373809814\n",
      "Time = 12.646733283996582\n",
      "Time = 11.66208815574646\n",
      "Time = 12.544102191925049\n",
      "Time = 12.233740091323853\n",
      "Time = 12.417336225509644\n",
      "Time = 12.190422058105469\n",
      "Time = 12.026900053024292\n",
      "Time = 11.900805473327637\n",
      "Time = 12.36181378364563\n",
      "Time = 12.391449213027954\n",
      "Time = 12.49219560623169\n",
      "Time = 12.204087018966675\n",
      "Time = 15.307767152786255\n",
      "Time = 12.181447267532349\n",
      "Time = 12.075066566467285\n",
      "Time = 17.946768522262573\n",
      "Time = 14.687917947769165\n",
      "Time = 15.362326860427856\n",
      "Time = 12.27193307876587\n",
      "Time = 12.48121690750122\n",
      "Time = 12.066978693008423\n",
      "Time = 12.423476219177246\n",
      "Time = 12.328850984573364\n",
      "Time = 12.815590858459473\n",
      "Time = 11.695049524307251\n",
      "Time = 12.592737913131714\n",
      "Time = 11.98949408531189\n",
      "Time = 12.171332359313965\n",
      "Time = 11.506423950195312\n",
      "Time = 12.470346450805664\n",
      "Time = 12.157263040542603\n",
      "Time = 12.184125423431396\n",
      "Time = 12.194416046142578\n",
      "Time = 12.546546220779419\n",
      "Time = 11.618469476699829\n",
      "Time = 12.749548435211182\n",
      "Time = 11.657994985580444\n",
      "Time = 13.814513683319092\n",
      "Time = 12.566620588302612\n",
      "Time = 12.25927734375\n",
      "Time = 11.160163640975952\n",
      "Time = 11.305059671401978\n",
      "Time = 13.330312013626099\n",
      "Time = 12.399752140045166\n",
      "Time = 13.72575330734253\n",
      "Time = 12.171440601348877\n",
      "Time = 12.587382555007935\n",
      "Time = 12.378623485565186\n",
      "Time = 12.512059688568115\n",
      "Time = 12.238067388534546\n",
      "Time = 15.573952436447144\n",
      "Time = 17.28155255317688\n",
      "Time = 15.85772705078125\n",
      "Time = 12.405513048171997\n",
      "Time = 13.875413417816162\n",
      "Time = 12.1168212890625\n",
      "Time = 13.382642269134521\n",
      "Time = 12.237221002578735\n",
      "Time = 13.184324026107788\n",
      "Time = 12.30567741394043\n",
      "Time = 13.179118394851685\n",
      "Time = 12.347356081008911\n",
      "Time = 12.756082773208618\n",
      "Time = 12.548716306686401\n",
      "Time = 12.500598907470703\n",
      "Time = 15.914561748504639\n",
      "Time = 12.314483880996704\n",
      "Time = 13.224551916122437\n",
      "Time = 12.37942910194397\n",
      "Time = 13.29133129119873\n",
      "Time = 11.445998907089233\n",
      "Time = 13.286578178405762\n",
      "Time = 12.46634817123413\n",
      "Time = 13.072852849960327\n",
      "Time = 11.12088680267334\n",
      "Time = 13.157455444335938\n",
      "Time = 15.046568632125854\n",
      "Time = 18.103695154190063\n",
      "Time = 14.819859027862549\n",
      "Time = 13.704570055007935\n",
      "Time = 12.423282146453857\n",
      "Time = 12.82537055015564\n",
      "Time = 12.372178316116333\n",
      "Time = 12.881449937820435\n",
      "Time = 12.35977840423584\n",
      "Time = 15.212308883666992\n",
      "Time = 14.448322296142578\n",
      "Time = 13.503289222717285\n",
      "Time = 12.326095342636108\n",
      "Time = 12.359654903411865\n",
      "Time = 13.483076095581055\n",
      "Time = 11.348240852355957\n",
      "Time = 13.2633216381073\n",
      "Time = 11.386951208114624\n",
      "Time = 15.721890449523926\n",
      "Time = 12.420380115509033\n",
      "Time = 13.946016550064087\n",
      "Time = 12.25703501701355\n",
      "Time = 13.643890857696533\n",
      "Time = 11.363589763641357\n",
      "Time = 13.54518461227417\n",
      "Time = 12.227846384048462\n",
      "Time = 13.068897247314453\n",
      "Time = 12.234041213989258\n",
      "Time = 11.293924570083618\n",
      "Time = 13.318212032318115\n",
      "Time = 12.151617527008057\n",
      "Time = 12.527159214019775\n",
      "Time = 12.323147773742676\n",
      "Time = 13.488978147506714\n",
      "Time = 12.28681206703186\n",
      "Time = 13.564727544784546\n",
      "Time = 12.19660496711731\n",
      "Time = 13.034753799438477\n",
      "Time = 12.218786478042603\n",
      "Time = 12.927827835083008\n",
      "Time = 14.81687045097351\n",
      "Time = 18.21395707130432\n",
      "Time = 11.395803928375244\n",
      "Time = 11.696724891662598\n",
      "Time = 14.242536306381226\n",
      "Time = 11.598766803741455\n",
      "Time = 13.507357835769653\n",
      "Time = 12.210233449935913\n",
      "Time = 12.86842131614685\n",
      "Time = 12.370038270950317\n",
      "Time = 13.09925365447998\n",
      "Time = 12.523800373077393\n",
      "Time = 13.887173175811768\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-a480a03556e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcleanDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriteStream\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mforeachBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomputations_3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;34m.\u001b[0m\u001b[0mtrigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessingTime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'6 second'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/streaming.py\u001b[0m in \u001b[0;36mawaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1301\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1303\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1033\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1034\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1200\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1201\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Trigger the processing\n",
    "cleanDF.writeStream\\\n",
    "    .foreachBatch(computations_3)\\\n",
    "    .trigger(processingTime='6 second')\\\n",
    "    .start()\\\n",
    "    .awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you also want to delete any data of your local Kafka environment including any events you have created along the way, run the command:\n",
    "\n",
    "`` $ rm -rf /tmp/kafka-logs /tmp/zookeeper `` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
