{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming with Kafka and Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I try to implement a basic pipeline for the project conecting kafka with spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computer setting\n",
    "I downloaded and located in my home the spark file **spark-3.1.2-bin-hadoop3.2** and also the kafka file **kafka_2.13-2.7.0**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Kafka and Spark ready"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standalone cluster deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now initialize all the required variables with `findspark.init()` by passing the path to the spark folder we downloaded previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/usr/local/spark')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to start the master, This will spin up the spark master with address spark://localhost:7077 and a cluster dashboark at localhost:8080.\n",
    "\n",
    "We can now create a worker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the spark session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create the spark session. With the following command we are asking to the master (and resource manager) to create an application with required resources and configurations. In this case we are using all the default options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "KAFKA_BOOTSTRAP_SERVERS = ''\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"spark://master:7077\")\\\n",
    "    .appName(\"Spark Streaming\")\\\n",
    "    .config(\"spark.jars.packages\",\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.1\")\\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://master:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark Streaming</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f87730a2e80>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KAFKA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "KAFKA_HOME = '/usr/local/kafka'\n",
    "KAFKA_BOOTSTRAP_SERVERS = 'slave01:9092'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#By some reason I can't launch this from here using OS, so i open the terminals in the KAFKA_HOME folder\n",
    "# and launch the zookeper and the kafka server comands manually\n",
    "\n",
    "\n",
    "# Start Zookeeper\n",
    "# bin/zookeeper-server-start.sh config/zookeeper.properties \n",
    "#os.system('{0}/bin/zookeeper-server-start.sh {0}/config/zookeeper.properties'.format(KAFKA_HOME)) \n",
    "    \n",
    "# Start one Kafka Broker\n",
    "#bin/kafka-server-start.sh config/server.properties\n",
    "#os.system('{0}/bin/kafka-server-start.sh {0}/config/server.properties'.format(KAFKA_HOME)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the topics for kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CreateTopicsResponse_v3(throttle_time_ms=0, topic_errors=[(topic='Experiment_measurements', error_code=0, error_message=None), (topic='results', error_code=0, error_message=None)])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from kafka.admin import KafkaAdminClient, NewTopic\n",
    "kafka_admin = KafkaAdminClient(\n",
    "        bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,\n",
    "    )\n",
    "\n",
    "#Here we will inject the data\n",
    "new_topic_a = NewTopic(name='Experiment_measurements', \n",
    "                       num_partitions=1, \n",
    "                       replication_factor=1)\n",
    "\n",
    "#Here we inject the number of processed hits, post cleaning\n",
    "new_topic_b = NewTopic(name='results', \n",
    "                       num_partitions=1, \n",
    "                       replication_factor=1)\n",
    "\n",
    "kafka_admin.create_topics(new_topics=[new_topic_a,new_topic_b])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['results', 'Experiment_measurements']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kafka_admin.list_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KAFKA - SPARK INTEGRATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data from the kafka topic (define the consumer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDF = spark\\\n",
    "    .readStream\\\n",
    "    .format(\"kafka\")\\\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP_SERVERS)\\\n",
    "    .option('subscribe', 'Experiment_measurements')\\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inputDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json, col, when, sum as ssum\n",
    "from pyspark.sql.types import StructField, StructType, DoubleType, IntegerType\n",
    "\n",
    "## the schema of the json data format used to create the messages\n",
    "schema = StructType(\n",
    "        [\n",
    "                StructField(\"HEAD\",        IntegerType()),\n",
    "                StructField(\"FPGA\",        IntegerType()),\n",
    "                StructField(\"TDC_CHANNEL\", IntegerType()),\n",
    "                StructField(\"ORBIT_CNT\",   DoubleType()),\n",
    "                StructField(\"BX_COUNTER\",  IntegerType()),\n",
    "                StructField(\"TDC_MEAS\",    DoubleType())\n",
    "        ]  \n",
    ")\n",
    "\n",
    "## a new DF can be created from the previous by using the pyspark.sql functions\n",
    "jsonDF = inputDF.select(from_json(col(\"value\").alias('value').cast(\"string\"), schema).alias('value'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: struct (nullable = true)\n",
      " |    |-- HEAD: integer (nullable = true)\n",
      " |    |-- FPGA: integer (nullable = true)\n",
      " |    |-- TDC_CHANNEL: integer (nullable = true)\n",
      " |    |-- ORBIT_CNT: double (nullable = true)\n",
      " |    |-- BX_COUNTER: integer (nullable = true)\n",
      " |    |-- TDC_MEAS: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jsonDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#jsonDF.writeStream\\\n",
    "#   .outputMode(\"append\")\\\n",
    "#   .format(\"console\")\\\n",
    "#   .start()\\\n",
    "#   .awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatDF = jsonDF.selectExpr(\"value.HEAD\", \n",
    "                           \"value.FPGA\", \n",
    "                           \"value.TDC_CHANNEL\",\n",
    "                           \"value.ORBIT_CNT\",\n",
    "                           \"value.BX_COUNTER\",\n",
    "                           \"value.TDC_MEAS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- HEAD: integer (nullable = true)\n",
      " |-- FPGA: integer (nullable = true)\n",
      " |-- TDC_CHANNEL: integer (nullable = true)\n",
      " |-- ORBIT_CNT: double (nullable = true)\n",
      " |-- BX_COUNTER: integer (nullable = true)\n",
      " |-- TDC_MEAS: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flatDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flatDF.writeStream\\\n",
    "#   .outputMode(\"append\")\\\n",
    "#   .format(\"console\")\\\n",
    "#   .start()\\\n",
    "#   .awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SPARK processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "#Keep the events where \"HEAD\"=2\n",
    "cleanDF = flatDF.where(col('HEAD')==2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computations(DF, epoch):\n",
    "    #This function perform the whole operations on the received batch,\n",
    "    #\n",
    "\n",
    "    #As the 4 calculations that we have to perform are done foe each chamber we set 4 dataframes\n",
    "    chamber_1 = DF.filter((col(\"FPGA\") == 0) & (col(\"TDC_CHANNEL\")>=0) & (col(\"TDC_CHANNEL\")<=63))\n",
    "    chamber_2 = DF.filter((col(\"FPGA\") == 0) & (col(\"TDC_CHANNEL\")>=64) & (col(\"TDC_CHANNEL\")<=127))\n",
    "    chamber_3 = DF.filter((col(\"FPGA\") == 1) & (col(\"TDC_CHANNEL\")>=0) & (col(\"TDC_CHANNEL\")<=63))\n",
    "    chamber_4 = DF.filter((col(\"FPGA\") == 1) & (col(\"TDC_CHANNEL\")>=64) & (col(\"TDC_CHANNEL\")<=127))\n",
    "\n",
    "    #Now we can count the number of events in each chamber\n",
    "    n_c1 = chamber_1.count()\n",
    "    n_c2 = chamber_2.count()\n",
    "    n_c3 = chamber_3.count()\n",
    "    n_c4 = chamber_4.count()\n",
    "\n",
    "    #Total number of events\n",
    "    n = n_c1 + n_c2 + n_c3 + n_c4\n",
    "\n",
    "\n",
    "    #Histograms    \n",
    "    h_c1 = chamber_1.groupBy('TDC_CHANNEL').count().collect()\n",
    "    h_c2 = chamber_2.groupBy('TDC_CHANNEL').count().collect()\n",
    "    h_c3 = chamber_3.groupBy('TDC_CHANNEL').count().collect()\n",
    "    h_c4 = chamber_4.groupBy('TDC_CHANNEL').count().collect()\n",
    "\n",
    "    h_active_1 = chamber_1.groupBy('TDC_CHANNEL','ORBIT_CNT').count().collect()\n",
    "    h_active_2 = chamber_2.groupBy('TDC_CHANNEL','ORBIT_CNT').count().collect()\n",
    "    h_active_3 = chamber_3.groupBy('TDC_CHANNEL','ORBIT_CNT').count().collect()\n",
    "    h_active_4 = chamber_4.groupBy('TDC_CHANNEL','ORBIT_CNT').count().collect()\n",
    "    \n",
    "    \n",
    "    #Organise the results to send them to one topic as a dictionary\n",
    "    results = {'Total_events': n,\n",
    "              'Events_per_chamber': [n_c1,n_c2,n_c3,n_c4],\n",
    "              'Histogram_1': [h_c1, h_c2, h_c3, h_c4],\n",
    "              'Histogram_2': [h_active_1,h_active_2,h_active_3,h_active_4]}\n",
    "    \n",
    "    #publish the results in the \"results\" topic for further usage\n",
    "    producer.send(topic='results', value=json.dumps(results).encode('utf-8'))\n",
    "    producer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computations_2(DF, epoch):\n",
    "    start=time.time()\n",
    "    #This function perform the whole operations on the received batch,\n",
    "    #\n",
    "\n",
    "    #As the 4 calculations that we have to perform are done foe each chamber we set 4 dataframes\n",
    "    chamber_1 = DF.filter((col(\"FPGA\") == 0) & (col(\"TDC_CHANNEL\")>=0) & (col(\"TDC_CHANNEL\")<=63))\n",
    "    chamber_2 = DF.filter((col(\"FPGA\") == 0) & (col(\"TDC_CHANNEL\")>=64) & (col(\"TDC_CHANNEL\")<=127))\n",
    "    chamber_3 = DF.filter((col(\"FPGA\") == 1) & (col(\"TDC_CHANNEL\")>=0) & (col(\"TDC_CHANNEL\")<=63))\n",
    "    chamber_4 = DF.filter((col(\"FPGA\") == 1) & (col(\"TDC_CHANNEL\")>=64) & (col(\"TDC_CHANNEL\")<=127))\n",
    "\n",
    "    #Initialize results dictionary\n",
    "    results = {}\n",
    "    results[\"Total Count\"] = {}\n",
    "    chamber_name = [\"Chamber_1\", \"Chamber_2\", \"Chamber_3\", \"Chamber_4\"]\n",
    "    for chamber in chamber_name:\n",
    "        results[chamber] = {}\n",
    "        results[chamber][\"Count\"] = {}\n",
    "        \n",
    "    #Now we can count the number of events in each chamber\n",
    "    results[\"Chamber_1\"][\"Count\"] = chamber_1.count()\n",
    "    results[\"Chamber_2\"][\"Count\"] = chamber_2.count()\n",
    "    results[\"Chamber_3\"][\"Count\"] = chamber_3.count()\n",
    "    results[\"Chamber_4\"][\"Count\"] = chamber_4.count()\n",
    "\n",
    "    #Total number of events\n",
    "    results[\"Total Count\"] = results[\"Chamber_1\"][\"Count\"] + results[\"Chamber_2\"][\"Count\"] + \\\n",
    "                             results[\"Chamber_3\"][\"Count\"] + results[\"Chamber_4\"][\"Count\"]\n",
    "    \n",
    "\n",
    "    # Compute histograms for each chamber   \n",
    "    i=0    \n",
    "    for chamber in [chamber_1, chamber_2, chamber_3, chamber_4]:\n",
    "        #Initialize dictionary partitions to save the results\n",
    "        for hist in [\"Hist_1\",\"Hist_2\"]:\n",
    "            results[chamber_name[i]][hist] = {}\n",
    "            results[chamber_name[i]][hist][\"Bins\"] = {}\n",
    "            results[chamber_name[i]][hist][\"Counts\"] = {}\n",
    "        \n",
    "        if(chamber.count()!=0): \n",
    "            #Histogram 1\n",
    "            bins, counts = (\n",
    "                chamber.select(\"TDC_CHANNEL\")\n",
    "                .rdd.map(lambda x: x.TDC_CHANNEL)\n",
    "                .histogram(list(np.arange(0,170,5)))\n",
    "            )\n",
    "            \n",
    "            results[chamber_name[i]][\"Hist_1\"][\"Bins\"] = bins\n",
    "            results[chamber_name[i]][\"Hist_1\"][\"Counts\"] = counts\n",
    "            \n",
    "            #Histogram 2\n",
    "            bins, counts = (\n",
    "            chamber.groupBy(\"TDC_CHANNEL\",\"ORBIT_CNT\")\n",
    "            .count()\n",
    "            .select(\"ORBIT_CNT\")\n",
    "            .rdd.map(lambda x: x.ORBIT_CNT)\n",
    "            .histogram(list(np.arange(6.e5,1.e7,0.5e6)))\n",
    "            )\n",
    "            \n",
    "            results[chamber_name[i]][\"Hist_2\"][\"Bins\"] = bins\n",
    "            results[chamber_name[i]][\"Hist_2\"][\"Counts\"] = counts            \n",
    "            \n",
    "        else:\n",
    "            #Histogram 1\n",
    "            results[chamber_name[i]][\"Hist_1\"][\"Bins\"] = list(np.arange(0,170,5))\n",
    "            counts = list(np.arange(0,170,5)* 0) \n",
    "            results[chamber_name[i]][\"Hist_1\"][\"Counts\"] = counts\n",
    "            \n",
    "            #Histogram 2\n",
    "            results[chamber_name[i]][\"Hist_2\"][\"Bins\"] = list(np.arange(6.e5,1.e7,0.5e6))\n",
    "            counts = list(np.arange(6.e5,1.e7,0.5e6)* 0) \n",
    "            results[chamber_name[i]][\"Hist_2\"][\"Counts\"] = counts\n",
    "        i +=1\n",
    "    \n",
    "    end =time.time()\n",
    "    print(\"Time =\",end-start)\n",
    "    producer.send(topic=\"results\", value= str(results).encode('utf-8'))\n",
    "    producer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computations_3(DF, epoch):\n",
    "    start=time.time()\n",
    "    #This function perform the whole operations on the received batch,\n",
    "    \n",
    "    #Add a column with the chamber number\n",
    "    DF_new = DF.filter(col(\"HEAD\")==2).withColumn('chamber',when((col(\"FPGA\") == 0) & (col(\"TDC_CHANNEL\")<=63),1).\n",
    "                                 when((col(\"FPGA\") == 0) & (col(\"TDC_CHANNEL\")>=64),2).\n",
    "                                 when((col(\"FPGA\") == 1) & (col(\"TDC_CHANNEL\")<=63),3).\n",
    "                                 when((col(\"FPGA\") == 1) & (col(\"TDC_CHANNEL\")>=64),4)).\\\n",
    "                                 select([ col('TDC_CHANNEL'), col('ORBIT_CNT'),\n",
    "                                    col('BX_COUNTER'),col('TDC_MEAS'),\n",
    "                                    col('chamber')])\n",
    "    #DF_new.persist()\n",
    "    #DF_new.show()\n",
    "    #Initialize results dictionary\n",
    "    results = {}\n",
    "    results[\"Total Count\"] = {}\n",
    "    results[\"Index\"] = time.time()\n",
    "    chamber_name = [\"Chamber_1\", \"Chamber_2\", \"Chamber_3\", \"Chamber_4\"]\n",
    "    for chamber in chamber_name:\n",
    "        results[chamber] = {}\n",
    "        results[chamber][\"Count\"] = {}\n",
    "        for hist in [\"Hist_1\",\"Hist_2\"]:\n",
    "            results[chamber][hist] = {}\n",
    "            results[chamber][hist][\"Bins\"] = {}\n",
    "            results[chamber][hist][\"Counts\"] = {}\n",
    "        \n",
    "    # Compute histograms for each chamber   \n",
    "    for i in [1,2,3,4]:\n",
    "        #Now we can count the number of events in each chamber\n",
    "        chamber = DF_new.filter(col(\"chamber\") == i).persist()\n",
    "        results[f\"Chamber_{i}\"][\"Count\"] = chamber.count()\n",
    "        \n",
    "        if(results[f\"Chamber_{i}\"][\"Count\"]!=0):\n",
    "            \n",
    "            #Histogram 1\n",
    "            bins, counts = (\n",
    "            chamber.select(\"TDC_CHANNEL\")\n",
    "                 .rdd.map(lambda x: x.TDC_CHANNEL)\n",
    "                 .histogram(list(np.arange(0,170,5)))\n",
    "            )\n",
    "            \n",
    "            results[f\"Chamber_{i}\"][\"Hist_1\"][\"Bins\"] = bins\n",
    "            results[f\"Chamber_{i}\"][\"Hist_1\"][\"Counts\"] = counts\n",
    "            \n",
    "            #Histogram 2\n",
    "            bins, counts = (\n",
    "            chamber.groupBy(\"TDC_CHANNEL\",\"ORBIT_CNT\")\n",
    "            .count()\n",
    "            .select(\"ORBIT_CNT\")\n",
    "            .rdd.map(lambda x: x.ORBIT_CNT)\n",
    "            .histogram(list(np.arange(6.e5,1.e7,0.5e6)))\n",
    "            )\n",
    "            \n",
    "            results[f\"Chamber_{i}\"][\"Hist_2\"][\"Bins\"] = bins\n",
    "            results[f\"Chamber_{i}\"][\"Hist_2\"][\"Counts\"] = counts            \n",
    "                \n",
    "        else:\n",
    "            #Histogram 1\n",
    "            results[f\"Chamber_{i}\"][\"Hist_1\"][\"Bins\"] = list(np.arange(0,170,5))\n",
    "            counts = list(np.arange(0,170,5)* 0) \n",
    "            results[f\"Chamber_{i}\"][\"Hist_1\"][\"Counts\"] = counts\n",
    "            \n",
    "             #Histogram 2\n",
    "            results[f\"Chamber_{i}\"][\"Hist_2\"][\"Bins\"] = list(np.arange(6.e5,1.e7,0.5e6))\n",
    "            counts = list(np.arange(6.e5,1.e7,0.5e6)* 0) \n",
    "            results[f\"Chamber_{i}\"][\"Hist_2\"][\"Counts\"] = counts\n",
    "        chamber.unpersist()\n",
    "        \n",
    "    results[\"Total Count\"] = results[\"Chamber_1\"][\"Count\"] + results[\"Chamber_2\"][\"Count\"] + \\\n",
    "                             results[\"Chamber_3\"][\"Count\"] + results[\"Chamber_4\"][\"Count\"]\n",
    "    end =time.time()\n",
    "    print(\"Time =\",end-start)\n",
    "       \n",
    "    \n",
    "    producer.send(topic=\"results\", value= str(results).encode('utf-8'))\n",
    "    #producer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computations_4(df, epoch):\n",
    "    start=time.time()\n",
    "\n",
    "    ## TOTAL NUMBER OF PROCESSED HITS\n",
    "    clean_df = df.filter(df.HEAD == 2)\n",
    "    total_hits = clean_df.count()\n",
    "    \n",
    "    ## CHAMBER FILTERING\n",
    "    c_fp = clean_df.filter(clean_df.FPGA == 0)\n",
    "    c_ga = clean_df.filter(clean_df.FPGA == 1)\n",
    "    c_0 = c_fp.filter(c_fp.TDC_CHANNEL < 64)\n",
    "    c_1 = c_fp.filter(c_fp.TDC_CHANNEL >= 64)\n",
    "    c_2 = c_ga.filter(c_fp.TDC_CHANNEL < 64)\n",
    "    c_3 = c_ga.filter(c_fp.TDC_CHANNEL >= 64)\n",
    "    \n",
    "    ## TOTAL NUMBER OF PROCESSED HITS PER CHAMBER\n",
    "    hits_0 = c_0.count()\n",
    "    hits_1 = c_1.count()\n",
    "    hits_2 = c_2.count()\n",
    "    hits_3 = c_3.count()\n",
    "    \n",
    "    ## ACTIVE TDC_CHANNEL PER CHAMBER\n",
    "    hist_0 = c_0.groupBy('TDC_CHANNEL').count().select('TDC_CHANNEL',col('count').alias('COUNT')).collect()\n",
    "    hist_1 = c_1.groupBy('TDC_CHANNEL').count().select('TDC_CHANNEL',col('count').alias('COUNT')).collect()\n",
    "    hist_2 = c_2.groupBy('TDC_CHANNEL').count().select('TDC_CHANNEL',col('count').alias('COUNT')).collect()\n",
    "    hist_3 = c_3.groupBy('TDC_CHANNEL').count().select('TDC_CHANNEL',col('count').alias('COUNT')).collect()\n",
    "    \n",
    "    ## ACTIVE TDC_CHANNEL PER CHAMBER PER ORBIT_CNT\n",
    "    orb_0 = c_0.groupBy('TDC_CHANNEL','ORBIT_CNT').count().select('TDC_CHANNEL','ORBIT_CNT',col('count').alias('COUNT')).collect()\n",
    "    orb_1 = c_1.groupBy('TDC_CHANNEL','ORBIT_CNT').count().select('TDC_CHANNEL','ORBIT_CNT',col('count').alias('COUNT')).collect()\n",
    "    orb_2 = c_2.groupBy('TDC_CHANNEL','ORBIT_CNT').count().select('TDC_CHANNEL','ORBIT_CNT',col('count').alias('COUNT')).collect()\n",
    "    orb_3 = c_3.groupBy('TDC_CHANNEL','ORBIT_CNT').count().select('TDC_CHANNEL','ORBIT_CNT',col('count').alias('COUNT')).collect()\n",
    "\n",
    "    end =time.time()\n",
    "    print(\"Time =\",end-start)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "def computations_5(df, epoch):\n",
    "    start=time.time()\n",
    "\n",
    "    ## FILTERING DATA AND SETTING CHAMBER\n",
    "    clean_df = df.filter(col(\"HEAD\")==2).withColumn('CHAMBER',\n",
    "                                when(col(\"FPGA\") == 0,\n",
    "                                     when(col(\"TDC_CHANNEL\")<=63,1).\\\n",
    "                                     otherwise(2)).\\\n",
    "                                                    otherwise(\n",
    "                                    when(col(\"TDC_CHANNEL\")<=63,3).\\\n",
    "                                    otherwise(4)\n",
    "                                )).\\\n",
    "                                select([ col('TDC_CHANNEL'), col('ORBIT_CNT'),\n",
    "                                   col('BX_COUNTER'),col('TDC_MEAS'),\n",
    "                                   col('CHAMBER')])\n",
    "\n",
    "    ## TOTAL NUMBER OF PROCESSED HITS\n",
    "    total_hits = clean_df.count()\n",
    "\n",
    "    ## TOTAL NUMBER OF PROCESSED HITS PER CHAMBER\n",
    "    chamber_hits = clean_df.groupBy('CHAMBER').count().select(col('CHAMBER'),col('count').alias('COUNT'))#.collect()\n",
    "    \n",
    "    ## ACTIVE TDC_CHANNEL PER CHAMBER\n",
    "    hist_1 = clean_df.groupBy('CHAMBER','TDC_CHANNEL').count().select('CHAMBER','TDC_CHANNEL',col('count').alias('COUNT'))#.collect()\n",
    "\n",
    "    ## ACTIVE TDC_CHANNEL PER CHAMBER PER ORBIT_CNT\n",
    "    hist_2 = clean_df\\\n",
    "        .groupBy('CHAMBER','ORBIT_CNT')\\\n",
    "        .agg(F.countDistinct('TDC_CHANNEL')\\\n",
    "        .alias('ACTIVE_CHANNELS'))#.collect()\n",
    "\n",
    "    ## COLLECTING RESULTS\n",
    "    _chamber_hits = chamber_hits.collect()\n",
    "    \n",
    "    _hist_1 = hist_1.groupBy('CHAMBER').agg(\n",
    "    F.map_from_entries(\n",
    "        F.collect_list(\n",
    "            F.struct(\"TDC_CHANNEL\", \"COUNT\"))).alias(\"COUNT\")\n",
    "        ).collect()\n",
    "\n",
    "    _hist_2 = hist_2.groupBy('CHAMBER').agg(\n",
    "    F.map_from_entries(\n",
    "        F.collect_list(\n",
    "            F.struct(\"ORBIT_CNT\",\"ACTIVE_CHANNELS\"))).alias(\"COUNT\")\n",
    "        ).collect()\n",
    "\n",
    "    ## JSON FORMATING OF RESULTS\n",
    "    _hist_1_dict = {row.CHAMBER: row.COUNT for row in _hist_1}\n",
    "\n",
    "    _hist_2_dict = {row.CHAMBER: row.COUNT for row in _hist_2}\n",
    "\n",
    "    results = {f'Chamber_{row.CHAMBER}': {\n",
    "        'Count': int(row.COUNT),\n",
    "        'Hist_1': _hist_1_dict[row.CHAMBER],\n",
    "        'Hist_2': _hist_2_dict[row.CHAMBER]} for row in _chamber_hits}\n",
    "\n",
    "    results.update({\n",
    "        'Index': time.time(),#TODO: Better indexing\n",
    "        'Total Count': int(total_hits)\n",
    "    })\n",
    "\n",
    "    end = time.time()\n",
    "    print(\"Time =\",end-start)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computations_6(df, epoch):\n",
    "    start=time.time()\n",
    "\n",
    "    ## TOTAL NUMBER OF PROCESSED HITS\n",
    "    clean_df = df.filter(col(\"HEAD\")==2).withColumn('CHAMBER',\n",
    "                                when(col(\"FPGA\") == 0,\n",
    "                                     when(col(\"TDC_CHANNEL\")<=63,1).\\\n",
    "                                     otherwise(2)).\\\n",
    "                                                    otherwise(\n",
    "                                    when(col(\"TDC_CHANNEL\")<=63,3).\\\n",
    "                                    otherwise(4)\n",
    "                                )).\\\n",
    "                                select([ col('TDC_CHANNEL'), col('ORBIT_CNT'),\n",
    "                                   col('BX_COUNTER'),col('TDC_MEAS'),\n",
    "                                   col('CHAMBER')])\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    # TOTAL NUMBER OF PROCESSED HITS\n",
    "    total_hits = clean_df.count()\n",
    "    results = {'total_hits': int(total_hits), 'chambers': {f'chamber_{i}': {} for i in range(1,5)}}\n",
    "    \n",
    "    ## This is the most general grouping, from it we will count the other groupings\n",
    "    ## ACTIVE TDC_CHANNEL PER CHAMBER PER ORBIT_CNT\n",
    "    hist_2 = clean_df.groupBy('CHAMBER','TDC_CHANNEL','ORBIT_CNT').count().select('CHAMBER','TDC_CHANNEL','ORBIT_CNT',col('count').alias('COUNT'))#.collect()\n",
    "    \n",
    "    ## ACTIVE TDC_CHANNEL PER CHAMBER\n",
    "    hist_1 = hist_2.groupby('CHAMBER','TDC_CHANNEL').agg(ssum('COUNT')).select('CHAMBER','TDC_CHANNEL',col('sum(COUNT)').alias('COUNT'))#.collect()\n",
    "\n",
    "    ## TOTAL NUMBER OF PROCESSED HITS PER CHAMBER\n",
    "    chamber_hits = hist_1.groupBy('CHAMBER').agg(ssum('COUNT')).select(col('CHAMBER'),col('sum(COUNT)').alias('COUNT'))#.collect()\n",
    "\n",
    "    hist_2.persist()\n",
    "    hist_1.persist()\n",
    "\n",
    "    _chamber_hits = chamber_hits.collect()\n",
    "    \n",
    "    _hist_1 = hist_1.groupBy('CHAMBER').agg(\n",
    "    F.map_from_entries(\n",
    "        F.collect_list(\n",
    "            F.struct(\"TDC_CHANNEL\", \"COUNT\"))).alias(\"distribution\")\n",
    "        ).collect()\n",
    "\n",
    "    _hist_2 = hist_2.groupBy('CHAMBER').agg(\n",
    "    F.map_from_entries(\n",
    "        F.collect_list(\n",
    "            F.struct(\n",
    "                F.concat_ws('_',\"ORBIT_CNT\",\"TDC_CHANNEL\"), \n",
    "                \"COUNT\"))).alias(\"distribution\")\n",
    "        ).collect()\n",
    "    \n",
    "#    hist_2.unpersist()\n",
    "#    hist_1.unpersist()\n",
    "    \n",
    "    end =time.time()\n",
    "    print(\"Time =\",end-start)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computations_8(df, epoch):\n",
    "    start=time.time()\n",
    "\n",
    "    ## FILTERING DATA AND SETTING CHAMBER\n",
    "    clean_df = df.filter(col(\"HEAD\")==2).withColumn('CHAMBER',\n",
    "                                when(col(\"FPGA\") == 0,\n",
    "                                     when(col(\"TDC_CHANNEL\")<=63,1).\\\n",
    "                                     otherwise(2)).\\\n",
    "                                                    otherwise(\n",
    "                                    when(col(\"TDC_CHANNEL\")<=63,3).\\\n",
    "                                    otherwise(4)\n",
    "                                )).\\\n",
    "                                select([ col('TDC_CHANNEL'), col('ORBIT_CNT'),\n",
    "                                   col('BX_COUNTER'),col('TDC_MEAS'),\n",
    "                                   col('CHAMBER')])\n",
    "\n",
    "    ## TOTAL NUMBER OF PROCESSED HITS\n",
    "    total_hits = clean_df.count()\n",
    "    if not total_hits: return\n",
    "\n",
    "    ## TOTAL NUMBER OF PROCESSED HITS PER CHAMBER\n",
    "    chamber_hits = clean_df\\\n",
    "        .groupBy('CHAMBER').count()\\\n",
    "        .select(col('CHAMBER'),col('count').alias('COUNT'))#.collect()\n",
    "    \n",
    "    ## ACTIVE TDC_CHANNEL PER CHAMBER\n",
    "    min_v_1 = 0\n",
    "    max_v_1 = 170\n",
    "    inc_1 = 5\n",
    "    hist_1_bins = np.arange(min_v_1,max_v_1,inc_1)\n",
    "    hist_1 = clean_df\\\n",
    "        .filter((min_v_1<=F.col('TDC_CHANNEL')) & (F.col('TDC_CHANNEL')<=max_v_1))\\\n",
    "        .withColumn('BIN', F.floor((F.col('TDC_CHANNEL')-min_v_1)/inc_1))\\\n",
    "        .groupBy('CHAMBER','BIN')\\\n",
    "        .count().select('CHAMBER','BIN', col('count').alias('COUNT'))\n",
    "\n",
    "    ## ACTIVE TDC_CHANNEL PER CHAMBER PER ORBIT_CNT\n",
    "    min_v_2 = 6.e5#clean_df.agg(F.min(F.col('ORBIT_CNT')).alias('min')).collect()[-1].min\n",
    "    max_v_2 = 1.e7#clean_df.agg(F.max(F.col('ORBIT_CNT')).alias('max')).collect()[-1].max\n",
    "    inc_2 = 0.5e6\n",
    "    hist_2_bins = np.arange(min_v_2,max_v_2,inc_2)\n",
    "    hist_2 = clean_df\\\n",
    "        .groupBy('CHAMBER','ORBIT_CNT')\\\n",
    "        .agg(F.countDistinct('TDC_CHANNEL').alias('ACTIVE_CHANNELS'))\\\n",
    "        .filter((min_v_2<=F.col('ORBIT_CNT'))&(F.col('ORBIT_CNT')<=max_v_2))\\\n",
    "        .withColumn('BIN',F.floor((F.col('ORBIT_CNT')-min_v_2)/inc_2))\\\n",
    "        .groupBy('CHAMBER','BIN')\\\n",
    "        .agg(F.sum('ACTIVE_CHANNELS').alias('COUNT'))#.collect()\n",
    "\n",
    "    ## COLLECTING RESULTS\n",
    "    _chamber_hits = chamber_hits.collect()\n",
    "    \n",
    "    _hist_1 = hist_1.groupBy('CHAMBER').agg(\n",
    "    F.map_from_entries(\n",
    "        F.collect_list(\n",
    "            F.struct(\"BIN\", \"COUNT\"))).alias(\"COUNT\")\n",
    "        ).collect()\n",
    "\n",
    "    _hist_2 = hist_2.groupBy('CHAMBER').agg(\n",
    "    F.map_from_entries(\n",
    "        F.collect_list(\n",
    "            F.struct(\"BIN\",\"COUNT\"))).alias(\"COUNT\")\n",
    "        ).collect()\n",
    "\n",
    "    ## NUMPIFY RESULTS\n",
    "    def numpify(bins, pos_count):\n",
    "        counter = np.zeros(len(bins))#np.zeros(len(bins)-1)?\n",
    "        positions = np.array(list(pos_count.keys()))\n",
    "        counts = np.array(list(pos_count.values()))\n",
    "        counter[positions] = counts\n",
    "        return counter\n",
    "\n",
    "    ## JSON FORMATING OF RESULTS\n",
    "    _hist_1_dict = {row.CHAMBER: {\n",
    "        'Bins': list(hist_1_bins), 'Counts': list(numpify(hist_1_bins,row.COUNT))\n",
    "    } for row in _hist_1}\n",
    "\n",
    "    _hist_2_dict = {row.CHAMBER: {\n",
    "        'Bins': list(hist_2_bins), 'Counts': list(numpify(hist_2_bins,row.COUNT))\n",
    "    } for row in _hist_2}\n",
    "\n",
    "    results = {f'Chamber_{row.CHAMBER}': {\n",
    "        'Count': int(row.COUNT),\n",
    "        'Hist_1': _hist_1_dict[row.CHAMBER],\n",
    "        'Hist_2': _hist_2_dict[row.CHAMBER]} for row in _chamber_hits}\n",
    "\n",
    "    results.update({\n",
    "        'Index': time.time(),#TODO: Better indexing\n",
    "        'Total Count': int(total_hits)\n",
    "    })\n",
    "\n",
    "    end = time.time()\n",
    "    print(\"Time =\",end-start)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer\n",
    "\n",
    "#Send the results to the kafka topic\n",
    "#Initialize the producer\n",
    "producer = KafkaProducer(bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS)#, value_serializer=lambda x: json.dumps(x).encode('utf-8'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time = 2.227421522140503\n",
      "Time = 2.7635810375213623\n",
      "Time = 3.3613486289978027\n",
      "Time = 3.5853147506713867\n",
      "Time = 3.3991353511810303\n",
      "Time = 3.7051334381103516\n",
      "Time = 3.2248916625976562\n",
      "Time = 3.5056025981903076\n",
      "Time = 3.3946402072906494\n",
      "Time = 4.933444023132324\n",
      "Time = 2.993786573410034\n",
      "Time = 3.6017584800720215\n",
      "Time = 3.2536540031433105\n",
      "Time = 4.564283132553101\n",
      "Time = 3.215409517288208\n",
      "Time = 3.6412529945373535\n",
      "Time = 3.0823898315429688\n",
      "Time = 4.491344928741455\n",
      "Time = 3.2127554416656494\n",
      "Time = 3.444035530090332\n",
      "Time = 3.084981679916382\n",
      "Time = 3.4713096618652344\n",
      "Time = 3.0603010654449463\n",
      "Time = 4.366866588592529\n",
      "Time = 2.893354892730713\n",
      "Time = 3.3169102668762207\n",
      "Time = 3.273897171020508\n",
      "Time = 3.5856244564056396\n",
      "Time = 2.873936653137207\n",
      "Time = 3.195481300354004\n",
      "Time = 2.976933002471924\n",
      "Time = 3.3591108322143555\n",
      "Time = 2.9608500003814697\n",
      "Time = 3.337040901184082\n",
      "Time = 2.9769084453582764\n",
      "Time = 3.463216781616211\n",
      "Time = 2.828148365020752\n",
      "Time = 3.2497658729553223\n",
      "Time = 2.914550304412842\n",
      "Time = 4.211586952209473\n",
      "Time = 2.877408981323242\n",
      "Time = 3.2233171463012695\n",
      "Time = 2.9447216987609863\n",
      "Time = 4.2245519161224365\n",
      "Time = 3.0038843154907227\n",
      "Time = 3.142428159713745\n",
      "Time = 3.6940219402313232\n",
      "Time = 3.977800130844116\n",
      "Time = 3.306948661804199\n",
      "Time = 3.7327611446380615\n",
      "Time = 2.841461658477783\n",
      "Time = 3.09330415725708\n",
      "Time = 2.916572332382202\n",
      "Time = 3.5502989292144775\n",
      "Time = 2.9414031505584717\n",
      "Time = 3.4839158058166504\n",
      "Time = 3.0975539684295654\n",
      "Time = 3.4134624004364014\n",
      "Time = 2.989241361618042\n",
      "Time = 3.3776156902313232\n",
      "Time = 3.062546730041504\n",
      "Time = 3.1963813304901123\n",
      "Time = 3.258896589279175\n",
      "Time = 3.3794422149658203\n",
      "Time = 3.097700834274292\n",
      "Time = 3.516629219055176\n",
      "Time = 3.043616533279419\n",
      "Time = 3.259002447128296\n",
      "Time = 2.779802083969116\n",
      "Time = 3.299233913421631\n",
      "Time = 3.09560227394104\n",
      "Time = 3.377758741378784\n",
      "Time = 3.077971935272217\n",
      "Time = 3.2980282306671143\n",
      "Time = 2.9469683170318604\n",
      "Time = 3.3447983264923096\n",
      "Time = 3.076244354248047\n",
      "Time = 3.4303462505340576\n",
      "Time = 3.2271411418914795\n",
      "Time = 3.4128787517547607\n",
      "Time = 3.4807934761047363\n",
      "Time = 3.814821720123291\n",
      "Time = 3.2510695457458496\n",
      "Time = 4.734241247177124\n",
      "Time = 3.4943337440490723\n",
      "Time = 4.039783954620361\n",
      "Time = 3.2782270908355713\n",
      "Time = 3.4707159996032715\n",
      "Time = 2.990008592605591\n",
      "Time = 3.2866697311401367\n",
      "Time = 3.252173662185669\n",
      "Time = 3.3350789546966553\n",
      "Time = 3.1378824710845947\n",
      "Time = 3.299727439880371\n",
      "Time = 2.9654042720794678\n",
      "Time = 4.375105142593384\n",
      "Time = 3.1386725902557373\n",
      "Time = 3.2932608127593994\n",
      "Time = 3.18992280960083\n",
      "Time = 3.217410087585449\n",
      "Time = 2.902296543121338\n",
      "Time = 3.36460018157959\n",
      "Time = 3.0560014247894287\n",
      "Time = 3.190375566482544\n",
      "Time = 2.9797463417053223\n",
      "Time = 4.314643383026123\n",
      "Time = 3.1634268760681152\n",
      "Time = 3.399568796157837\n",
      "Time = 3.2443370819091797\n",
      "Time = 3.5640957355499268\n",
      "Time = 3.0593326091766357\n",
      "Time = 3.500704050064087\n",
      "Time = 3.235058307647705\n",
      "Time = 3.295264959335327\n",
      "Time = 3.190532684326172\n",
      "Time = 3.238534688949585\n",
      "Time = 2.998155355453491\n",
      "Time = 3.2139670848846436\n",
      "Time = 2.840440511703491\n",
      "Time = 3.2140748500823975\n",
      "Time = 2.8933932781219482\n",
      "Time = 4.126685857772827\n",
      "Time = 3.0634055137634277\n",
      "Time = 3.1910665035247803\n",
      "Time = 2.9959354400634766\n",
      "Time = 4.345240116119385\n",
      "Time = 3.1190149784088135\n",
      "Time = 4.596899032592773\n",
      "Time = 3.0630476474761963\n",
      "Time = 3.5087625980377197\n",
      "Time = 2.820636749267578\n",
      "Time = 3.308972120285034\n",
      "Time = 3.0829758644104004\n",
      "Time = 3.3002820014953613\n",
      "Time = 3.1294543743133545\n",
      "Time = 3.4719974994659424\n",
      "Time = 2.8744871616363525\n",
      "Time = 4.093477725982666\n",
      "Time = 3.158743143081665\n",
      "Time = 3.4810593128204346\n",
      "Time = 3.1933858394622803\n",
      "Time = 3.358973264694214\n",
      "Time = 3.078500509262085\n",
      "Time = 3.3300273418426514\n",
      "Time = 3.0735321044921875\n",
      "Time = 3.4688544273376465\n",
      "Time = 3.0197746753692627\n",
      "Time = 4.470308780670166\n",
      "Time = 3.0712764263153076\n",
      "Time = 3.4244258403778076\n",
      "Time = 2.800940990447998\n",
      "Time = 3.1458306312561035\n",
      "Time = 2.899136781692505\n",
      "Time = 3.437286138534546\n",
      "Time = 2.8998661041259766\n",
      "Time = 3.6039698123931885\n",
      "Time = 2.890166997909546\n",
      "Time = 4.121148109436035\n",
      "Time = 2.8738765716552734\n",
      "Time = 3.3268613815307617\n",
      "Time = 2.778902769088745\n",
      "Time = 3.4791433811187744\n",
      "Time = 2.8498470783233643\n",
      "Time = 3.2260191440582275\n",
      "Time = 3.175647020339966\n",
      "Time = 3.269472599029541\n",
      "Time = 2.957899808883667\n",
      "Time = 3.235105276107788\n",
      "Time = 3.2196805477142334\n",
      "Time = 4.726861476898193\n",
      "Time = 3.2892537117004395\n",
      "Time = 4.858890533447266\n",
      "Time = 3.3182194232940674\n",
      "Time = 3.4593377113342285\n",
      "Time = 3.3348259925842285\n",
      "Time = 4.8873186111450195\n",
      "Time = 3.006551504135132\n",
      "Time = 4.575664043426514\n",
      "Time = 3.0637900829315186\n",
      "Time = 3.408252239227295\n",
      "Time = 3.0986955165863037\n",
      "Time = 3.3685216903686523\n",
      "Time = 3.1975185871124268\n",
      "Time = 3.3941032886505127\n",
      "Time = 3.1106979846954346\n",
      "Time = 4.7285473346710205\n",
      "Time = 3.086458683013916\n",
      "Time = 4.571599245071411\n",
      "Time = 3.070467710494995\n",
      "Time = 3.4383671283721924\n",
      "Time = 3.0376780033111572\n",
      "Time = 3.594207763671875\n",
      "Time = 3.15006947517395\n",
      "Time = 3.5093536376953125\n",
      "Time = 3.0042617321014404\n",
      "Time = 4.470433235168457\n",
      "Time = 3.1050424575805664\n",
      "Time = 3.21720027923584\n",
      "Time = 2.9154231548309326\n",
      "Time = 3.2126433849334717\n",
      "Time = 3.2577760219573975\n",
      "Time = 3.4099485874176025\n",
      "Time = 3.0859832763671875\n",
      "Time = 3.172901153564453\n",
      "Time = 2.885413646697998\n",
      "Time = 4.102615833282471\n",
      "Time = 3.047135591506958\n",
      "Time = 4.626221179962158\n",
      "Time = 2.83905029296875\n",
      "Time = 3.4338626861572266\n",
      "Time = 3.0417633056640625\n",
      "Time = 3.2209224700927734\n",
      "Time = 3.0133273601531982\n",
      "Time = 3.2522268295288086\n",
      "Time = 3.0091137886047363\n",
      "Time = 3.2796661853790283\n",
      "Time = 3.0271685123443604\n",
      "Time = 3.549525022506714\n",
      "Time = 3.03527569770813\n",
      "Time = 4.42286229133606\n",
      "Time = 2.9385173320770264\n",
      "Time = 3.5077199935913086\n",
      "Time = 3.0525267124176025\n",
      "Time = 3.2140953540802\n",
      "Time = 3.5120601654052734\n",
      "Time = 3.54536771774292\n",
      "Time = 3.203038215637207\n",
      "Time = 4.702743053436279\n",
      "Time = 3.249765634536743\n",
      "Time = 3.3939740657806396\n",
      "Time = 3.3000667095184326\n",
      "Time = 3.5640599727630615\n",
      "Time = 3.133880615234375\n",
      "Time = 3.4963369369506836\n",
      "Time = 3.19722580909729\n",
      "Time = 4.828230857849121\n",
      "Time = 3.191117525100708\n",
      "Time = 3.524540662765503\n",
      "Time = 3.3337111473083496\n",
      "Time = 3.458047866821289\n",
      "Time = 2.9104344844818115\n",
      "Time = 3.32509708404541\n",
      "Time = 2.7978427410125732\n",
      "Time = 3.339569568634033\n",
      "Time = 3.1111738681793213\n",
      "Time = 3.240366220474243\n",
      "Time = 2.843888759613037\n",
      "Time = 3.4694573879241943\n",
      "Time = 2.895571708679199\n",
      "Time = 3.42875337600708\n",
      "Time = 3.1564953327178955\n",
      "Time = 4.597921133041382\n",
      "Time = 2.89831805229187\n",
      "Time = 3.1004254817962646\n",
      "Time = 2.8858673572540283\n",
      "Time = 4.14384913444519\n",
      "Time = 2.9299511909484863\n",
      "Time = 3.497997283935547\n",
      "Time = 2.970144510269165\n",
      "Time = 3.4336602687835693\n",
      "Time = 3.074657917022705\n",
      "Time = 3.6053388118743896\n",
      "Time = 3.1693673133850098\n",
      "Time = 4.7522971630096436\n",
      "Time = 2.9767119884490967\n",
      "Time = 3.720292091369629\n",
      "Time = 2.9519972801208496\n",
      "Time = 3.505742073059082\n",
      "Time = 2.877788782119751\n",
      "Time = 4.112082004547119\n",
      "Time = 2.912639617919922\n",
      "Time = 3.201258420944214\n",
      "Time = 2.831954002380371\n",
      "Time = 3.4516515731811523\n",
      "Time = 2.9785611629486084\n",
      "Time = 3.2596380710601807\n",
      "Time = 2.982404947280884\n",
      "Time = 3.4685161113739014\n",
      "Time = 2.9667372703552246\n",
      "Time = 3.3694610595703125\n",
      "Time = 3.0987284183502197\n",
      "Time = 4.509568691253662\n",
      "Time = 2.993812084197998\n",
      "Time = 3.1451163291931152\n",
      "Time = 3.1071126461029053\n",
      "Time = 4.609835624694824\n",
      "Time = 2.9104647636413574\n",
      "Time = 3.38212251663208\n",
      "Time = 3.053051710128784\n",
      "Time = 3.261233329772949\n",
      "Time = 3.177319288253784\n",
      "Time = 3.3333537578582764\n",
      "Time = 2.991600513458252\n",
      "Time = 3.2026500701904297\n",
      "Time = 2.8838610649108887\n",
      "Time = 3.4684388637542725\n",
      "Time = 2.944606065750122\n",
      "Time = 4.172175168991089\n",
      "Time = 3.0290656089782715\n",
      "Time = 3.4213056564331055\n",
      "Time = 2.882283926010132\n",
      "Time = 4.154077768325806\n",
      "Time = 2.9435389041900635\n",
      "Time = 3.4041855335235596\n",
      "Time = 2.9253251552581787\n",
      "Time = 3.531686305999756\n",
      "Time = 2.802900552749634\n",
      "Time = 3.411179780960083\n",
      "Time = 2.925510883331299\n",
      "Time = 3.209465980529785\n",
      "Time = 2.8873205184936523\n",
      "Time = 3.039562225341797\n",
      "Time = 4.3659820556640625\n",
      "Time = 5.460264444351196\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-6c968a8cc2c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcleanDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriteStream\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mforeachBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomputations_8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;34m.\u001b[0m\u001b[0mtrigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessingTime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'6 second'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/streaming.py\u001b[0m in \u001b[0;36mawaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1301\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1303\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1033\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1034\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1200\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1201\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Trigger the processing\n",
    "cleanDF.writeStream\\\n",
    "    .foreachBatch(computations_8)\\\n",
    "    .trigger(processingTime='6 second')\\\n",
    "    .start()\\\n",
    "    .awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you also want to delete any data of your local Kafka environment including any events you have created along the way, run the command:\n",
    "\n",
    "`` $ rm -rf /tmp/kafka-logs /tmp/zookeeper `` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
