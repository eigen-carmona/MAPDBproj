{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAPD-B distributed processing exam\n",
    "## Project 4: Streaming processing of cosmic rays using Drift Tubes detectors\n",
    "\n",
    "\n",
    "The goal of this project is to reproduce a real-time processing of real data collected in a\n",
    "particle physics detector and publish the results in a dashboard for live monitoring.\n",
    "\n",
    "\n",
    "### Students:\n",
    "+ **Capettini Hilario** (2013031)\n",
    "\n",
    "+ **Carmona Gerardo** (2005005)\n",
    "\n",
    "+ **Monaco Saverio** (2012264)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTS\n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import findspark\n",
    "from pyspark.sql import SparkSession\n",
    "from kafka.admin import KafkaAdminClient, NewTopic\n",
    "from pyspark.sql.functions import from_json, col, when, sum as ssum\n",
    "from pyspark.sql.types import StructField, StructType, DoubleType, IntegerType\n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "import pyspark.sql.functions as F\n",
    "from kafka import KafkaProducer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Kafka and Spark ready"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now initialize all the required variables with `findspark.init()` by passing the path to the spark folder we downloaded previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init('/usr/local/spark')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Spark session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create the spark session. With the following command we are asking to the master (and resource manager) to create an application with required resources and configurations. In this case we are using all the default options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"spark://master:7077\")\\\n",
    "    .appName(\"Spark Streaming\")\\\n",
    "    .config(\"spark.jars.packages\",\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.1\")\\\n",
    "    .config(\"spark.sql.shuffle.partitions\",8)\\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://master:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark Streaming</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fd2e7580ba8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "KAFKA_HOME = '/usr/local/kafka'\n",
    "KAFKA_BOOTSTRAP_SERVERS = 'slave01:9092'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the topics for Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CreateTopicsResponse_v3(throttle_time_ms=0, topic_errors=[(topic='Experiment_measurements', error_code=0, error_message=None), (topic='results', error_code=0, error_message=None)])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kafka_admin = KafkaAdminClient(\n",
    "        bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,\n",
    "    )\n",
    "\n",
    "#Here we will inject the data\n",
    "new_topic_a = NewTopic(name='Experiment_measurements', \n",
    "                       num_partitions=16, \n",
    "                       replication_factor=1)\n",
    "\n",
    "#Here we inject the number of processed hits, post cleaning\n",
    "new_topic_b = NewTopic(name='results', \n",
    "                       num_partitions=1, \n",
    "                       replication_factor=1)\n",
    "\n",
    "kafka_admin.create_topics(new_topics=[new_topic_a,new_topic_b])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['results', 'Experiment_measurements']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kafka_admin.list_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kafka - Spark INTEGRATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data from the Kafka topic (define the consumer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDF = spark\\\n",
    "    .readStream\\\n",
    "    .format(\"kafka\")\\\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP_SERVERS)\\\n",
    "    .option('subscribe', 'Experiment_measurements')\\\n",
    "    .load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The schema of the json data format used to create the messages\n",
    "schema = StructType(\n",
    "        [\n",
    "                StructField(\"HEAD\",        IntegerType()),\n",
    "                StructField(\"FPGA\",        IntegerType()),\n",
    "                StructField(\"TDC_CHANNEL\", IntegerType()),\n",
    "                StructField(\"ORBIT_CNT\",   DoubleType()),\n",
    "                StructField(\"BX_COUNTER\",  IntegerType()),\n",
    "                StructField(\"TDC_MEAS\",    DoubleType())\n",
    "        ]  \n",
    ")\n",
    "\n",
    "## a new DF can be created from the previous by using the pyspark.sql functions\n",
    "jsonDF = inputDF.select(from_json(col(\"value\").alias('value').cast(\"string\"), schema).alias('value'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatDF = jsonDF.selectExpr(\"value.HEAD\", \n",
    "                           \"value.FPGA\", \n",
    "                           \"value.TDC_CHANNEL\",\n",
    "                           \"value.ORBIT_CNT\",\n",
    "                           \"value.BX_COUNTER\",\n",
    "                           \"value.TDC_MEAS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- HEAD: integer (nullable = true)\n",
      " |-- FPGA: integer (nullable = true)\n",
      " |-- TDC_CHANNEL: integer (nullable = true)\n",
      " |-- ORBIT_CNT: double (nullable = true)\n",
      " |-- BX_COUNTER: integer (nullable = true)\n",
      " |-- TDC_MEAS: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flatDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FILTERING OF THE DATA\n",
    "## we only keep the events with \"HEAD\" = 2 and \"TDC_CHANNEL\" <= 128\n",
    "\n",
    "cleanDF = flatDF.where((col('HEAD')==2) & (col('TDC_CHANNEL') <= 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Colection of functions for the main computation\n",
    "\n",
    "def chamber_assignment(df):\n",
    "    '''Assign chamber number and leave the scintillator carriers with chamber == null'''\n",
    "\n",
    "    return(df.withColumn('CHAMBER',when(col(\"FPGA\") == 0,\n",
    "                                                when(col(\"TDC_CHANNEL\")<=63,1).\\\n",
    "                                                otherwise(when(col(\"TDC_CHANNEL\")<128,2))).\\\n",
    "                                           otherwise(when(col(\"TDC_CHANNEL\")<=63,3).\\\n",
    "                                                     otherwise(when(col(\"TDC_CHANNEL\")<128,4))\n",
    "                                           )).\\\n",
    "                                           select([ col('TDC_CHANNEL'), col('ORBIT_CNT'),\n",
    "                                           col('BX_COUNTER'),col('TDC_MEAS'),\n",
    "                                           col('CHAMBER')])\n",
    "          )\n",
    "\n",
    "\n",
    "def scintillator_data(df):\n",
    "    '''Define a dataframe containing the relevant information for \n",
    "    the scintillator analysis''' \n",
    "    \n",
    "    #First we filter the events encoding the passage time,\n",
    "    #then we add the PASSAGE time for each event \n",
    "    #Finally if we have two scilantor hits within the same orbit we keep \n",
    "    #the one with the smaller time\n",
    "    return(df.filter((col(\"CHAMBER\").isNull()) & (col(\"FPGA\") == 1)).\\\n",
    "                          withColumn(\"PASSAGETIME\", 25 * (col(\"ORBIT_CNT\") * 3564 + col(\"BX_COUNTER\") + col(\"TDC_MEAS\")/30)).\\\n",
    "                          drop(\"TDC_CHANNEL\").drop(\"BX_COUNTER\").\\\n",
    "                          drop(\"TDC_MEAS\").drop(\"CHAMBER\").\\\n",
    "                          groupBy(\"ORBIT_CNT\").min(\"PASSAGETIME\").\\\n",
    "                          withColumnRenamed(\"ORBIT_CNT\",\"ORBIT_CNT_sci\").\\\n",
    "                          withColumnRenamed(\"min(PASSAGETIME)\",\"PASSAGETIME\")\n",
    "          )\n",
    "\n",
    "\n",
    "def histogram_a(df,min_v,max_v,inc,key):# TODO: replicate the function generalization to the actual code\n",
    "    '''This function return the bins and counts for the first type of requested histogram'''\n",
    "    hist_bins = np.arange(min_v,max_v,inc)\n",
    "    hist = df\\\n",
    "        .filter((min_v<=F.col(key)) & (F.col(key)<=max_v))\\\n",
    "        .withColumn('BIN', F.floor((F.col(key)-min_v)/inc))\\\n",
    "        .groupBy('CHAMBER','BIN')\\\n",
    "        .count().select('CHAMBER','BIN', col('count').alias('COUNT'))\n",
    "    return (hist_bins, hist)\n",
    "\n",
    "\n",
    "def histogram_b(df,min_v,max_v,inc,key_1,key_2):\n",
    "    '''This function return the bins and counts for the second type of requested histogram'''\n",
    "    hist_bins = np.arange(min_v,max_v,inc)\n",
    "    hist = df\\\n",
    "        .groupBy('CHAMBER',key_1)\\\n",
    "        .agg(F.countDistinct(key_2).alias('ACTIVE'))\\\n",
    "        .filter((min_v<=F.col(key_1))&(F.col(key_1)<=max_v))\\\n",
    "        .withColumn('BIN',F.floor((F.col(key_1)-min_v)/inc))\\\n",
    "        .groupBy('CHAMBER','BIN')\\\n",
    "        .agg(F.sum('ACTIVE').alias('COUNT'))\n",
    "    return(hist_bins, hist)\n",
    "\n",
    "\n",
    "def numpify(bins, pos_count):\n",
    "    '''NUMPIFY RESULTS'''\n",
    "    counter = np.zeros(len(bins))#np.zeros(len(bins)-1)?\n",
    "    positions = np.array(list(pos_count.keys()))\n",
    "    counts = np.array(list(pos_count.values()))\n",
    "    counter[positions] = counts\n",
    "    return counter\n",
    "\n",
    "\n",
    "def prepare_results(hist, hist_bins):\n",
    "    '''COLLECTING RESULTS'''    \n",
    "    _hist = hist.groupBy('CHAMBER').agg(\n",
    "    F.map_from_entries(\n",
    "        F.collect_list(\n",
    "            F.struct(\"BIN\", \"COUNT\"))).alias(\"COUNT\")\n",
    "        ).collect()    \n",
    "\n",
    "    # JSON FORMATING OF RESULTS\n",
    "    _hist_dict = {row.CHAMBER: {\n",
    "        'Bins': list(hist_bins), 'Counts': list(numpify(hist_bins,row.COUNT))\n",
    "    } for row in _hist}\n",
    "    \n",
    "    return _hist_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computations(df, epoch, log):\n",
    "    '''This is the main function of the code, it requires a dataframe as input. The dataframe is analysed\n",
    "       and the results are published in the kafka topic \"results\" '''\n",
    "    main_df = chamber_assignment(df)\n",
    "\n",
    "    scintillator_df = scintillator_data(main_df)\n",
    "    \n",
    "    ### Drop the columns with null values from main_df\n",
    "    hit_df = main_df.na.drop(subset=[\"CHAMBER\"])\n",
    "    \n",
    "    ## TOTAL NUMBER OF PROCESSED HITS\n",
    "    total_hits = hit_df.count()\n",
    "    if not total_hits: return\n",
    "\n",
    "    ## TOTAL NUMBER OF PROCESSED HITS PER CHAMBER\n",
    "    chamber_hits = hit_df\\\n",
    "        .groupBy('CHAMBER').count()\\\n",
    "        .select(col('CHAMBER'),col('count').alias('COUNT'))\n",
    "    \n",
    "    ## ACTIVE TDC_CHANNEL PER CHAMBER\n",
    "    min_v_1 = 0\n",
    "    max_v_1 = 127\n",
    "    inc_1 = 5\n",
    "    hist_1_bins, hist_1 = histogram_a(hit_df,min_v_1,max_v_1,inc_1, 'TDC_CHANNEL')\n",
    "    \n",
    "    ## ACTIVE TDC_CHANNEL PER CHAMBER PER ORBIT_CNT\n",
    "    min_v_2 = 6.e5 #main_df.agg(F.min(F.col('ORBIT_CNT')).alias('min')).collect()[-1].min\n",
    "    max_v_2 = 1.e7 #main_df.agg(F.max(F.col('ORBIT_CNT')).alias('max')).collect()[-1].max\n",
    "    inc_2 = 0.5e6\n",
    "    hist_2_bins, hist_2 = histogram_b(hit_df,min_v_2,max_v_2,inc_2, 'ORBIT_CNT', 'TDC_CHANNEL')\n",
    "    \n",
    "    \n",
    "    ### keep only the hits with a scintillator signal within the same orbit\n",
    "    chamber_sci = hit_df.join(scintillator_df,main_df.ORBIT_CNT ==  scintillator_df.ORBIT_CNT_sci,\"inner\")\n",
    "\n",
    "    ## ADD TIME CORRECTION BY CHAMBER\n",
    "    chamber_sci = chamber_sci.withColumn('TIME_OFFSET',when(col(\"CHAMBER\") == 1, 93.9).\\\n",
    "                                                       when(col(\"CHAMBER\") == 2, 101.4).\\\n",
    "                                                       when(col(\"CHAMBER\") == 3, 95.5).\\\n",
    "                                                       when(col(\"CHAMBER\") == 4, 92.4))\n",
    "\n",
    "    ### Add the ABSSOLUTETIME and DRIFTIME\n",
    "    chamber_sci = chamber_sci.withColumn(\"ABSOLUTETIME\",\n",
    "                             25 * (col(\"ORBIT_CNT\") * 3564 + col(\"BX_COUNTER\") + col(\"TDC_MEAS\")/30)).\\\n",
    "                              withColumn(\"DRIFTIME\",col(\"ABSOLUTETIME\")-col(\"PASSAGETIME\") + col(\"TIME_OFFSET\"))\n",
    "   \n",
    "\n",
    "    ## ACTIVE TDC_CHANNEL PER CHAMBER WITHIN SCINTILLATOR SIGNAL\n",
    "    min_v_3 = 0\n",
    "    max_v_3= 127\n",
    "    inc_3 = 5\n",
    "    hist_3_bins, hist_3 = histogram_a(chamber_sci,min_v_3,max_v_3,inc_3, 'TDC_CHANNEL')\n",
    "    \n",
    "\n",
    "    ## HISTOGRAM OF DRIFTIME, PER CHAMBER\n",
    "    min_v_4 = 0\n",
    "    max_v_4= 1000\n",
    "    inc_4 = 10\n",
    "    hist_4_bins, hist_4 = histogram_a(chamber_sci,min_v_4,max_v_4,inc_4, 'DRIFTIME')\n",
    "\n",
    "\n",
    "    # PREPARE THE RESULTS\n",
    "    _chamber_hits = {row.CHAMBER: int(row.COUNT) for row in chamber_hits.collect()}\n",
    "    _hist_1_dict = prepare_results(hist_1,hist_1_bins)\n",
    "    _hist_2_dict = prepare_results(hist_2,hist_2_bins)\n",
    "    _hist_3_dict = prepare_results(hist_3,hist_3_bins)\n",
    "    _hist_4_dict = prepare_results(hist_4,hist_4_bins)\n",
    "    \n",
    "    default = lambda bins: {'Bins': list(bins), 'Counts' : [0]*(len(bins)-1)}\n",
    "    \n",
    "    results = {f'Chamber_{i}': {\n",
    "        'Count': _chamber_hits.get(i, 0),\n",
    "        'Hist_1': _hist_1_dict.get(i, default(hist_1_bins)),\n",
    "        'Hist_2': _hist_2_dict.get(i, default(hist_2_bins)),\n",
    "        'Hist_3': _hist_3_dict.get(i, default(hist_3_bins)),\n",
    "        'Hist_4': _hist_4_dict.get(i, default(hist_4_bins))} for i in range(1,5)}\n",
    "\n",
    "    results.update({\n",
    "        'Index': time.time(),\n",
    "        'Total Count': int(total_hits)\n",
    "    })\n",
    "\n",
    "    log(results)\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Send the results to the kafka topic\n",
    "#Initialize the producer\n",
    "producer = KafkaProducer(bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS)\n",
    "\n",
    "logger = lambda value: producer.send(topic=\"results\", value= str(value).encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-61b5593c2bf0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcleanDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriteStream\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mforeachBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcomputations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;34m.\u001b[0m\u001b[0mtrigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessingTime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'5 seconds'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/streaming.py\u001b[0m in \u001b[0;36mawaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1301\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1303\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1033\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1034\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1200\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1201\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Trigger the processing\n",
    "cleanDF.writeStream\\\n",
    "    .foreachBatch(lambda df, epoch: computations(df,epoch,logger))\\\n",
    "    .trigger(processingTime='5 seconds')\\\n",
    "    .start()\\\n",
    "    .awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
