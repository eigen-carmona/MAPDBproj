{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAPD-B distributed processing exam\n",
    "## Project 4: Streaming processing of cosmic rays using Drift Tubes detectors\n",
    "\n",
    "\n",
    "The goal of this project is to reproduce a real-time processing of real data collected in a\n",
    "particle physics detector and publish the results in a dashboard for live monitoring.\n",
    "\n",
    "\n",
    "### Students:\n",
    "+ **Capettini Hilario** (2013031)\n",
    "\n",
    "+ **Carmona Gerardo** (2005005)\n",
    "\n",
    "+ **Monaco Saverio** (2012264)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extremely brief introduction, the elements and data flow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The cluster "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster design, what was installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTS\n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import findspark\n",
    "from pyspark.sql import SparkSession\n",
    "from kafka.admin import KafkaAdminClient, NewTopic\n",
    "from pyspark.sql.functions import from_json, col, when, sum as ssum\n",
    "from pyspark.sql.types import StructField, StructType, DoubleType, IntegerType\n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "import pyspark.sql.functions as F\n",
    "from kafka import KafkaProducer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming with Kafka and Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I try to implement a basic pipeline for the project conecting kafka with spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Kafka and Spark ready"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now initialize all the required variables with `findspark.init()` by passing the path to the spark folder we downloaded previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init('/usr/local/spark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%script bash --no-raise-error\n",
    "#$SPARK_HOME/sbin/start-all.sh\n",
    "#$SPARK_HOME/sbin/start-master.sh\n",
    "\n",
    "# # start master \n",
    "# $SPARK_HOME/sbin/start-master.sh --host localhost \\\n",
    "#     --port 7077 --webui-port 8080\n",
    "    \n",
    "# # start worker\n",
    "# $SPARK_HOME/sbin/start-worker.sh spark://localhost:7077 \\\n",
    "#     --cores 8 --memory 6g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Spark session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create the spark session. With the following command we are asking to the master (and resource manager) to create an application with required resources and configurations. In this case we are using all the default options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"spark://master:7077\")\\\n",
    "    .appName(\"Spark Streaming\")\\\n",
    "    .config(\"spark.jars.packages\",\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.1\")\\\n",
    "    .config(\"spark.sql.shuffle.partitions\",8)\\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KAFKA_HOME = '/usr/local/kafka'\n",
    "KAFKA_BOOTSTRAP_SERVERS = 'slave01:9092'\n",
    "#KAFKA_BOOTSTRAP_SERVERS = 'localhost:9092'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#By some reason I can't launch this from here using OS, so i open the terminals in the KAFKA_HOME folder\n",
    "# and launch the zookeper and the kafka server comands manually\n",
    "\n",
    "\n",
    "# Start Zookeeper\n",
    "# bin/zookeeper-server-start.sh config/zookeeper.properties \n",
    "#os.system('{0}/bin/zookeeper-server-start.sh {0}/config/zookeeper.properties'.format(KAFKA_HOME)) \n",
    "    \n",
    "# Start one Kafka Broker\n",
    "#bin/kafka-server-start.sh config/server.properties\n",
    "#os.system('{0}/bin/kafka-server-start.sh {0}/config/server.properties'.format(KAFKA_HOME)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the topics for Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "kafka_admin = KafkaAdminClient(\n",
    "        bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,\n",
    "    )\n",
    "\n",
    "#Here we will inject the data\n",
    "new_topic_a = NewTopic(name='Experiment_measurements', \n",
    "                       num_partitions=16, \n",
    "                       replication_factor=1)\n",
    "\n",
    "#Here we inject the number of processed hits, post cleaning\n",
    "new_topic_b = NewTopic(name='results', \n",
    "                       num_partitions=1, \n",
    "                       replication_factor=1)\n",
    "\n",
    "kafka_admin.create_topics(new_topics=[new_topic_a,new_topic_b])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_admin.list_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kafka - Spark INTEGRATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data from the Kafka topic (define the consumer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDF = spark\\\n",
    "    .readStream\\\n",
    "    .format(\"kafka\")\\\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP_SERVERS)\\\n",
    "    .option('subscribe', 'Experiment_measurements')\\\n",
    "    .load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The schema of the json data format used to create the messages\n",
    "schema = StructType(\n",
    "        [\n",
    "                StructField(\"HEAD\",        IntegerType()),\n",
    "                StructField(\"FPGA\",        IntegerType()),\n",
    "                StructField(\"TDC_CHANNEL\", IntegerType()),\n",
    "                StructField(\"ORBIT_CNT\",   DoubleType()),\n",
    "                StructField(\"BX_COUNTER\",  IntegerType()),\n",
    "                StructField(\"TDC_MEAS\",    DoubleType())\n",
    "        ]  \n",
    ")\n",
    "\n",
    "## a new DF can be created from the previous by using the pyspark.sql functions\n",
    "jsonDF = inputDF.select(from_json(col(\"value\").alias('value').cast(\"string\"), schema).alias('value'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatDF = jsonDF.selectExpr(\"value.HEAD\", \n",
    "                           \"value.FPGA\", \n",
    "                           \"value.TDC_CHANNEL\",\n",
    "                           \"value.ORBIT_CNT\",\n",
    "                           \"value.BX_COUNTER\",\n",
    "                           \"value.TDC_MEAS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FILTERING OF THE DATA\n",
    "## we only keep the events with \"HEAD\" = 2 and \"TDC_CHANNEL\" <= 128\n",
    "\n",
    "cleanDF = flatDF.where((col('HEAD')==2) & (col('TDC_CHANNEL') <= 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Colection of functions for the main computation\n",
    "\n",
    "def chamber_assignment(df):\n",
    "    '''Assign chamber number and leave the scintillator carriers with chamber == null'''\n",
    "    \n",
    "    return(df.withColumn('CHAMBER',when(col(\"FPGA\") == 0, \n",
    "                                                when(col(\"TDC_CHANNEL\")<=63,1).\\\n",
    "                                                otherwise(when(col(\"TDC_CHANNEL\")<128,2))).\\\n",
    "                                           otherwise(when(col(\"TDC_CHANNEL\")<=63,3).\\\n",
    "                                                     otherwise(when(col(\"TDC_CHANNEL\")<128,4))\n",
    "                                           )).\\\n",
    "                                           select([ col('TDC_CHANNEL'), col('ORBIT_CNT'),\n",
    "                                           col('BX_COUNTER'),col('TDC_MEAS'),\n",
    "                                           col('CHAMBER')])\n",
    "          )\n",
    "\n",
    "def scintillator_data(df):\n",
    "    '''Define a dataframe containing the relevant information for \n",
    "    the scintillator analysis''' \n",
    "    \n",
    "    #First we filter the events encoding the passage time,\n",
    "    #then we add the PASSAGE time for each event \n",
    "    #Finally if we have two scilantor hits within the same orbit we keep \n",
    "    #the one with the smaller time\n",
    "    return(df.filter((col(\"CHAMBER\").isNull())).\\\n",
    "                          withColumn(\"PASSAGETIME\", 25 * (col(\"ORBIT_CNT\") * 3564 + col(\"BX_COUNTER\") + col(\"TDC_MEAS\")/30)).\\\n",
    "                          drop(\"TDC_CHANNEL\").drop(\"BX_COUNTER\").\\\n",
    "                          drop(\"TDC_MEAS\").drop(\"CHAMBER\").\\\n",
    "                          groupBy(\"ORBIT_CNT\").min(\"PASSAGETIME\").\\\n",
    "                          withColumnRenamed(\"ORBIT_CNT\",\"ORBIT_CNT_sci\").\\\n",
    "                          withColumnRenamed(\"min(PASSAGETIME)\",\"PASSAGETIME\")\n",
    "          )\n",
    "\n",
    "\n",
    "def histogram_1(df,min_v_1,max_v_1,inc_1):\n",
    "    '''This function return the bins and counts for the first requested histogram'''\n",
    "    hist_1_bins = np.arange(min_v_1,max_v_1,inc_1)\n",
    "    hist_1 = df\\\n",
    "        .filter((min_v_1<=F.col('TDC_CHANNEL')) & (F.col('TDC_CHANNEL')<=max_v_1))\\\n",
    "        .withColumn('BIN', F.floor((F.col('TDC_CHANNEL')-min_v_1)/inc_1))\\\n",
    "        .groupBy('CHAMBER','BIN')\\\n",
    "        .count().select('CHAMBER','BIN', col('count').alias('COUNT'))\n",
    "    return (hist_1_bins, hist_1)\n",
    "    \n",
    "\n",
    "def histogram_2(df,min_v_2,max_v_2,inc_2):\n",
    "    '''This function return the bins and counts for the second requested histogram'''\n",
    "    hist_2_bins = np.arange(min_v_2,max_v_2,inc_2)\n",
    "    hist_2 = df\\\n",
    "        .groupBy('CHAMBER','ORBIT_CNT')\\\n",
    "        .agg(F.countDistinct('TDC_CHANNEL').alias('ACTIVE_CHANNELS'))\\\n",
    "        .filter((min_v_2<=F.col('ORBIT_CNT'))&(F.col('ORBIT_CNT')<=max_v_2))\\\n",
    "        .withColumn('BIN',F.floor((F.col('ORBIT_CNT')-min_v_2)/inc_2))\\\n",
    "        .groupBy('CHAMBER','BIN')\\\n",
    "        .agg(F.sum('ACTIVE_CHANNELS').alias('COUNT'))#.collect()\n",
    "    return(hist_2_bins, hist_2)\n",
    "\n",
    "def histogram_3(df,min_v_3,max_v_3,inc_3):\n",
    "    '''This function return the bins and counts for the third requested histogram'''\n",
    "    hist_3_bins = np.arange(min_v_3,max_v_3,inc_3)\n",
    "    hist_3 = df\\\n",
    "        .filter((min_v_3<=F.col('TDC_CHANNEL')) & (F.col('TDC_CHANNEL')<=max_v_3))\\\n",
    "        .withColumn('BIN', F.floor((F.col('TDC_CHANNEL')-min_v_3)/inc_3))\\\n",
    "        .groupBy('CHAMBER','BIN')\\\n",
    "        .count().select('CHAMBER','BIN', col('count').alias('COUNT')) \n",
    "    return(hist_3_bins, hist_3)\n",
    "\n",
    "def histogram_4(df,min_v_4,max_v_4,inc_4):\n",
    "    '''This function return the bins and counts for the fourth requested histogram'''\n",
    "    hist_4_bins = np.arange(min_v_4,max_v_4,inc_4)\n",
    "    hist_4 = df\\\n",
    "        .filter((min_v_4<=F.col('DRIFTIME')) & (F.col('DRIFTIME')<=max_v_4))\\\n",
    "        .withColumn('BIN', F.floor((F.col('DRIFTIME')-min_v_4)/inc_4))\\\n",
    "        .groupBy('CHAMBER','BIN')\\\n",
    "        .count().select('CHAMBER','BIN', col('count').alias('COUNT')) \n",
    "    return(hist_4_bins, hist_4)\n",
    "\n",
    "\n",
    "   \n",
    "def numpify(bins, pos_count):\n",
    "    '''NUMPIFY RESULTS'''\n",
    "    counter = np.zeros(len(bins))#np.zeros(len(bins)-1)?\n",
    "    positions = np.array(list(pos_count.keys()))\n",
    "    counts = np.array(list(pos_count.values()))\n",
    "    counter[positions] = counts\n",
    "    return counter\n",
    "\n",
    "\n",
    "def prepare_results(chamber_hits,hist_1,hist_2,hist_3,hist_4,hist_1_bins,hist_2_bins,hist_3_bins,hist_4_bins):\n",
    "    '''COLLECTING RESULTS'''\n",
    "    _chamber_hits = chamber_hits.collect()\n",
    "    \n",
    "    _hist_1 = hist_1.groupBy('CHAMBER').agg(\n",
    "    F.map_from_entries(\n",
    "        F.collect_list(\n",
    "            F.struct(\"BIN\", \"COUNT\"))).alias(\"COUNT\")\n",
    "        ).collect()\n",
    "\n",
    "    _hist_2 = hist_2.groupBy('CHAMBER').agg(\n",
    "    F.map_from_entries(\n",
    "        F.collect_list(\n",
    "            F.struct(\"BIN\",\"COUNT\"))).alias(\"COUNT\")\n",
    "        ).collect()\n",
    "\n",
    "    _hist_3 = hist_3.groupBy('CHAMBER').agg(\n",
    "    F.map_from_entries(\n",
    "        F.collect_list(\n",
    "            F.struct(\"BIN\", \"COUNT\"))).alias(\"COUNT\")\n",
    "        ).collect()\n",
    "    \n",
    "    _hist_4 = hist_4.groupBy('CHAMBER').agg(\n",
    "    F.map_from_entries(\n",
    "        F.collect_list(\n",
    "            F.struct(\"BIN\", \"COUNT\"))).alias(\"COUNT\")\n",
    "        ).collect()\n",
    "    \n",
    "\n",
    "    # JSON FORMATING OF RESULTS\n",
    "    _hist_1_dict = {row.CHAMBER: {\n",
    "        'Bins': list(hist_1_bins), 'Counts': list(numpify(hist_1_bins,row.COUNT))\n",
    "    } for row in _hist_1}\n",
    "\n",
    "    _hist_2_dict = {row.CHAMBER: {\n",
    "        'Bins': list(hist_2_bins), 'Counts': list(numpify(hist_2_bins,row.COUNT))\n",
    "    } for row in _hist_2}\n",
    "    \n",
    "    _hist_3_dict = {row.CHAMBER: {\n",
    "        'Bins': list(hist_3_bins), 'Counts': list(numpify(hist_3_bins,row.COUNT))\n",
    "    } for row in _hist_3}\n",
    "        \n",
    "    _hist_4_dict = {row.CHAMBER: {\n",
    "        'Bins': list(hist_4_bins), 'Counts': list(numpify(hist_4_bins,row.COUNT))\n",
    "    } for row in _hist_4}\n",
    "    \n",
    "    return(_chamber_hits,_hist_1_dict,_hist_2_dict,_hist_3_dict,_hist_4_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computations(df, epoch):\n",
    "    '''This is the main function of the code, it requires a dataframe as input. The dataframe is analysed\n",
    "       and the results are published in the kafka topic \"results\" '''\n",
    "    \n",
    "    #start=time.time()\n",
    "    main_df = chamber_assignment(df)\n",
    "\n",
    "    scintillator_df = scintillator_data(main_df)\n",
    "    \n",
    "    #Drop the columns with null values from main_df\n",
    "    hit_df = main_df.na.drop(subset=[\"CHAMBER\"])#.show()\n",
    "    \n",
    "    ## TOTAL NUMBER OF PROCESSED HITS\n",
    "    total_hits = hit_df.count()\n",
    "    if not total_hits: return\n",
    "\n",
    "    ## TOTAL NUMBER OF PROCESSED HITS PER CHAMBER\n",
    "    chamber_hits = hit_df\\\n",
    "        .groupBy('CHAMBER').count()\\\n",
    "        .select(col('CHAMBER'),col('count').alias('COUNT'))#.collect()\n",
    "    \n",
    "    ## ACTIVE TDC_CHANNEL PER CHAMBER\n",
    "    min_v_1 = 0\n",
    "    max_v_1 = 170\n",
    "    inc_1 = 5\n",
    "    hist_1_bins, hist_1 = histogram_1(hit_df,min_v_1,max_v_1,inc_1)\n",
    "    \n",
    "    ## ACTIVE TDC_CHANNEL PER CHAMBER PER ORBIT_CNT\n",
    "    min_v_2 = 6.e5 #main_df.agg(F.min(F.col('ORBIT_CNT')).alias('min')).collect()[-1].min\n",
    "    max_v_2 = 1.e7 #main_df.agg(F.max(F.col('ORBIT_CNT')).alias('max')).collect()[-1].max\n",
    "    inc_2 = 0.5e6\n",
    "    hist_2_bins, hist_2 = histogram_2(hit_df,min_v_2,max_v_2,inc_2)\n",
    "    \n",
    "    \n",
    "    #keep only the hits with a scintillator signal within the same orbit\n",
    "    chamber_sci = hit_df.join(scintillator_df,main_df.ORBIT_CNT ==  scintillator_df.ORBIT_CNT_sci,\"inner\")\n",
    "\n",
    "    ## ADD TIME CORRECTION BY CHAMBER\n",
    "    chamber_sci = chamber_sci.withColumn('TIME_OFFSET',when(col(\"CHAMBER\") == 1, 93.9).\\\n",
    "                                                       when(col(\"CHAMBER\") == 2, 101.4).\\\n",
    "                                                       when(col(\"CHAMBER\") == 3, 95.5).\\\n",
    "                                                       when(col(\"CHAMBER\") == 4, 92.4))\n",
    "\n",
    "    #Add the ABSSOLUTETIME and DRIFTIME\n",
    "    chamber_sci = chamber_sci.withColumn(\"ABSOLUTETIME\",\n",
    "                             25 * (col(\"ORBIT_CNT\") * 3564 + col(\"BX_COUNTER\") + col(\"TDC_MEAS\")/30)).\\\n",
    "                              withColumn(\"DRIFTIME\",col(\"ABSOLUTETIME\")-col(\"PASSAGETIME\") + col(\"TIME_OFFSET\"))\n",
    "   \n",
    "\n",
    "    ## ACTIVE TDC_CHANNEL PER CHAMBER WITHIN SCINTILLATOR SIGNAL\n",
    "    min_v_3 = 0\n",
    "    max_v_3= 170\n",
    "    inc_3 = 5\n",
    "    hist_3_bins, hist_3 = histogram_3(chamber_sci,min_v_3,max_v_3,inc_3)\n",
    "    \n",
    "\n",
    "    ## HISTOGRAM OF DRIFTIME, PER CHAMBER\n",
    "    min_v_4 = 0\n",
    "    max_v_4= 1000\n",
    "    inc_4 = 10\n",
    "    hist_4_bins, hist_4 = histogram_4(chamber_sci,min_v_4,max_v_4,inc_4)\n",
    "    \n",
    "    #PREPARE THE RESULTS\n",
    "    _chamber_hits,_hist_1_dict,_hist_2_dict,_hist_3_dict,_hist_4_dict = prepare_results(chamber_hits,hist_1,hist_2,hist_3,hist_4,hist_1_bins,hist_2_bins,hist_3_bins,hist_4_bins)\n",
    "    \n",
    "    \n",
    "    results = {f'Chamber_{row.CHAMBER}': {\n",
    "        'Count': int(row.COUNT),\n",
    "        'Hist_1': _hist_1_dict.get(row.CHAMBER, {'Bins': list(np.arange(min_v_1,max_v_1,inc_1)), 'Counts' : [0]*(len(list(np.arange(min_v_1,max_v_1,inc_1)))-1)}),\n",
    "        'Hist_2': _hist_2_dict.get(row.CHAMBER, {'Bins': list(np.arange(min_v_2,max_v_2,inc_2)), 'Counts' : [0]*(len(list(np.arange(min_v_2,max_v_2,inc_2)))-1)}),\n",
    "        'Hist_3': _hist_3_dict.get(row.CHAMBER, {'Bins': list(np.arange(min_v_3,max_v_3,inc_3)), 'Counts' : [0]*(len(list(np.arange(min_v_3,max_v_3,inc_3)))-1)}),\n",
    "        'Hist_4': _hist_4_dict.get(row.CHAMBER, {'Bins': list(np.arange(min_v_4,max_v_4,inc_4)), 'Counts' : [0]*(len(list(np.arange(min_v_4,max_v_4,inc_4)))-1)})} for row in _chamber_hits}\n",
    "\n",
    "    results.update({\n",
    "        'Index': time.time(),\n",
    "        'Total Count': int(total_hits)\n",
    "    })\n",
    "\n",
    "    producer.send(topic=\"results\", value= str(results).encode('utf-8'))\n",
    "    #producer.flush()\n",
    "    #end = time.time()\n",
    "\n",
    "    #print(\"Time =\",end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Send the results to the kafka topic\n",
    "#Initialize the producer\n",
    "producer = KafkaProducer(bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Trigger the processing\n",
    "cleanDF.writeStream\\\n",
    "    .foreachBatch(computations)\\\n",
    "    .trigger(processingTime='5 seconds')\\\n",
    "    .start()\\\n",
    "    .awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you also want to delete any data of your local Kafka environment including any events you have created along the way, run the command:\n",
    "\n",
    "`` $ rm -rf /tmp/kafka-logs /tmp/zookeeper `` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vertical scalability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Horizontal scalability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling with ammount of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
