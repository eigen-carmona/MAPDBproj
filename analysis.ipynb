{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0da510bd",
   "metadata": {},
   "source": [
    "# MAPD-B distributed processing exam\n",
    "## Project 4: Streaming processing of cosmic rays using Drift Tubes detectors\n",
    "\n",
    "\n",
    "The goal of this project is to reproduce a real-time processing of real data collected in a\n",
    "particle physics detector and publish the results in a dashboard for live monitoring.\n",
    "\n",
    "\n",
    "### Students:\n",
    "+ **Capettini Hilario** (2013031)\n",
    "\n",
    "+ **Carmona Gerardo** (2005005)\n",
    "\n",
    "+ **Monaco Saverio** (2012264)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d5eece",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5506ff5e",
   "metadata": {},
   "source": [
    "Extremely brief introduction, the elements and data flow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675ab9d6",
   "metadata": {},
   "source": [
    "## The cluster "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ef4657",
   "metadata": {},
   "source": [
    "Cluster design, what was installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808c322f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTS\n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import findspark\n",
    "from pyspark.sql import SparkSession\n",
    "from kafka.admin import KafkaAdminClient, NewTopic\n",
    "from pyspark.sql.functions import from_json, col, when, sum as ssum\n",
    "from pyspark.sql.types import StructField, StructType, DoubleType, IntegerType\n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "import pyspark.sql.functions as F\n",
    "from kafka import KafkaProducer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0b153b",
   "metadata": {},
   "source": [
    "## Streaming with Kafka and Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c29f2c",
   "metadata": {},
   "source": [
    "Here I try to implement a basic pipeline for the project conecting kafka with spark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff97de2",
   "metadata": {},
   "source": [
    "## Get Kafka and Spark ready"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e31a79",
   "metadata": {},
   "source": [
    "We can now initialize all the required variables with `findspark.init()` by passing the path to the spark folder we downloaded previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47812465",
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init('/usr/local/spark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3219416c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%script bash --no-raise-error\n",
    "#$SPARK_HOME/sbin/start-all.sh\n",
    "#$SPARK_HOME/sbin/start-master.sh\n",
    "\n",
    "# # start master \n",
    "# $SPARK_HOME/sbin/start-master.sh --host localhost \\\n",
    "#     --port 7077 --webui-port 8080\n",
    "    \n",
    "# # start worker\n",
    "# $SPARK_HOME/sbin/start-worker.sh spark://localhost:7077 \\\n",
    "#     --cores 8 --memory 6g"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bca81c",
   "metadata": {},
   "source": [
    "## Create the Spark session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada5beb0",
   "metadata": {},
   "source": [
    "We can now create the spark session. With the following command we are asking to the master (and resource manager) to create an application with required resources and configurations. In this case we are using all the default options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ff8168",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"spark://master:7077\")\\\n",
    "    .appName(\"Spark Streaming\")\\\n",
    "    .config(\"spark.jars.packages\",\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.1\")\\\n",
    "    .config(\"spark.sql.shuffle.partitions\",8)\\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3de935",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d5198a",
   "metadata": {},
   "source": [
    "## Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c79e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "KAFKA_HOME = '/usr/local/kafka'\n",
    "KAFKA_BOOTSTRAP_SERVERS = 'slave01:9092'\n",
    "#KAFKA_BOOTSTRAP_SERVERS = 'localhost:9092'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2af986c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#By some reason I can't launch this from here using OS, so i open the terminals in the KAFKA_HOME folder\n",
    "# and launch the zookeper and the kafka server comands manually\n",
    "\n",
    "\n",
    "# Start Zookeeper\n",
    "# bin/zookeeper-server-start.sh config/zookeeper.properties \n",
    "#os.system('{0}/bin/zookeeper-server-start.sh {0}/config/zookeeper.properties'.format(KAFKA_HOME)) \n",
    "    \n",
    "# Start one Kafka Broker\n",
    "#bin/kafka-server-start.sh config/server.properties\n",
    "#os.system('{0}/bin/kafka-server-start.sh {0}/config/server.properties'.format(KAFKA_HOME)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29016f8",
   "metadata": {},
   "source": [
    "### Create the topics for Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802d50e7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "kafka_admin = KafkaAdminClient(\n",
    "        bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,\n",
    "    )\n",
    "\n",
    "#Here we will inject the data\n",
    "new_topic_a = NewTopic(name='Experiment_measurements', \n",
    "                       num_partitions=16, \n",
    "                       replication_factor=1)\n",
    "\n",
    "#Here we inject the number of processed hits, post cleaning\n",
    "new_topic_b = NewTopic(name='results', \n",
    "                       num_partitions=1, \n",
    "                       replication_factor=1)\n",
    "\n",
    "kafka_admin.create_topics(new_topics=[new_topic_a,new_topic_b])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a27310",
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_admin.list_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08005f6a",
   "metadata": {},
   "source": [
    "## Kafka - Spark INTEGRATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59cca67",
   "metadata": {},
   "source": [
    "### Read the data from the Kafka topic (define the consumer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf97a58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDF = spark\\\n",
    "    .readStream\\\n",
    "    .format(\"kafka\")\\\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP_SERVERS)\\\n",
    "    .option('subscribe', 'Experiment_measurements')\\\n",
    "    .load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2ec07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The schema of the json data format used to create the messages\n",
    "schema = StructType(\n",
    "        [\n",
    "                StructField(\"HEAD\",        IntegerType()),\n",
    "                StructField(\"FPGA\",        IntegerType()),\n",
    "                StructField(\"TDC_CHANNEL\", IntegerType()),\n",
    "                StructField(\"ORBIT_CNT\",   DoubleType()),\n",
    "                StructField(\"BX_COUNTER\",  IntegerType()),\n",
    "                StructField(\"TDC_MEAS\",    DoubleType())\n",
    "        ]  \n",
    ")\n",
    "\n",
    "## a new DF can be created from the previous by using the pyspark.sql functions\n",
    "jsonDF = inputDF.select(from_json(col(\"value\").alias('value').cast(\"string\"), schema).alias('value'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d7380a",
   "metadata": {},
   "outputs": [],
   "source": [
    "flatDF = jsonDF.selectExpr(\"value.HEAD\", \n",
    "                           \"value.FPGA\", \n",
    "                           \"value.TDC_CHANNEL\",\n",
    "                           \"value.ORBIT_CNT\",\n",
    "                           \"value.BX_COUNTER\",\n",
    "                           \"value.TDC_MEAS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcb6e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "flatDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831b9e03",
   "metadata": {},
   "source": [
    "## Spark processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f881208",
   "metadata": {},
   "outputs": [],
   "source": [
    "## FILTERING OF THE DATA\n",
    "## we only keep the events with \"HEAD\" = 2 and \"TDC_CHANNEL\" <= 128\n",
    "\n",
    "cleanDF = flatDF.where((col('HEAD')==2) & (col('TDC_CHANNEL') <= 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477e765c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Colection of functions for the main computation\n",
    "\n",
    "def chamber_assignment(df):\n",
    "    '''Assign chamber number and leave the scintillator carriers with chamber == null'''\n",
    "\n",
    "    return(df.withColumn('CHAMBER',when(col(\"FPGA\") == 0,\n",
    "                                                when(col(\"TDC_CHANNEL\")<=63,1).\\\n",
    "                                                otherwise(when(col(\"TDC_CHANNEL\")<128,2))).\\\n",
    "                                           otherwise(when(col(\"TDC_CHANNEL\")<=63,3).\\\n",
    "                                                     otherwise(when(col(\"TDC_CHANNEL\")<128,4))\n",
    "                                           )).\\\n",
    "                                           select([ col('TDC_CHANNEL'), col('ORBIT_CNT'),\n",
    "                                           col('BX_COUNTER'),col('TDC_MEAS'),\n",
    "                                           col('CHAMBER')])\n",
    "          )\n",
    "\n",
    "\n",
    "def scintillator_data(df):\n",
    "    '''Define a dataframe containing the relevant information for \n",
    "    the scintillator analysis''' \n",
    "    \n",
    "    #First we filter the events encoding the passage time,\n",
    "    #then we add the PASSAGE time for each event \n",
    "    #Finally if we have two scilantor hits within the same orbit we keep \n",
    "    #the one with the smaller time\n",
    "    return(df.filter((col(\"CHAMBER\").isNull()) & (col(\"FPGA\") == 1)).\\\n",
    "                          withColumn(\"PASSAGETIME\", 25 * (col(\"ORBIT_CNT\") * 3564 + col(\"BX_COUNTER\") + col(\"TDC_MEAS\")/30)).\\\n",
    "                          drop(\"TDC_CHANNEL\").drop(\"BX_COUNTER\").\\\n",
    "                          drop(\"TDC_MEAS\").drop(\"CHAMBER\").\\\n",
    "                          groupBy(\"ORBIT_CNT\").min(\"PASSAGETIME\").\\\n",
    "                          withColumnRenamed(\"ORBIT_CNT\",\"ORBIT_CNT_sci\").\\\n",
    "                          withColumnRenamed(\"min(PASSAGETIME)\",\"PASSAGETIME\")\n",
    "          )\n",
    "\n",
    "\n",
    "def histogram_a(df,min_v,max_v,inc,key):# TODO: replicate the function generalization to the actual code\n",
    "    '''This function return the bins and counts for the first type of requested histogram'''\n",
    "    hist_bins = np.arange(min_v,max_v,inc)\n",
    "    hist = df\\\n",
    "        .filter((min_v<=F.col(key)) & (F.col(key)<=max_v))\\\n",
    "        .withColumn('BIN', F.floor((F.col(key)-min_v)/inc))\\\n",
    "        .groupBy('CHAMBER','BIN')\\\n",
    "        .count().select('CHAMBER','BIN', col('count').alias('COUNT'))\n",
    "    return (hist_bins, hist)\n",
    "\n",
    "\n",
    "def histogram_b(df,min_v,max_v,inc,key_1,key_2):\n",
    "    '''This function return the bins and counts for the second type of requested histogram'''\n",
    "    hist_bins = np.arange(min_v,max_v,inc)\n",
    "    hist = df\\\n",
    "        .groupBy('CHAMBER',key_1)\\\n",
    "        .agg(F.countDistinct(key_2).alias('ACTIVE'))\\\n",
    "        .filter((min_v<=F.col(key_1))&(F.col(key_1)<=max_v))\\\n",
    "        .withColumn('BIN',F.floor((F.col(key_1)-min_v)/inc))\\\n",
    "        .groupBy('CHAMBER','BIN')\\\n",
    "        .agg(F.sum('ACTIVE').alias('COUNT'))\n",
    "    return(hist_bins, hist)\n",
    "\n",
    "\n",
    "def numpify(bins, pos_count):\n",
    "    '''NUMPIFY RESULTS'''\n",
    "    counter = np.zeros(len(bins))#np.zeros(len(bins)-1)?\n",
    "    positions = np.array(list(pos_count.keys()))\n",
    "    counts = np.array(list(pos_count.values()))\n",
    "    counter[positions] = counts\n",
    "    return counter\n",
    "\n",
    "\n",
    "def prepare_results(hist, hist_bins):\n",
    "    '''COLLECTING RESULTS'''    \n",
    "    _hist = hist.groupBy('CHAMBER').agg(\n",
    "    F.map_from_entries(\n",
    "        F.collect_list(\n",
    "            F.struct(\"BIN\", \"COUNT\"))).alias(\"COUNT\")\n",
    "        ).collect()    \n",
    "\n",
    "    # JSON FORMATING OF RESULTS\n",
    "    _hist_dict = {row.CHAMBER: {\n",
    "        'Bins': list(hist_bins), 'Counts': list(numpify(hist_bins,row.COUNT))\n",
    "    } for row in _hist}\n",
    "    \n",
    "    return _hist_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a1cf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computations(df, epoch, log):\n",
    "    '''This is the main function of the code, it requires a dataframe as input. The dataframe is analysed\n",
    "       and the results are published in the kafka topic \"results\" '''\n",
    "    main_df = chamber_assignment(df)\n",
    "\n",
    "    scintillator_df = scintillator_data(main_df)\n",
    "    \n",
    "    ### Drop the columns with null values from main_df\n",
    "    hit_df = main_df.na.drop(subset=[\"CHAMBER\"])\n",
    "    \n",
    "    ## TOTAL NUMBER OF PROCESSED HITS\n",
    "    total_hits = hit_df.count()\n",
    "    if not total_hits: return\n",
    "\n",
    "    ## TOTAL NUMBER OF PROCESSED HITS PER CHAMBER\n",
    "    chamber_hits = hit_df\\\n",
    "        .groupBy('CHAMBER').count()\\\n",
    "        .select(col('CHAMBER'),col('count').alias('COUNT'))\n",
    "    \n",
    "    ## ACTIVE TDC_CHANNEL PER CHAMBER\n",
    "    min_v_1 = 0\n",
    "    max_v_1 = 127\n",
    "    inc_1 = 5\n",
    "    hist_1_bins, hist_1 = histogram_a(hit_df,min_v_1,max_v_1,inc_1, 'TDC_CHANNEL')\n",
    "    \n",
    "    ## ACTIVE TDC_CHANNEL PER CHAMBER PER ORBIT_CNT\n",
    "    min_v_2 = 6.e5 #main_df.agg(F.min(F.col('ORBIT_CNT')).alias('min')).collect()[-1].min\n",
    "    max_v_2 = 1.e7 #main_df.agg(F.max(F.col('ORBIT_CNT')).alias('max')).collect()[-1].max\n",
    "    inc_2 = 0.5e6\n",
    "    hist_2_bins, hist_2 = histogram_b(hit_df,min_v_2,max_v_2,inc_2, 'ORBIT_CNT', 'TDC_CHANNEL')\n",
    "    \n",
    "    \n",
    "    ### keep only the hits with a scintillator signal within the same orbit\n",
    "    chamber_sci = hit_df.join(scintillator_df,main_df.ORBIT_CNT ==  scintillator_df.ORBIT_CNT_sci,\"inner\")\n",
    "\n",
    "    ## ADD TIME CORRECTION BY CHAMBER\n",
    "    chamber_sci = chamber_sci.withColumn('TIME_OFFSET',when(col(\"CHAMBER\") == 1, 93.9).\\\n",
    "                                                       when(col(\"CHAMBER\") == 2, 101.4).\\\n",
    "                                                       when(col(\"CHAMBER\") == 3, 95.5).\\\n",
    "                                                       when(col(\"CHAMBER\") == 4, 92.4))\n",
    "\n",
    "    ### Add the ABSSOLUTETIME and DRIFTIME\n",
    "    chamber_sci = chamber_sci.withColumn(\"ABSOLUTETIME\",\n",
    "                             25 * (col(\"ORBIT_CNT\") * 3564 + col(\"BX_COUNTER\") + col(\"TDC_MEAS\")/30)).\\\n",
    "                              withColumn(\"DRIFTIME\",col(\"ABSOLUTETIME\")-col(\"PASSAGETIME\") + col(\"TIME_OFFSET\"))\n",
    "   \n",
    "\n",
    "    ## ACTIVE TDC_CHANNEL PER CHAMBER WITHIN SCINTILLATOR SIGNAL\n",
    "    min_v_3 = 0\n",
    "    max_v_3= 127\n",
    "    inc_3 = 5\n",
    "    hist_3_bins, hist_3 = histogram_a(chamber_sci,min_v_3,max_v_3,inc_3, 'TDC_CHANNEL')\n",
    "    \n",
    "\n",
    "    ## HISTOGRAM OF DRIFTIME, PER CHAMBER\n",
    "    min_v_4 = 0\n",
    "    max_v_4= 1000\n",
    "    inc_4 = 10\n",
    "    hist_4_bins, hist_4 = histogram_a(chamber_sci,min_v_4,max_v_4,inc_4, 'DRIFTIME')\n",
    "\n",
    "\n",
    "    # PREPARE THE RESULTS\n",
    "    _chamber_hits = {row.CHAMBER: int(row.COUNT) for row in chamber_hits.collect()}\n",
    "    _hist_1_dict = prepare_results(hist_1,hist_1_bins)\n",
    "    _hist_2_dict = prepare_results(hist_2,hist_2_bins)\n",
    "    _hist_3_dict = prepare_results(hist_3,hist_3_bins)\n",
    "    _hist_4_dict = prepare_results(hist_4,hist_4_bins)\n",
    "    \n",
    "    default = lambda bins: {'Bins': list(bins), 'Counts' : [0]*(len(bins)-1)}\n",
    "    \n",
    "    results = {f'Chamber_{i}': {\n",
    "        'Count': _chamber_hits.get(i, 0),\n",
    "        'Hist_1': _hist_1_dict.get(i, default(hist_1_bins)),\n",
    "        'Hist_2': _hist_2_dict.get(i, default(hist_2_bins)),\n",
    "        'Hist_3': _hist_3_dict.get(i, default(hist_3_bins)),\n",
    "        'Hist_4': _hist_4_dict.get(i, default(hist_4_bins))} for i in range(1,5)}\n",
    "\n",
    "    results.update({\n",
    "        'Index': time.time(),\n",
    "        'Total Count': int(total_hits)\n",
    "    })\n",
    "\n",
    "    log(results)\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6919294b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Send the results to the kafka topic\n",
    "#Initialize the producer\n",
    "producer = KafkaProducer(bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS)\n",
    "\n",
    "logger = lambda value: producer.send(topic=\"results\", value= str(value).encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c56013",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Trigger the processing\n",
    "cleanDF.writeStream\\\n",
    "    .foreachBatch(lambda df, epoch: computations(df,epoch,logger))\\\n",
    "    .trigger(processingTime='5 seconds')\\\n",
    "    .start()\\\n",
    "    .awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afac86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48a8fb3",
   "metadata": {},
   "source": [
    "If you also want to delete any data of your local Kafka environment including any events you have created along the way, run the command:\n",
    "\n",
    "`` $ rm -rf /tmp/kafka-logs /tmp/zookeeper `` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798e72ec",
   "metadata": {},
   "source": [
    "##  Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dad219",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "71e3639b",
   "metadata": {},
   "source": [
    "### Vertical scalability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65d870b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8a8005e5",
   "metadata": {},
   "source": [
    "### Horizontal scalability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0674729",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d5a408ca",
   "metadata": {},
   "source": [
    "### Scaling with ammount of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a9ffcc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
