{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e73b3fd",
   "metadata": {},
   "source": [
    "# Streaming with Kafka and Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827b85a1",
   "metadata": {},
   "source": [
    "Here I try to implement a basic pipeline for the project conecting kafka with spark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632ee96d",
   "metadata": {},
   "source": [
    "### Computer setting\n",
    "I downloaded and located in my home the spark file **spark-3.1.2-bin-hadoop3.2** and also the kafka file **kafka_2.13-2.7.0**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ea3a6e",
   "metadata": {},
   "source": [
    "## Get Kafka and Spark ready"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e149a414",
   "metadata": {},
   "source": [
    "### Standalone cluster deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88481fb2",
   "metadata": {},
   "source": [
    "We can now initialize all the required variables with `findspark.init()` by passing the path to the spark folder we downloaded previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39d3a5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/home/saverio/spark-3.1.2-bin-hadoop3.2/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f5eaee",
   "metadata": {},
   "source": [
    "First we need to start the master, This will spin up the spark master with address spark://localhost:7077 and a cluster dashboark at localhost:8080.\n",
    "\n",
    "We can now create a worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0736488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting org.apache.spark.deploy.master.Master, logging to /home/saverio/spark-3.1.2-bin-hadoop3.2//logs/spark-saverio-org.apache.spark.deploy.master.Master-1-saverio-PU301LA.out\n",
      "starting org.apache.spark.deploy.worker.Worker, logging to /home/saverio/spark-3.1.2-bin-hadoop3.2//logs/spark-saverio-org.apache.spark.deploy.worker.Worker-1-saverio-PU301LA.out\n"
     ]
    }
   ],
   "source": [
    "%%script bash --no-raise-error\n",
    "\n",
    "# start master \n",
    "$SPARK_HOME/sbin/start-master.sh --host localhost \\\n",
    "    --port 7077 --webui-port 8080\n",
    "    \n",
    "# start worker\n",
    "$SPARK_HOME/sbin/start-worker.sh spark://localhost:7077 \\\n",
    "    --cores 2 --memory 1g\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d18b2d7",
   "metadata": {},
   "source": [
    "## Create the spark session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24d00a0",
   "metadata": {},
   "source": [
    "We can now create the spark session. With the following command we are asking to the master (and resource manager) to create an application with required resources and configurations. In this case we are using all the default options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40e94454",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/08/21 16:09:00 WARN Utils: Your hostname, saverio-PU301LA resolves to a loopback address: 127.0.1.1; using 192.168.1.24 instead (on interface wlp3s0)\n",
      "21/08/21 16:09:00 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/saverio/spark-3.1.2-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/saverio/spark-3.1.2-bin-hadoop3.2/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/saverio/.ivy2/cache\n",
      "The jars for the packages stored in: /home/saverio/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-249b0cb4-ffbd-4558-8091-7ff1b41bb78f;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.1.1 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.1.1 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.6.0 in central\n",
      "\tfound com.github.luben#zstd-jni;1.4.8-1 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.2 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      ":: resolution report :: resolve 615ms :: artifacts dl 15ms\n",
      "\t:: modules in use:\n",
      "\tcom.github.luben#zstd-jni;1.4.8-1 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.6.0 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.1.1 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.1.1 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.2 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   9   |   0   |   0   |   0   ||   9   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-249b0cb4-ffbd-4558-8091-7ff1b41bb78f\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 9 already retrieved (0kB/14ms)\n",
      "21/08/21 16:09:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "KAFKA_BOOTSTRAP_SERVERS = ''\n",
    "\n",
    "    \n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"spark://localhost:7077\")\\\n",
    "    .appName(\"Spark structured streaming application\")\\\n",
    "    .config(\"spark.jars.packages\",\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.1\")\\\n",
    "    .getOrCreate()\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c00f187",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.24:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://localhost:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark structured streaming application</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fbe56091040>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d515875",
   "metadata": {},
   "source": [
    "## KAFKA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1b41e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "KAFKA_HOME = '/home/saverio/kafka_2.13-2.8.0'\n",
    "KAFKA_BOOTSTRAP_SERVERS = 'localhost:9092'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "536dd111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/saverio/kafka_2.13-2.8.0/bin/zookeeper-server-start.sh /home/saverio/kafka_2.13-2.8.0/config/zookeeper.properties\n",
      "/home/saverio/kafka_2.13-2.8.0/bin/kafka-server-start.sh /home/saverio/kafka_2.13-2.8.0/config/server.properties\n"
     ]
    }
   ],
   "source": [
    "#By some reason I can't launch this from here using OS, so i open the terminals in the KAFKA_HOME folder\n",
    "# and launch the zookeper and the kafka server comands manually\n",
    "\n",
    "\n",
    "# Start Zookeeper\n",
    "# bin/zookeeper-server-start.sh config/zookeeper.properties \n",
    "print('{0}/bin/zookeeper-server-start.sh {0}/config/zookeeper.properties'.format(KAFKA_HOME)) \n",
    "    \n",
    "# Start one Kafka Broker\n",
    "#bin/kafka-server-start.sh config/server.properties\n",
    "print('{0}/bin/kafka-server-start.sh {0}/config/server.properties'.format(KAFKA_HOME)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08e50af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d0a8fcbf",
   "metadata": {},
   "source": [
    "### Create the topics for kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0c7471f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CreateTopicsResponse_v3(throttle_time_ms=0, topic_errors=[(topic='Experiment_measurements', error_code=0, error_message=None), (topic='results', error_code=0, error_message=None)])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from kafka.admin import KafkaAdminClient, NewTopic\n",
    "kafka_admin = KafkaAdminClient(\n",
    "        bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,\n",
    "    )\n",
    "\n",
    "#Here we will inject the data\n",
    "new_topic_a = NewTopic(name='Experiment_measurements', \n",
    "                       num_partitions=1, \n",
    "                       replication_factor=1)\n",
    "\n",
    "#Here we inject the number of processed hits, post cleaning\n",
    "new_topic_b = NewTopic(name='results', \n",
    "                       num_partitions=1, \n",
    "                       replication_factor=1)\n",
    "\n",
    "kafka_admin.create_topics(new_topics=[new_topic_a,new_topic_b])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ecf70ea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['results', 'Experiment_measurements']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kafka_admin.list_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f58e90",
   "metadata": {},
   "source": [
    "## KAFKA - SPARK INTEGRATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c5f9f0",
   "metadata": {},
   "source": [
    "### Read the data from the kafka topic (define the consumer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "581fc2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDF = spark\\\n",
    "    .readStream\\\n",
    "    .format(\"kafka\")\\\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP_SERVERS)\\\n",
    "    .option('subscribe', 'Experiment_measurements')\\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8445674",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inputDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92c9e3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json, col, when\n",
    "from pyspark.sql.types import StructField, StructType, DoubleType, IntegerType\n",
    "\n",
    "## the schema of the json data format used to create the messages\n",
    "schema = StructType(\n",
    "        [\n",
    "                StructField(\"HEAD\",        IntegerType()),\n",
    "                StructField(\"FPGA\",        IntegerType()),\n",
    "                StructField(\"TDC_CHANNEL\", IntegerType()),\n",
    "                StructField(\"ORBIT_CNT\",   DoubleType()),\n",
    "                StructField(\"BX_COUNTER\",  IntegerType()),\n",
    "                StructField(\"TDC_MEAS\",    DoubleType())\n",
    "        ]  \n",
    ")\n",
    "\n",
    "## a new DF can be created from the previous by using the pyspark.sql functions\n",
    "jsonDF = inputDF.select(from_json(col(\"value\").alias('value').cast(\"string\"), schema).alias('value'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e51f3af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: struct (nullable = true)\n",
      " |    |-- HEAD: integer (nullable = true)\n",
      " |    |-- FPGA: integer (nullable = true)\n",
      " |    |-- TDC_CHANNEL: integer (nullable = true)\n",
      " |    |-- ORBIT_CNT: double (nullable = true)\n",
      " |    |-- BX_COUNTER: integer (nullable = true)\n",
      " |    |-- TDC_MEAS: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jsonDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e85552bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#jsonDF.writeStream\\\n",
    "#   .outputMode(\"append\")\\\n",
    "#   .format(\"console\")\\\n",
    "#   .start()\\\n",
    "#   .awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "194ce688",
   "metadata": {},
   "outputs": [],
   "source": [
    "flatDF = jsonDF.selectExpr(\"value.HEAD\", \n",
    "                           \"value.FPGA\", \n",
    "                           \"value.TDC_CHANNEL\",\n",
    "                           \"value.ORBIT_CNT\",\n",
    "                           \"value.BX_COUNTER\",\n",
    "                           \"value.TDC_MEAS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c998e0a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- HEAD: integer (nullable = true)\n",
      " |-- FPGA: integer (nullable = true)\n",
      " |-- TDC_CHANNEL: integer (nullable = true)\n",
      " |-- ORBIT_CNT: double (nullable = true)\n",
      " |-- BX_COUNTER: integer (nullable = true)\n",
      " |-- TDC_MEAS: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flatDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e780d855",
   "metadata": {},
   "outputs": [],
   "source": [
    "#flatDF.writeStream\\\n",
    "#   .outputMode(\"append\")\\\n",
    "#   .format(\"console\")\\\n",
    "#   .start()\\\n",
    "#   .awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4850e120",
   "metadata": {},
   "source": [
    "### SPARK processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dcf3f256",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "#Keep the events where \"HEAD\"=2\n",
    "cleanDF = flatDF.where(col('HEAD')==2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "addb605f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computations(DF, epoch):\n",
    "    #This function perform the whole operations on the received batch,\n",
    "    #\n",
    "\n",
    "    #As the 4 calculations that we have to perform are done foe each chamber we set 4 dataframes\n",
    "    chamber_1 = DF.filter((col(\"FPGA\") == 0) & (col(\"TDC_CHANNEL\")>=0) & (col(\"TDC_CHANNEL\")<=63))\n",
    "    chamber_2 = DF.filter((col(\"FPGA\") == 0) & (col(\"TDC_CHANNEL\")>=64) & (col(\"TDC_CHANNEL\")<=127))\n",
    "    chamber_3 = DF.filter((col(\"FPGA\") == 1) & (col(\"TDC_CHANNEL\")>=0) & (col(\"TDC_CHANNEL\")<=63))\n",
    "    chamber_4 = DF.filter((col(\"FPGA\") == 1) & (col(\"TDC_CHANNEL\")>=64) & (col(\"TDC_CHANNEL\")<=127))\n",
    "\n",
    "    #Now we can count the number of events in each chamber\n",
    "    n_c1 = chamber_1.count()\n",
    "    n_c2 = chamber_2.count()\n",
    "    n_c3 = chamber_3.count()\n",
    "    n_c4 = chamber_4.count()\n",
    "\n",
    "    #Total number of events\n",
    "    n = n_c1 + n_c2 + n_c3 + n_c4\n",
    "\n",
    "\n",
    "    #Histograms    \n",
    "    h_c1 = chamber_1.groupBy('TDC_CHANNEL').count().collect()\n",
    "    h_c2 = chamber_2.groupBy('TDC_CHANNEL').count().collect()\n",
    "    h_c3 = chamber_3.groupBy('TDC_CHANNEL').count().collect()\n",
    "    h_c4 = chamber_4.groupBy('TDC_CHANNEL').count().collect()\n",
    "\n",
    "    h_active_1 = chamber_1.groupBy('TDC_CHANNEL','ORBIT_CNT').count().collect()\n",
    "    h_active_2 = chamber_2.groupBy('TDC_CHANNEL','ORBIT_CNT').count().collect()\n",
    "    h_active_3 = chamber_3.groupBy('TDC_CHANNEL','ORBIT_CNT').count().collect()\n",
    "    h_active_4 = chamber_4.groupBy('TDC_CHANNEL','ORBIT_CNT').count().collect()\n",
    "    \n",
    "    \n",
    "    #Organise the results to send them to one topic as a dictionary\n",
    "    results = {'Total_events': n,\n",
    "              'Events_per_chamber': [n_c1,n_c2,n_c3,n_c4],\n",
    "              'Histogram_1': [h_c1, h_c2, h_c3, h_c4],\n",
    "              'Histogram_2': [h_active_1,h_active_2,h_active_3,h_active_4]}\n",
    "    \n",
    "    #publish the results in the \"results\" topic for further usage\n",
    "    producer.send(topic='results', value=json.dumps(results).encode('utf-8'))\n",
    "    producer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f1e5d3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computations_2(DF, epoch):\n",
    "    start=time.time()\n",
    "    #This function perform the whole operations on the received batch,\n",
    "    #\n",
    "\n",
    "    #As the 4 calculations that we have to perform are done foe each chamber we set 4 dataframes\n",
    "    chamber_1 = DF.filter((col(\"FPGA\") == 0) & (col(\"TDC_CHANNEL\")>=0) & (col(\"TDC_CHANNEL\")<=63))\n",
    "    chamber_2 = DF.filter((col(\"FPGA\") == 0) & (col(\"TDC_CHANNEL\")>=64) & (col(\"TDC_CHANNEL\")<=127))\n",
    "    chamber_3 = DF.filter((col(\"FPGA\") == 1) & (col(\"TDC_CHANNEL\")>=0) & (col(\"TDC_CHANNEL\")<=63))\n",
    "    chamber_4 = DF.filter((col(\"FPGA\") == 1) & (col(\"TDC_CHANNEL\")>=64) & (col(\"TDC_CHANNEL\")<=127))\n",
    "\n",
    "    #Initialize results dictionary\n",
    "    results = {}\n",
    "    results[\"Total Count\"] = {}\n",
    "    chamber_name = [\"Chamber_1\", \"Chamber_2\", \"Chamber_3\", \"Chamber_4\"]\n",
    "    for chamber in chamber_name:\n",
    "        results[chamber] = {}\n",
    "        results[chamber][\"Count\"] = {}\n",
    "        \n",
    "    #Now we can count the number of events in each chamber\n",
    "    results[\"Chamber_1\"][\"Count\"] = chamber_1.count()\n",
    "    results[\"Chamber_2\"][\"Count\"] = chamber_2.count()\n",
    "    results[\"Chamber_3\"][\"Count\"] = chamber_3.count()\n",
    "    results[\"Chamber_4\"][\"Count\"] = chamber_4.count()\n",
    "\n",
    "    #Total number of events\n",
    "    results[\"Total Count\"] = results[\"Chamber_1\"][\"Count\"] + results[\"Chamber_2\"][\"Count\"] + \\\n",
    "                             results[\"Chamber_3\"][\"Count\"] + results[\"Chamber_4\"][\"Count\"]\n",
    "    \n",
    "\n",
    "    # Compute histograms for each chamber   \n",
    "    i=0    \n",
    "    for chamber in [chamber_1, chamber_2, chamber_3, chamber_4]:\n",
    "        #Initialize dictionary partitions to save the results\n",
    "        for hist in [\"Hist_1\",\"Hist_2\"]:\n",
    "            results[chamber_name[i]][hist] = {}\n",
    "            results[chamber_name[i]][hist][\"Bins\"] = {}\n",
    "            results[chamber_name[i]][hist][\"Counts\"] = {}\n",
    "        \n",
    "        if(chamber.count()!=0): \n",
    "            #Histogram 1\n",
    "            bins, counts = (\n",
    "                chamber.select(\"TDC_CHANNEL\")\n",
    "                .rdd.map(lambda x: x.TDC_CHANNEL)\n",
    "                .histogram(list(np.arange(0,170,5)))\n",
    "            )\n",
    "            \n",
    "            results[chamber_name[i]][\"Hist_1\"][\"Bins\"] = bins\n",
    "            results[chamber_name[i]][\"Hist_1\"][\"Counts\"] = counts\n",
    "            \n",
    "            #Histogram 2\n",
    "            bins, counts = (\n",
    "            chamber.groupBy(\"TDC_CHANNEL\",\"ORBIT_CNT\")\n",
    "            .count()\n",
    "            .select(\"ORBIT_CNT\")\n",
    "            .rdd.map(lambda x: x.ORBIT_CNT)\n",
    "            .histogram(list(np.arange(6.e5,1.e7,0.5e6)))\n",
    "            )\n",
    "            \n",
    "            results[chamber_name[i]][\"Hist_2\"][\"Bins\"] = bins\n",
    "            results[chamber_name[i]][\"Hist_2\"][\"Counts\"] = counts            \n",
    "            \n",
    "        else:\n",
    "            #Histogram 1\n",
    "            results[chamber_name[i]][\"Hist_1\"][\"Bins\"] = list(np.arange(0,170,5))\n",
    "            counts = list(np.arange(0,170,5)* 0) \n",
    "            results[chamber_name[i]][\"Hist_1\"][\"Counts\"] = counts\n",
    "            \n",
    "            #Histogram 2\n",
    "            results[chamber_name[i]][\"Hist_2\"][\"Bins\"] = list(np.arange(6.e5,1.e7,0.5e6))\n",
    "            counts = list(np.arange(6.e5,1.e7,0.5e6)* 0) \n",
    "            results[chamber_name[i]][\"Hist_2\"][\"Counts\"] = counts\n",
    "        i +=1\n",
    "    \n",
    "    end =time.time()\n",
    "    print(\"Time =\",end-start)\n",
    "    producer.send(topic=\"results\", value= str(results).encode('utf-8'))\n",
    "    producer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "162d6953",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computations_3(DF, epoch):\n",
    "    start=time.time()\n",
    "    #This function perform the whole operations on the received batch,\n",
    "    \n",
    "    #Add a column with the chamber number\n",
    "    DF_new = DF.withColumn('chamber',when((col(\"FPGA\") == 0) & (col(\"TDC_CHANNEL\")>=0) & (col(\"TDC_CHANNEL\")<=63),1).\n",
    "                                 when((col(\"FPGA\") == 0) & (col(\"TDC_CHANNEL\")>=64) & (col(\"TDC_CHANNEL\")<=127),2).\n",
    "                                 when((col(\"FPGA\") == 1) & (col(\"TDC_CHANNEL\")>=0) & (col(\"TDC_CHANNEL\")<=63),3).\n",
    "                                 when((col(\"FPGA\") == 1) & (col(\"TDC_CHANNEL\")>=64) & (col(\"TDC_CHANNEL\")<=127),4)).\\\n",
    "                                 select([ col('TDC_CHANNEL'), col('ORBIT_CNT'),\n",
    "                                    col('BX_COUNTER'),col('TDC_MEAS'),\n",
    "                                    col('chamber')])\n",
    "    DF_new.persist()\n",
    "    #DF_new.show()\n",
    "    #Initialize results dictionary\n",
    "    results = {}\n",
    "    results[\"Total Count\"] = {}\n",
    "    results[\"Index\"] = time.time()\n",
    "    chamber_name = [\"Chamber_1\", \"Chamber_2\", \"Chamber_3\", \"Chamber_4\"]\n",
    "    for chamber in chamber_name:\n",
    "        results[chamber] = {}\n",
    "        results[chamber][\"Count\"] = {}\n",
    "        for hist in [\"Hist_1\",\"Hist_2\"]:\n",
    "            results[chamber][hist] = {}\n",
    "            results[chamber][hist][\"Bins\"] = {}\n",
    "            results[chamber][hist][\"Counts\"] = {}\n",
    "        \n",
    "    # Compute histograms for each chamber   \n",
    "    for i in [1,2,3,4]:\n",
    "        #Now we can count the number of events in each chamber\n",
    "        chamber = DF_new.filter(col(\"chamber\") == i).persist()\n",
    "        results[f\"Chamber_{i}\"][\"Count\"] = chamber.count()\n",
    "        \n",
    "        if(results[f\"Chamber_{i}\"][\"Count\"]!=0):\n",
    "            \n",
    "            #Histogram 1\n",
    "            bins, counts = (\n",
    "            chamber.select(\"TDC_CHANNEL\")\n",
    "                 .rdd.map(lambda x: x.TDC_CHANNEL)\n",
    "                 .histogram(list(np.arange(0,170,5)))\n",
    "            )\n",
    "            \n",
    "            results[f\"Chamber_{i}\"][\"Hist_1\"][\"Bins\"] = bins\n",
    "            results[f\"Chamber_{i}\"][\"Hist_1\"][\"Counts\"] = counts\n",
    "            \n",
    "            #Histogram 2\n",
    "            bins, counts = (\n",
    "            chamber.groupBy(\"TDC_CHANNEL\",\"ORBIT_CNT\")\n",
    "            .count()\n",
    "            .select(\"ORBIT_CNT\")\n",
    "            .rdd.map(lambda x: x.ORBIT_CNT)\n",
    "            .histogram(list(np.arange(6.e5,1.e7,0.5e6)))\n",
    "            )\n",
    "            \n",
    "            results[f\"Chamber_{i}\"][\"Hist_2\"][\"Bins\"] = bins\n",
    "            results[f\"Chamber_{i}\"][\"Hist_2\"][\"Counts\"] = counts            \n",
    "                \n",
    "        else:\n",
    "            #Histogram 1\n",
    "            results[f\"Chamber_{i}\"][\"Hist_1\"][\"Bins\"] = list(np.arange(0,170,5))\n",
    "            counts = list(np.arange(0,170,5)* 0) \n",
    "            results[f\"Chamber_{i}\"][\"Hist_1\"][\"Counts\"] = counts\n",
    "            \n",
    "             #Histogram 2\n",
    "            results[f\"Chamber_{i}\"][\"Hist_2\"][\"Bins\"] = list(np.arange(6.e5,1.e7,0.5e6))\n",
    "            counts = list(np.arange(6.e5,1.e7,0.5e6)* 0) \n",
    "            results[f\"Chamber_{i}\"][\"Hist_2\"][\"Counts\"] = counts\n",
    "        chamber = DF_new.filter(col(\"chamber\") == i).unpersist()  \n",
    "        \n",
    "    results[\"Total Count\"] = results[\"Chamber_1\"][\"Count\"] + results[\"Chamber_2\"][\"Count\"] + \\\n",
    "                             results[\"Chamber_3\"][\"Count\"] + results[\"Chamber_4\"][\"Count\"]\n",
    "    end =time.time()\n",
    "    #print(\"Time =\",end-start)\n",
    "       \n",
    "    \n",
    "    producer.send(topic=\"results\", value= str(results).encode('utf-8'))\n",
    "    producer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d94126a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer\n",
    "\n",
    "#Send the results to the kafka topic\n",
    "#Initialize the producer\n",
    "producer = KafkaProducer(bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253b9d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/08/21 16:09:34 WARN StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-f6088481-eb8e-49d4-b84e-7b0fe34418bb. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "21/08/21 16:09:41 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 6000 milliseconds, but spent 6931 milliseconds\n",
      "21/08/21 16:10:24 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 6000 milliseconds, but spent 36347 milliseconds\n",
      "21/08/21 16:10:43 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 6000 milliseconds, but spent 19354 milliseconds\n",
      "21/08/21 16:11:00 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 6000 milliseconds, but spent 17050 milliseconds\n",
      "21/08/21 16:11:16 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 6000 milliseconds, but spent 16223 milliseconds\n",
      "21/08/21 16:11:32 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 6000 milliseconds, but spent 15259 milliseconds\n",
      "21/08/21 16:11:48 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 6000 milliseconds, but spent 16327 milliseconds\n",
      "21/08/21 16:12:06 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 6000 milliseconds, but spent 18067 milliseconds\n",
      "21/08/21 16:12:22 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 6000 milliseconds, but spent 15702 milliseconds\n",
      "21/08/21 16:12:36 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 6000 milliseconds, but spent 14652 milliseconds\n",
      "21/08/21 16:12:54 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 6000 milliseconds, but spent 17164 milliseconds\n",
      "21/08/21 16:13:11 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 6000 milliseconds, but spent 17109 milliseconds\n",
      "21/08/21 16:13:25 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 6000 milliseconds, but spent 13908 milliseconds\n",
      "21/08/21 16:13:38 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 6000 milliseconds, but spent 13370 milliseconds\n",
      "21/08/21 16:13:54 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 6000 milliseconds, but spent 15836 milliseconds\n",
      "21/08/21 16:14:08 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 6000 milliseconds, but spent 13999 milliseconds\n",
      "21/08/21 16:14:22 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 6000 milliseconds, but spent 13994 milliseconds\n",
      "21/08/21 16:14:36 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 6000 milliseconds, but spent 13751 milliseconds\n",
      "21/08/21 16:14:50 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 6000 milliseconds, but spent 13944 milliseconds\n",
      "21/08/21 16:15:03 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 6000 milliseconds, but spent 13351 milliseconds\n",
      "21/08/21 16:15:17 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 6000 milliseconds, but spent 14213 milliseconds\n",
      "21/08/21 16:15:32 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 6000 milliseconds, but spent 14486 milliseconds\n",
      "21/08/21 16:15:45 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 6000 milliseconds, but spent 13801 milliseconds\n",
      "21/08/21 16:15:59 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 6000 milliseconds, but spent 13878 milliseconds\n",
      "21/08/21 16:16:13 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 6000 milliseconds, but spent 13790 milliseconds\n",
      "21/08/21 16:16:28 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 6000 milliseconds, but spent 14889 milliseconds\n",
      "21/08/21 16:16:43 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 6000 milliseconds, but spent 14695 milliseconds\n",
      "21/08/21 16:16:57 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 6000 milliseconds, but spent 14811 milliseconds\n",
      "21/08/21 16:17:12 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 6000 milliseconds, but spent 14650 milliseconds\n",
      "21/08/21 16:17:26 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 6000 milliseconds, but spent 14251 milliseconds\n",
      "21/08/21 16:17:40 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 6000 milliseconds, but spent 13887 milliseconds\n",
      "21/08/21 16:17:55 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 6000 milliseconds, but spent 15007 milliseconds\n",
      "21/08/21 16:18:10 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 6000 milliseconds, but spent 14762 milliseconds\n",
      "21/08/21 16:18:24 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 6000 milliseconds, but spent 14254 milliseconds\n",
      "21/08/21 16:18:38 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 6000 milliseconds, but spent 13523 milliseconds\n",
      "21/08/21 16:18:51 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 6000 milliseconds, but spent 13306 milliseconds\n",
      "21/08/21 16:19:05 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 6000 milliseconds, but spent 13840 milliseconds\n",
      "21/08/21 16:19:20 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 6000 milliseconds, but spent 14971 milliseconds\n",
      "21/08/21 16:19:34 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 6000 milliseconds, but spent 14397 milliseconds\n",
      "21/08/21 16:19:49 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 6000 milliseconds, but spent 14602 milliseconds\n",
      "21/08/21 16:20:03 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 6000 milliseconds, but spent 13730 milliseconds\n",
      "21/08/21 16:20:17 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 6000 milliseconds, but spent 14356 milliseconds\n",
      "21/08/21 16:20:32 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 6000 milliseconds, but spent 14664 milliseconds\n",
      "21/08/21 16:20:45 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 6000 milliseconds, but spent 13139 milliseconds\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Trigger the processing\n",
    "cleanDF.writeStream\\\n",
    "    .foreachBatch(computations_3)\\\n",
    "    .trigger(processingTime='6 second')\\\n",
    "    .start()\\\n",
    "    .awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fd73b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5484d6cf",
   "metadata": {},
   "source": [
    "If you also want to delete any data of your local Kafka environment including any events you have created along the way, run the command:\n",
    "\n",
    "`` $ rm -rf /tmp/kafka-logs /tmp/zookeeper `` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f22ff1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
