{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4113484",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init('/usr/local/spark/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7f7ab0f-9050-4976-9b67-000cc1042b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting org.apache.spark.deploy.master.Master, logging to /usr/local/spark//logs/spark-saverio-org.apache.spark.deploy.master.Master-1-saverio-PU301LA.out\n",
      "starting org.apache.spark.deploy.worker.Worker, logging to /usr/local/spark//logs/spark-saverio-org.apache.spark.deploy.worker.Worker-1-saverio-PU301LA.out\n"
     ]
    }
   ],
   "source": [
    "# start master \n",
    "!$SPARK_HOME/sbin/start-master.sh --host localhost \\\n",
    "    --port 7077 --webui-port 8080\n",
    "    \n",
    "# start worker\n",
    "!$SPARK_HOME/sbin/start-worker.sh spark://localhost:7077 \\\n",
    "    --cores 2 --memory 1g\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bea9017",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/08/30 16:34:06 WARN Utils: Your hostname, saverio-PU301LA resolves to a loopback address: 127.0.1.1; using 192.168.1.24 instead (on interface wlp3s0)\n",
      "21/08/30 16:34:06 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "21/08/30 16:34:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"spark://localhost:7077\")\\\n",
    "    .appName(\"First\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b818a9fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.24:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://localhost:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>First</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f7ae567d610>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c72d8c72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.24:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://localhost:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>First</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=spark://localhost:7077 appName=First>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get spark context -> entry point used to work with RDD\n",
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "225a312d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "from pyspark.sql.functions import col, when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f19af8b-cd10-4377-9ce5-649aee6cfa2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json, col, when\n",
    "from pyspark.sql.types import StructField, StructType, DoubleType, IntegerType\n",
    "\n",
    "schema = StructType(\n",
    "        [\n",
    "                StructField(\"HEAD\",        IntegerType()),\n",
    "                StructField(\"FPGA\",        IntegerType()),\n",
    "                StructField(\"TDC_CHANNEL\", IntegerType()),\n",
    "                StructField(\"ORBIT_CNT\",   DoubleType()),\n",
    "                StructField(\"BX_COUNTER\",  IntegerType()),\n",
    "                StructField(\"TDC_MEAS\",    DoubleType())\n",
    "        ]  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "628357a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset on dataset/lecture2/dimuon\n",
    "inputDF = spark.read \\\n",
    "    .option(\"inferTimestamp\",\"false\") \\\n",
    "    .option(\"prefersDecimal\",\"false\") \\\n",
    "    .option(\"header\",\"True\")\\\n",
    "    .schema(schema)\\\n",
    "    .format('csv') \\\n",
    "    .load('./Data/data_000000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5d56370",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inputDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "756b5140",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "results[\"Total Count\"] = {}\n",
    "chamber_name = [\"Chamber_1\", \"Chamber_2\", \"Chamber_3\", \"Chamber_4\"]\n",
    "for chamber in chamber_name:\n",
    "    results[chamber] = {}\n",
    "    results[chamber][\"Count\"] = {}\n",
    "    for hist in [\"Hist_1\",\"Hist_2\"]:\n",
    "        results[chamber][hist] = {}\n",
    "        results[chamber][hist][\"Bins\"] = {}\n",
    "        results[chamber][hist][\"Counts\"] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9322187c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keep the events where \"HEAD\"=2\n",
    "cleanDF = inputDF.where(col('HEAD')==2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d2b9edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleanDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4957667d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chamberDF = cleanDF.withColumn('chamber',when((col(\"FPGA\") == 0) & (col(\"TDC_CHANNEL\")<=63),1).\n",
    "                                 when((col(\"FPGA\") == 0) & (col(\"TDC_CHANNEL\")>=64),2).\n",
    "                                 when((col(\"FPGA\") == 1) & (col(\"TDC_CHANNEL\")<=63),3).\n",
    "                                 when((col(\"FPGA\") == 1) & (col(\"TDC_CHANNEL\")>=64),4)).\\\n",
    "                                 select([ col('TDC_CHANNEL'), col('ORBIT_CNT'),\n",
    "                                    col('BX_COUNTER'),col('TDC_MEAS'),\n",
    "                                    col('chamber')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09d16a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 19:====================================================> (196 + 2) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41.836082458496094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "start =time.time()\n",
    "for i in [1,2,3,4]:\n",
    "    #Now we can count the number of events in each chamber\n",
    "    chamber = chamberDF.filter(col(\"chamber\") == i).persist()\n",
    "    results[f\"Chamber_{i}\"][\"Count\"] = chamber.count()\n",
    "    \n",
    "    if(results[f\"Chamber_{i}\"][\"Count\"]!=0):\n",
    "            \n",
    "        #Histogram 1\n",
    "        bins, counts = (\n",
    "        chamber.select(\"TDC_CHANNEL\")\n",
    "                .rdd.map(lambda x: x.TDC_CHANNEL)\n",
    "                .histogram(list(np.arange(0,170,5)))\n",
    "        )\n",
    "            \n",
    "        results[f\"Chamber_{i}\"][\"Hist_1\"][\"Bins\"] = bins\n",
    "        results[f\"Chamber_{i}\"][\"Hist_1\"][\"Counts\"] = counts\n",
    "            \n",
    "        #Histogram 2\n",
    "        bins, counts = (\n",
    "        chamber.groupBy(\"TDC_CHANNEL\",\"ORBIT_CNT\")\n",
    "        .count()\n",
    "        .select(\"ORBIT_CNT\")\n",
    "        .rdd.map(lambda x: x.ORBIT_CNT)\n",
    "        .histogram(list(np.arange(6.e5,1.e7,0.5e6)))\n",
    "        )\n",
    "            \n",
    "        results[f\"Chamber_{i}\"][\"Hist_2\"][\"Bins\"] = bins\n",
    "        results[f\"Chamber_{i}\"][\"Hist_2\"][\"Counts\"] = counts\n",
    "    chamber.unpersist()\n",
    "end =time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4a31130f-1a6e-4741-ad71-24c758da3701",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "af257966-ce20-477d-9bcc-09ba4faadce3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def computations_8(DF):\n",
    "    start=time.time()\n",
    "    #This function perform the whole operations on the received batch,\n",
    "    \n",
    "    #Add a column with the chamber number\n",
    "    DF_clean     = DF.filter(col(\"HEAD\")==2)\n",
    "    DF_clean.filter(col(\"HEAD\") < 128 )\n",
    "    DF_hit = DF_clean.withColumn('CHAMBER',when(col(\"FPGA\") == 0, \n",
    "                                                when(col(\"TDC_CHANNEL\")<=63,1).\\\n",
    "                                                otherwise(2)).\\\n",
    "                                           otherwise(when(col(\"TDC_CHANNEL\")<=63,3).\\\n",
    "                                                otherwise(4)\n",
    "                                           )).\\\n",
    "                                           select([ col('TDC_CHANNEL'), col('ORBIT_CNT'),\n",
    "                                           col('BX_COUNTER'),col('TDC_MEAS'),\n",
    "                                           col('CHAMBER')])\n",
    "    \n",
    "    #Initialize results dictionary\n",
    "    results = {}\n",
    "    results[\"Total Count\"] = {}\n",
    "    results[\"Index\"] = time.time()\n",
    "    chamber_name = [\"Chamber_1\", \"Chamber_2\", \"Chamber_3\", \"Chamber_4\"]\n",
    "    for chamber in chamber_name:\n",
    "        results[chamber] = {}\n",
    "        results[chamber][\"Count\"] = {}\n",
    "        for hist in [\"Hist_1\",\"Hist_2\",\"Hist_3\",\"Hist_4\"]:\n",
    "            results[chamber][hist] = {}\n",
    "            results[chamber][hist][\"Bins\"] = {}\n",
    "            results[chamber][hist][\"Counts\"] = {}\n",
    "            \n",
    "    #We prepare the scilantor data\n",
    "    #First we filter the events encoding the passage time,\n",
    "    #then we try to keep only one scilantor hit per orbit (the samaller)\n",
    "    DF_hit.filter(col('ORBIT_CNT')==617015).show()\n",
    "    \n",
    "    w = Window.partitionBy(['ORBIT_CNT'])\n",
    "    DF_test = DF_hit.withColumn('minTDC', f.min('TDC_MEAS').over(w))\\\n",
    "        .where(f.col('TDC_MEAS') == f.col('minTDC'))\\\n",
    "        .drop('minTDC')\n",
    "    \n",
    "    DF_test.filter(col('ORBIT_CNT')==617015).show()\n",
    "    '''DF_scilantor = DF_hit.groupBy(\"ORBIT_CNT\").agg(f.max('BX_COUNTER')).\\\n",
    "                      drop(\"ORBIT_CNT\").\\\n",
    "                      withColumnRenamed(\"min(ORBIT_CNT)\",\"ORBIT_CNT_sci\").\\\n",
    "                      withColumnRenamed(\"min(BX_COUNTER)\",\"BX_COUNTER_sci\").\\\n",
    "                      withColumnRenamed(\"min(TDC_MEAS)\",\"TDC_MEAS_sci\")'''\n",
    "    \n",
    "    \n",
    "    DF_scilantor.filter(col('ORBIT_CNT')==617015).show()\n",
    "    \n",
    "    #Add the PASSAGETIME time\n",
    "    DF_scilantor = DF_scilantor.withColumn(\"PASSAGETIME\", 25 * (col(\"ORBIT_CNT_sci\") * 3564 +\n",
    "                                                                col(\"BX_COUNTER_sci\") + \n",
    "                                                                col(\"TDC_MEAS_sci\")/30))\n",
    "    \n",
    "    #Drop the columns with null values from DF_hit\n",
    "    DF_hit.na.drop(subset=[\"CHAMBER\"])\n",
    "    \n",
    "    # Compute histograms for each chamber   \n",
    "    for i in [1,2,3,4]:     \n",
    "        #Now we can count the number of events in each chamber\n",
    "        chamber = DF_hit.filter(col(\"CHAMBER\") == i).persist()\n",
    "        results[f\"Chamber_{i}\"][\"Count\"] = chamber.count()\n",
    "        \n",
    "        if(results[f\"Chamber_{i}\"][\"Count\"]!=0):\n",
    "            \n",
    "            #Histogram 1\n",
    "            bins_1, counts_1 = (\n",
    "            chamber.select(\"TDC_CHANNEL\")\n",
    "                 .rdd.map(lambda x: x.TDC_CHANNEL)\n",
    "                 .histogram(list(np.arange(0,170,5)))\n",
    "            )\n",
    "            \n",
    "            results[f\"Chamber_{i}\"][\"Hist_1\"][\"Bins\"] = bins_1\n",
    "            results[f\"Chamber_{i}\"][\"Hist_1\"][\"Counts\"] = counts_1\n",
    "            \n",
    "            #Histogram 2\n",
    "            bins_2, counts_2 = (\n",
    "            chamber.groupBy(\"TDC_CHANNEL\",\"ORBIT_CNT\")\n",
    "            .count()\n",
    "            .select(\"ORBIT_CNT\")\n",
    "            .rdd.map(lambda x: x.ORBIT_CNT)\n",
    "            .histogram(list(np.arange(6.e5,1.e7,0.5e6)))\n",
    "            )\n",
    "            \n",
    "            results[f\"Chamber_{i}\"][\"Hist_2\"][\"Bins\"] = bins_2\n",
    "            results[f\"Chamber_{i}\"][\"Hist_2\"][\"Counts\"] = counts_2            \n",
    "            \n",
    "            \n",
    "            #keep only the hits with a scintillator signal within the same orbit\n",
    "            chamber_sci = chamber.join(DF_scilantor,chamber.ORBIT_CNT ==  DF_scilantor.ORBIT_CNT_sci,\"inner\")\n",
    "\n",
    "            #Add the ABSSOLUTETIME \n",
    "            chamber_sci = chamber_sci.withColumn(\"ABSOLUTETIME\",\n",
    "                             25 * (col(\"ORBIT_CNT\") * 3564 + col(\"BX_COUNTER\") + col(\"TDC_MEAS\")/30))\n",
    "\n",
    "            #Drop useless data\n",
    "            chamber_sci = chamber_sci.drop(\"HEAD\").drop(\"FPGA\").drop(\"BX_COUNTER_sci\").drop(\"TDC_MEAS_sci\").drop(\"BX_COUNTER\").drop(\"ORBIT_CNT_sci\")\n",
    " \n",
    "            #Add DRIFTIME\n",
    "            chamber_sci = chamber_sci.withColumn(\"DRIFTIME\",col(\"ABSOLUTETIME\")-col(\"PASSAGETIME\") + 95)#.show()\n",
    "        \n",
    "            #Histogram 3\n",
    "            bins_3, counts_3 = (\n",
    "            chamber_sci.select(\"TDC_CHANNEL\")\n",
    "                 .rdd.map(lambda x: x.TDC_CHANNEL)\n",
    "                 .histogram(list(np.arange(0,170,5)))\n",
    "            )    \n",
    "            results[f\"Chamber_{i}\"][\"Hist_3\"][\"Bins\"] = bins_3\n",
    "            results[f\"Chamber_{i}\"][\"Hist_3\"][\"Counts\"] = counts_3\n",
    "            \n",
    "            #Histogram 4\n",
    "            bins_4, counts_4 = (\n",
    "            chamber_sci.select(\"DRIFTIME\")\n",
    "                 .rdd.map(lambda x: x.DRIFTIME)\n",
    "                 .histogram(list(np.arange(-100,1000,10)))\n",
    "            )\n",
    "            results[f\"Chamber_{i}\"][\"Hist_4\"][\"Bins\"] = bins_4\n",
    "            results[f\"Chamber_{i}\"][\"Hist_4\"][\"Counts\"] = counts_4\n",
    "            \n",
    "            \n",
    "            \n",
    "        else:\n",
    "            #Histogram 1\n",
    "            results[f\"Chamber_{i}\"][\"Hist_1\"][\"Bins\"] = list(np.arange(0,170,5))\n",
    "            counts = list(np.arange(0,170,5)* 0) \n",
    "            results[f\"Chamber_{i}\"][\"Hist_1\"][\"Counts\"] = counts\n",
    "            \n",
    "             #Histogram 2\n",
    "            results[f\"Chamber_{i}\"][\"Hist_2\"][\"Bins\"] = list(np.arange(6.e5,1.e7,0.5e6))\n",
    "            counts = list(np.arange(6.e5,1.e7,0.5e6)* 0) \n",
    "            results[f\"Chamber_{i}\"][\"Hist_2\"][\"Counts\"] = counts\n",
    "            \n",
    "            #Histogram 3\n",
    "            results[f\"Chamber_{i}\"][\"Hist_3\"][\"Bins\"] = list(np.arange(0,170,5))\n",
    "            counts = list(np.arange(0,170,5)* 0) \n",
    "            results[f\"Chamber_{i}\"][\"Hist_3\"][\"Counts\"] = counts\n",
    "            \n",
    "             #Histogram 4\n",
    "            results[f\"Chamber_{i}\"][\"Hist_4\"][\"Bins\"] = list(np.arange(-100,1000,10))\n",
    "            counts = list(np.arange(-100,1000,10)* 0) \n",
    "            results[f\"Chamber_{i}\"][\"Hist_4\"][\"Counts\"] = counts\n",
    "        chamber.unpersist()\n",
    "        \n",
    "    results[\"Total Count\"] = results[\"Chamber_1\"][\"Count\"] + results[\"Chamber_2\"][\"Count\"] + \\\n",
    "                             results[\"Chamber_3\"][\"Count\"] + results[\"Chamber_4\"][\"Count\"]\n",
    "    end =time.time()\n",
    "    print(\"Time =\",end-start)\n",
    "       \n",
    "    producer.send(topic=\"results\", value= str(results).encode('utf-8'))\n",
    "    #producer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8544f507",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+----------+--------+-------+\n",
      "|TDC_CHANNEL|ORBIT_CNT|BX_COUNTER|TDC_MEAS|CHAMBER|\n",
      "+-----------+---------+----------+--------+-------+\n",
      "|         12| 617015.0|      3126|    20.0|      3|\n",
      "|        115| 617015.0|      2678|    17.0|      2|\n",
      "+-----------+---------+----------+--------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+----------+--------+-------+\n",
      "|TDC_CHANNEL|ORBIT_CNT|BX_COUNTER|TDC_MEAS|CHAMBER|\n",
      "+-----------+---------+----------+--------+-------+\n",
      "|        115| 617015.0|      2678|    17.0|      2|\n",
      "+-----------+---------+----------+--------+-------+\n",
      "\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'DF_scilantor' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3730/2911787758.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcomputations_8\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleanDF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_3730/1339601239.py\u001b[0m in \u001b[0;36mcomputations_8\u001b[0;34m(DF)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mDF_scilantor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ORBIT_CNT'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m617015\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;31m#Add the PASSAGETIME time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'DF_scilantor' referenced before assignment"
     ]
    }
   ],
   "source": [
    "computations_8(cleanDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1197fcfa-ccc2-4cd9-9fc4-2d0413fab410",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03118384-f6ca-48ff-b771-3647dc287c98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
