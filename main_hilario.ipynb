{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e73b3fd",
   "metadata": {},
   "source": [
    "# Streaming with Kafka and Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827b85a1",
   "metadata": {},
   "source": [
    "Here I try to implement a basic pipeline for the project conecting kafka with spark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632ee96d",
   "metadata": {},
   "source": [
    "### Computer setting\n",
    "I downloaded and located in my home the spark file **spark-3.1.2-bin-hadoop3.2** and also the kafka file **kafka_2.13-2.7.0**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ea3a6e",
   "metadata": {},
   "source": [
    "## Get Kafka and Spark ready"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e149a414",
   "metadata": {},
   "source": [
    "### Standalone cluster deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88481fb2",
   "metadata": {},
   "source": [
    "We can now initialize all the required variables with `findspark.init()` by passing the path to the spark folder we downloaded previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d3a5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/home/hilario/spark-3.1.2-bin-hadoop3.2/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f5eaee",
   "metadata": {},
   "source": [
    "First we need to start the master, This will spin up the spark master with address spark://localhost:7077 and a cluster dashboark at localhost:8080.\n",
    "\n",
    "We can now create a worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0736488",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script bash --no-raise-error\n",
    "\n",
    "# start master \n",
    "$SPARK_HOME/sbin/start-master.sh --host localhost \\\n",
    "    --port 7077 --webui-port 8080\n",
    "    \n",
    "# start worker\n",
    "$SPARK_HOME/sbin/start-worker.sh spark://localhost:7077 \\\n",
    "    --cores 4 --memory 2g\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d18b2d7",
   "metadata": {},
   "source": [
    "## Create the spark session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24d00a0",
   "metadata": {},
   "source": [
    "We can now create the spark session. With the following command we are asking to the master (and resource manager) to create an application with required resources and configurations. In this case we are using all the default options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e94454",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "KAFKA_BOOTSTRAP_SERVERS = ''\n",
    "\n",
    "    \n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"spark://localhost:7077\")\\\n",
    "    .appName(\"Spark structured streaming application\")\\\n",
    "    .config(\"spark.jars.packages\",\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.1\")\\\n",
    "    .getOrCreate()\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c00f187",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d515875",
   "metadata": {},
   "source": [
    "## KAFKA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b41e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "KAFKA_HOME = '/home/hilario/kafka_2.13-2.7.0'\n",
    "KAFKA_BOOTSTRAP_SERVERS = 'localhost:9092'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536dd111",
   "metadata": {},
   "outputs": [],
   "source": [
    "#By some reason I can't launch this from here using OS, so i open the terminals in the KAFKA_HOME folder\n",
    "# and launch the zookeper and the kafka server comands manually\n",
    "\n",
    "\n",
    "# Start Zookeeper\n",
    "# bin/zookeeper-server-start.sh config/zookeeper.properties \n",
    "#os.system('{0}/bin/zookeeper-server-start.sh {0}/config/zookeeper.properties'.format(KAFKA_HOME)) \n",
    "    \n",
    "# Start one Kafka Broker\n",
    "#bin/kafka-server-start.sh config/server.properties\n",
    "#os.system('{0}/bin/kafka-server-start.sh {0}/config/server.properties'.format(KAFKA_HOME)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08e50af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d0a8fcbf",
   "metadata": {},
   "source": [
    "### Create the topics for kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c7471f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka.admin import KafkaAdminClient, NewTopic\n",
    "kafka_admin = KafkaAdminClient(\n",
    "        bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,\n",
    "    )\n",
    "\n",
    "#Here we will inject the data\n",
    "new_topic_a = NewTopic(name='Experiment_measurements', \n",
    "                       num_partitions=1, \n",
    "                       replication_factor=1)\n",
    "\n",
    "#Here we inject the number of processed hits, post cleaning\n",
    "new_topic_b = NewTopic(name='results', \n",
    "                       num_partitions=1, \n",
    "                       replication_factor=1)\n",
    "\n",
    "kafka_admin.create_topics(new_topics=[new_topic_a,new_topic_b])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf70ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_admin.list_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f58e90",
   "metadata": {},
   "source": [
    "## KAFKA - SPARK INTEGRATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c5f9f0",
   "metadata": {},
   "source": [
    "### Read the data from the kafka topic (define the consumer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581fc2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDF = spark\\\n",
    "    .readStream\\\n",
    "    .format(\"kafka\")\\\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP_SERVERS)\\\n",
    "    .option('subscribe', 'Experiment_measurements')\\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8445674",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inputDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c9e3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json, col, when\n",
    "from pyspark.sql.types import StructField, StructType, DoubleType, IntegerType\n",
    "\n",
    "## the schema of the json data format used to create the messages\n",
    "schema = StructType(\n",
    "        [\n",
    "                StructField(\"HEAD\",        IntegerType()),\n",
    "                StructField(\"FPGA\",        IntegerType()),\n",
    "                StructField(\"TDC_CHANNEL\", IntegerType()),\n",
    "                StructField(\"ORBIT_CNT\",   DoubleType()),\n",
    "                StructField(\"BX_COUNTER\",  IntegerType()),\n",
    "                StructField(\"TDC_MEAS\",    DoubleType())\n",
    "        ]  \n",
    ")\n",
    "\n",
    "## a new DF can be created from the previous by using the pyspark.sql functions\n",
    "jsonDF = inputDF.select(from_json(col(\"value\").alias('value').cast(\"string\"), schema).alias('value'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51f3af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85552bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#jsonDF.writeStream\\\n",
    "#   .outputMode(\"append\")\\\n",
    "#   .format(\"console\")\\\n",
    "#   .start()\\\n",
    "#   .awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194ce688",
   "metadata": {},
   "outputs": [],
   "source": [
    "flatDF = jsonDF.selectExpr(\"value.HEAD\", \n",
    "                           \"value.FPGA\", \n",
    "                           \"value.TDC_CHANNEL\",\n",
    "                           \"value.ORBIT_CNT\",\n",
    "                           \"value.BX_COUNTER\",\n",
    "                           \"value.TDC_MEAS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c998e0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "flatDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e780d855",
   "metadata": {},
   "outputs": [],
   "source": [
    "#flatDF.writeStream\\\n",
    "#   .outputMode(\"append\")\\\n",
    "#   .format(\"console\")\\\n",
    "#   .start()\\\n",
    "#   .awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4850e120",
   "metadata": {},
   "source": [
    "### SPARK processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf3f256",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "#Keep the events where \"HEAD\"=2\n",
    "cleanDF = flatDF.where(col('HEAD')==2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addb605f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computations(DF, epoch):\n",
    "    #This function perform the whole operations on the received batch,\n",
    "    #\n",
    "\n",
    "    #As the 4 calculations that we have to perform are done foe each chamber we set 4 dataframes\n",
    "    chamber_1 = DF.filter((col(\"FPGA\") == 0) & (col(\"TDC_CHANNEL\")>=0) & (col(\"TDC_CHANNEL\")<=63))\n",
    "    chamber_2 = DF.filter((col(\"FPGA\") == 0) & (col(\"TDC_CHANNEL\")>=64) & (col(\"TDC_CHANNEL\")<=127))\n",
    "    chamber_3 = DF.filter((col(\"FPGA\") == 1) & (col(\"TDC_CHANNEL\")>=0) & (col(\"TDC_CHANNEL\")<=63))\n",
    "    chamber_4 = DF.filter((col(\"FPGA\") == 1) & (col(\"TDC_CHANNEL\")>=64) & (col(\"TDC_CHANNEL\")<=127))\n",
    "\n",
    "    #Now we can count the number of events in each chamber\n",
    "    n_c1 = chamber_1.count()\n",
    "    n_c2 = chamber_2.count()\n",
    "    n_c3 = chamber_3.count()\n",
    "    n_c4 = chamber_4.count()\n",
    "\n",
    "    #Total number of events\n",
    "    n = n_c1 + n_c2 + n_c3 + n_c4\n",
    "\n",
    "\n",
    "    #Histograms    \n",
    "    h_c1 = chamber_1.groupBy('TDC_CHANNEL').count().collect()\n",
    "    h_c2 = chamber_2.groupBy('TDC_CHANNEL').count().collect()\n",
    "    h_c3 = chamber_3.groupBy('TDC_CHANNEL').count().collect()\n",
    "    h_c4 = chamber_4.groupBy('TDC_CHANNEL').count().collect()\n",
    "\n",
    "    h_active_1 = chamber_1.groupBy('TDC_CHANNEL','ORBIT_CNT').count().collect()\n",
    "    h_active_2 = chamber_2.groupBy('TDC_CHANNEL','ORBIT_CNT').count().collect()\n",
    "    h_active_3 = chamber_3.groupBy('TDC_CHANNEL','ORBIT_CNT').count().collect()\n",
    "    h_active_4 = chamber_4.groupBy('TDC_CHANNEL','ORBIT_CNT').count().collect()\n",
    "    \n",
    "    \n",
    "    #Organise the results to send them to one topic as a dictionary\n",
    "    results = {'Total_events': n,\n",
    "              'Events_per_chamber': [n_c1,n_c2,n_c3,n_c4],\n",
    "              'Histogram_1': [h_c1, h_c2, h_c3, h_c4],\n",
    "              'Histogram_2': [h_active_1,h_active_2,h_active_3,h_active_4]}\n",
    "    \n",
    "    #publish the results in the \"results\" topic for further usage\n",
    "    producer.send(topic='results', value=json.dumps(results).encode('utf-8'))\n",
    "    producer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e5d3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computations_2(DF, epoch):\n",
    "    start=time.time()\n",
    "    #This function perform the whole operations on the received batch,\n",
    "    #\n",
    "\n",
    "    #As the 4 calculations that we have to perform are done foe each chamber we set 4 dataframes\n",
    "    chamber_1 = DF.filter((col(\"FPGA\") == 0) & (col(\"TDC_CHANNEL\")>=0) & (col(\"TDC_CHANNEL\")<=63))\n",
    "    chamber_2 = DF.filter((col(\"FPGA\") == 0) & (col(\"TDC_CHANNEL\")>=64) & (col(\"TDC_CHANNEL\")<=127))\n",
    "    chamber_3 = DF.filter((col(\"FPGA\") == 1) & (col(\"TDC_CHANNEL\")>=0) & (col(\"TDC_CHANNEL\")<=63))\n",
    "    chamber_4 = DF.filter((col(\"FPGA\") == 1) & (col(\"TDC_CHANNEL\")>=64) & (col(\"TDC_CHANNEL\")<=127))\n",
    "\n",
    "    #Initialize results dictionary\n",
    "    results = {}\n",
    "    results[\"Total Count\"] = {}\n",
    "    chamber_name = [\"Chamber_1\", \"Chamber_2\", \"Chamber_3\", \"Chamber_4\"]\n",
    "    for chamber in chamber_name:\n",
    "        results[chamber] = {}\n",
    "        results[chamber][\"Count\"] = {}\n",
    "        \n",
    "    #Now we can count the number of events in each chamber\n",
    "    results[\"Chamber_1\"][\"Count\"] = chamber_1.count()\n",
    "    results[\"Chamber_2\"][\"Count\"] = chamber_2.count()\n",
    "    results[\"Chamber_3\"][\"Count\"] = chamber_3.count()\n",
    "    results[\"Chamber_4\"][\"Count\"] = chamber_4.count()\n",
    "\n",
    "    #Total number of events\n",
    "    results[\"Total Count\"] = results[\"Chamber_1\"][\"Count\"] + results[\"Chamber_2\"][\"Count\"] + \\\n",
    "                             results[\"Chamber_3\"][\"Count\"] + results[\"Chamber_4\"][\"Count\"]\n",
    "    \n",
    "\n",
    "    # Compute histograms for each chamber   \n",
    "    i=0    \n",
    "    for chamber in [chamber_1, chamber_2, chamber_3, chamber_4]:\n",
    "        #Initialize dictionary partitions to save the results\n",
    "        for hist in [\"Hist_1\",\"Hist_2\"]:\n",
    "            results[chamber_name[i]][hist] = {}\n",
    "            results[chamber_name[i]][hist][\"Bins\"] = {}\n",
    "            results[chamber_name[i]][hist][\"Counts\"] = {}\n",
    "        \n",
    "        if(chamber.count()!=0): \n",
    "            #Histogram 1\n",
    "            bins, counts = (\n",
    "                chamber.select(\"TDC_CHANNEL\")\n",
    "                .rdd.map(lambda x: x.TDC_CHANNEL)\n",
    "                .histogram(list(np.arange(0,170,5)))\n",
    "            )\n",
    "            \n",
    "            results[chamber_name[i]][\"Hist_1\"][\"Bins\"] = bins\n",
    "            results[chamber_name[i]][\"Hist_1\"][\"Counts\"] = counts\n",
    "            \n",
    "            #Histogram 2\n",
    "            bins, counts = (\n",
    "            chamber.groupBy(\"TDC_CHANNEL\",\"ORBIT_CNT\")\n",
    "            .count()\n",
    "            .select(\"ORBIT_CNT\")\n",
    "            .rdd.map(lambda x: x.ORBIT_CNT)\n",
    "            .histogram(list(np.arange(6.e5,1.e7,0.5e6)))\n",
    "            )\n",
    "            \n",
    "            results[chamber_name[i]][\"Hist_2\"][\"Bins\"] = bins\n",
    "            results[chamber_name[i]][\"Hist_2\"][\"Counts\"] = counts            \n",
    "            \n",
    "        else:\n",
    "            #Histogram 1\n",
    "            results[chamber_name[i]][\"Hist_1\"][\"Bins\"] = list(np.arange(0,170,5))\n",
    "            counts = list(np.arange(0,170,5)* 0) \n",
    "            results[chamber_name[i]][\"Hist_1\"][\"Counts\"] = counts\n",
    "            \n",
    "            #Histogram 2\n",
    "            results[chamber_name[i]][\"Hist_2\"][\"Bins\"] = list(np.arange(6.e5,1.e7,0.5e6))\n",
    "            counts = list(np.arange(6.e5,1.e7,0.5e6)* 0) \n",
    "            results[chamber_name[i]][\"Hist_2\"][\"Counts\"] = counts\n",
    "        i +=1\n",
    "    \n",
    "    end =time.time()\n",
    "    print(\"Time =\",end-start)\n",
    "    producer.send(topic=\"results\", value= str(results).encode('utf-8'))\n",
    "    producer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162d6953",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computations_3(DF, epoch):\n",
    "    start=time.time()\n",
    "    #This function perform the whole operations on the received batch,\n",
    "    \n",
    "    #Add a column with the chamber number\n",
    "    DF_new = DF.withColumn('chamber',when((col(\"FPGA\") == 0) & (col(\"TDC_CHANNEL\")>=0) & (col(\"TDC_CHANNEL\")<=63),1).\n",
    "                                 when((col(\"FPGA\") == 0) & (col(\"TDC_CHANNEL\")>=64) & (col(\"TDC_CHANNEL\")<=127),2).\n",
    "                                 when((col(\"FPGA\") == 1) & (col(\"TDC_CHANNEL\")>=0) & (col(\"TDC_CHANNEL\")<=63),3).\n",
    "                                 when((col(\"FPGA\") == 1) & (col(\"TDC_CHANNEL\")>=64) & (col(\"TDC_CHANNEL\")<=127),4)).\\\n",
    "                                 select([ col('TDC_CHANNEL'), col('ORBIT_CNT'),\n",
    "                                    col('BX_COUNTER'),col('TDC_MEAS'),\n",
    "                                    col('chamber')])\n",
    "    DF_new.persist()\n",
    "    #DF_new.show()\n",
    "    #Initialize results dictionary\n",
    "    results = {}\n",
    "    results[\"Total Count\"] = {}\n",
    "    chamber_name = [\"Chamber_1\", \"Chamber_2\", \"Chamber_3\", \"Chamber_4\"]\n",
    "    for chamber in chamber_name:\n",
    "        results[chamber] = {}\n",
    "        results[chamber][\"Count\"] = {}\n",
    "        for hist in [\"Hist_1\",\"Hist_2\"]:\n",
    "            results[chamber][hist] = {}\n",
    "            results[chamber][hist][\"Bins\"] = {}\n",
    "            results[chamber][hist][\"Counts\"] = {}\n",
    "        \n",
    "    # Compute histograms for each chamber   \n",
    "    for i in [1,2,3,4]:\n",
    "        #Now we can count the number of events in each chamber\n",
    "        chamber = DF_new.filter(col(\"chamber\") == i).persist()\n",
    "        results[f\"Chamber_{i}\"][\"Count\"] = chamber.count()\n",
    "        \n",
    "        if(results[f\"Chamber_{i}\"][\"Count\"]!=0):\n",
    "            \n",
    "            #Histogram 1\n",
    "            bins, counts = (\n",
    "            chamber.select(\"TDC_CHANNEL\")\n",
    "                 .rdd.map(lambda x: x.TDC_CHANNEL)\n",
    "                 .histogram(list(np.arange(0,170,5)))\n",
    "            )\n",
    "            \n",
    "            results[f\"Chamber_{i}\"][\"Hist_1\"][\"Bins\"] = bins\n",
    "            results[f\"Chamber_{i}\"][\"Hist_1\"][\"Counts\"] = counts\n",
    "            \n",
    "            #Histogram 2\n",
    "            bins, counts = (\n",
    "            chamber.groupBy(\"TDC_CHANNEL\",\"ORBIT_CNT\")\n",
    "            .count()\n",
    "            .select(\"ORBIT_CNT\")\n",
    "            .rdd.map(lambda x: x.ORBIT_CNT)\n",
    "            .histogram(list(np.arange(6.e5,1.e7,0.5e6)))\n",
    "            )\n",
    "            \n",
    "            results[f\"Chamber_{i}\"][\"Hist_2\"][\"Bins\"] = bins\n",
    "            results[f\"Chamber_{i}\"][\"Hist_2\"][\"Counts\"] = counts            \n",
    "                \n",
    "        else:\n",
    "            #Histogram 1\n",
    "            results[f\"Chamber_{i}\"][\"Hist_1\"][\"Bins\"] = list(np.arange(0,170,5))\n",
    "            counts = list(np.arange(0,170,5)* 0) \n",
    "            results[f\"Chamber_{i}\"][\"Hist_1\"][\"Counts\"] = counts\n",
    "            \n",
    "             #Histogram 2\n",
    "            results[f\"Chamber_{i}\"][\"Hist_2\"][\"Bins\"] = list(np.arange(6.e5,1.e7,0.5e6))\n",
    "            counts = list(np.arange(6.e5,1.e7,0.5e6)* 0) \n",
    "            results[f\"Chamber_{i}\"][\"Hist_2\"][\"Counts\"] = counts\n",
    "        chamber = DF_new.filter(col(\"chamber\") == i).unpersist()  \n",
    "        \n",
    "    results[\"Total Count\"] = results[\"Chamber_1\"][\"Count\"] + results[\"Chamber_2\"][\"Count\"] + \\\n",
    "                             results[\"Chamber_3\"][\"Count\"] + results[\"Chamber_4\"][\"Count\"]\n",
    "    end =time.time()\n",
    "    #print(\"Time =\",end-start)\n",
    "       \n",
    "    \n",
    "    producer.send(topic=\"results\", value= str(results).encode('utf-8'))\n",
    "    producer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94126a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer\n",
    "\n",
    "#Send the results to the kafka topic\n",
    "#Initialize the producer\n",
    "producer = KafkaProducer(bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253b9d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trigger the processing\n",
    "cleanDF.writeStream\\\n",
    "    .foreachBatch(computations_3)\\\n",
    "    .trigger(processingTime='6 second')\\\n",
    "    .start()\\\n",
    "    .awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fd73b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5484d6cf",
   "metadata": {},
   "source": [
    "If you also want to delete any data of your local Kafka environment including any events you have created along the way, run the command:\n",
    "\n",
    "`` $ rm -rf /tmp/kafka-logs /tmp/zookeeper `` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f22ff1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
