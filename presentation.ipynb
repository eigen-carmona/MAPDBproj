{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c93baca1",
   "metadata": {},
   "source": [
    "# MAPD-B distributed processing exam\n",
    "## Project 4: Streaming processing of cosmic rays using Drift Tubes detectors\n",
    "\n",
    "### Students:\n",
    "* [Hilario Capettini](https://github.com/hcapettini2) (2013031)\n",
    "* [Javier Gerardo Carmona](https://github.com/eigen-carmona/) (2005005)\n",
    "* [Saverio Monaco](https://github.com/SaverioMonaco/) (2012264)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7460d89",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "\n",
    "The goal of this project is to reproduce a real-time processing of data collected in a\n",
    "particle physics detector and publish the results in a dashboard for live monitoring.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7fa1f3",
   "metadata": {},
   "source": [
    "### The project:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72970ad3",
   "metadata": {},
   "source": [
    "Each record (row) can be associated to each new signal processed in the DAQ of the detector. The data has the following structure:\n",
    "\n",
    "\n",
    "<img src=\"./imgs/data_structure.png\" width=500/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f2c3a2",
   "metadata": {},
   "source": [
    "**Required information per batch:**\n",
    "\n",
    "+ Total number of processed hits\n",
    "+ Total number of processed hits per chamber\n",
    "+ TDC_CHANNEL histogram per chamber\n",
    "+ Histogram of active TDC_CHANNEL  in each orbit, per chamber\n",
    "\n",
    "**Extra points**\n",
    "+ TDC_CHANNEL histogram per chamber\n",
    "+ DRIFTIME histogram per chamber"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06e8b84",
   "metadata": {},
   "source": [
    "### The pipeline:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54897f2e",
   "metadata": {},
   "source": [
    "![cluster_setup](./imgs/cluster_setup-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c88f8bd",
   "metadata": {},
   "source": [
    "### The cluster setup:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b241c8cb",
   "metadata": {},
   "source": [
    "5 Cloud Veneto VMs were assigned for this project:\n",
    "* MAPD-B_Gr17-5 10.67.22.83\n",
    "* MAPD-B_Gr17-4 10.67.22.136\n",
    "* MAPD-B_Gr17-3 10.67.22.102\n",
    "* MAPD-B_Gr17-2 10.67.22.39\n",
    "* MAPD-B_Gr17-1 10.67.22.137\n",
    "\n",
    "Each machine runs CentOs and the Specs are:\n",
    "* RAM:   8GB\n",
    "* VCPUs: 4\n",
    "* Disk:  25GB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faee04f9",
   "metadata": {},
   "source": [
    "### Setting the Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fc3559",
   "metadata": {},
   "source": [
    "- We chose a \"master node\" VM. In this case, we establish the following pairing:\n",
    "\n",
    "10.67.22.83 master\\\n",
    "10.67.22.137 slave01\\\n",
    "10.67.22.39 slave02\\\n",
    "10.67.22.102 slave03\\\n",
    "10.67.22.136 slave04\n",
    "\n",
    "which was added verbatim to the ```/etc/hosts``` file for every VM.\n",
    "\n",
    "- Then, we generate a public key for the master VM. This is added to each vm authorized keys. No passphrase is created, so that master can access any of the VMs without the need for a password.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a09ec6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setting Kafka and Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea473a18",
   "metadata": {},
   "source": [
    "### Spark\n",
    "#### Installation\n",
    "- We begin by installing ```java-11-openjdk``` along with ```spark-3.1.2``` in every one of the cluster's virtual machines.\n",
    "- In the master spark-env.sh file, we set the SPARK_MASTER variable, and in the slaves file we specify every slave by means of their host aliases.\n",
    "\n",
    "#### Execution\n",
    "- ```\\$SPARK_HOME/sbin/start-master.sh``` on master machine\n",
    "- ```\\$SPARK_HOME/sbin/start-worker.sh spark://master:7077``` on each desired worker (possibly including master)\n",
    "\n",
    "### Kafka\n",
    "#### Installation\n",
    "- ```wget -c https://dlcdn.apache.org/kafka/2.8.0/kafka_2.13-2.8.0.tgz```\n",
    "- ```tar -xzf kafka_2.13-2.8.0.tgz```\n",
    "\n",
    "#### Execution\n",
    "Once in $KAFKA_HOME, each of the following is executed on a different terminal\n",
    "- ```./bin/zookeeper-server-start.sh config/zookeeper.properties```\n",
    "- ```./bin/kafka-server-start.sh config/server.properties```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71359fb9",
   "metadata": {},
   "source": [
    "## Our approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec061a4",
   "metadata": {},
   "source": [
    "![main_diagram](./imgs/main_diagram-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a0f07d",
   "metadata": {},
   "source": [
    "With the spark master and workers are running, we can create a spark session on python using:\n",
    "```python\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"spark://master:7077\")\\\n",
    "    .appName(\"Spark Streaming\")\\\n",
    "    .config(\"spark.jars.packages\",\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.1\")\\\n",
    "    .config(\"spark.sql.shuffle.partitions\",8)\\\n",
    "    .getOrCreate()\n",
    "```\n",
    "\n",
    "By means of the ```kafka-python``` client, we create the topics that shall be used:\n",
    "```python\n",
    "kafka_admin = KafkaAdminClient(\n",
    "        bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,\n",
    "    )\n",
    "\n",
    "new_topic_a = NewTopic(name='Experiment_measurements', \n",
    "                       num_partitions=16, \n",
    "                       replication_factor=1)\n",
    "\n",
    "new_topic_b = NewTopic(name='results', \n",
    "                       num_partitions=1, \n",
    "                       replication_factor=1)\n",
    "\n",
    "kafka_admin.create_topics(new_topics=[new_topic_a,new_topic_b])\n",
    "```\n",
    "\n",
    "Once this is set, we start streaming the data for a spark dataframe:\n",
    "```pyhthon\n",
    "inputDF = spark\\\n",
    "    .readStream\\\n",
    "    .format(\"kafka\")\\\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP_SERVERS)\\\n",
    "    .option('subscribe', 'Experiment_measurements')\\\n",
    "    .load()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833626c3",
   "metadata": {},
   "source": [
    "### Producer\n",
    "We created a logger function to which a producer instance is fed, along with the csv source file. The approximate time between each log can be adjusted, as well as data synchronicity:\n",
    "```python\n",
    "def sender(file,producer,sleep_time=0.00015, synchronous = False):\n",
    "    path ='./Data/'\n",
    "    with open(path+file+'.csv') as file:\n",
    "        reader = csv.DictReader(file, delimiter=\",\")\n",
    "        for row in reader:\n",
    "            row[\"HEAD\"] = int(row[\"HEAD\"])\n",
    "            row[\"FPGA\"] = int(row[\"FPGA\"])\n",
    "            row[\"TDC_CHANNEL\"] = int(row[\"TDC_CHANNEL\"])\n",
    "            row[\"ORBIT_CNT\"] = float(row[\"ORBIT_CNT\"])\n",
    "            row[\"BX_COUNTER\"] = int(row[\"BX_COUNTER\"])\n",
    "            row[\"TDC_MEAS\"] = float(row[\"TDC_MEAS\"])\n",
    "            producer.send(topic='Experiment_measurements', value=row, )\n",
    "            if synchronous: producer.flush()\n",
    "            time.sleep(sleep_time)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e76e00",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14718c4",
   "metadata": {},
   "source": [
    "### Pre processing:\n",
    "For robustness, we specify the schema over the streamed dataframe, and make the respective transformations:\n",
    "```python\n",
    "schema = StructType(\n",
    "        [\n",
    "                StructField(\"HEAD\",        IntegerType()),\n",
    "                StructField(\"FPGA\",        IntegerType()),\n",
    "                StructField(\"TDC_CHANNEL\", IntegerType()),\n",
    "                StructField(\"ORBIT_CNT\",   DoubleType()),\n",
    "                StructField(\"BX_COUNTER\",  IntegerType()),\n",
    "                StructField(\"TDC_MEAS\",    DoubleType())\n",
    "        ]\n",
    ")\n",
    "\n",
    "jsonDF = inputDF.select(from_json(col(\"value\").alias('value').cast(\"string\"), schema).alias('value'))\n",
    "\n",
    "flatDF = jsonDF.selectExpr(\"value.HEAD\", \n",
    "                           \"value.FPGA\", \n",
    "                           \"value.TDC_CHANNEL\",\n",
    "                           \"value.ORBIT_CNT\",\n",
    "                           \"value.BX_COUNTER\",\n",
    "                           \"value.TDC_MEAS\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919a9558",
   "metadata": {},
   "source": [
    "### Processing\n",
    "\n",
    "#### Cleaning and extending the data\n",
    "We first filter the data-frame. For the default project part, only those rows with df.HEAD==2 and df.TDC_CHANNEL < 128 are relevant. Entries with df.TDC_CHANNEL ==128 are also considered for the extra part:\n",
    "```python\n",
    "cleanDF = flatDF.where((col('HEAD')==2) & (col('TDC_CHANNEL') <= 128))\n",
    "```\n",
    "\n",
    "We also add a chamber column to the dataframe:\n",
    "```python\n",
    "new_df = df.withColumn('CHAMBER',when(col(\"FPGA\") == 0,\n",
    "                                     when(col(\"TDC_CHANNEL\")<=63,1).\\\n",
    "                                     otherwise(when(col(\"TDC_CHANNEL\")<128,2))).\\\n",
    "                                otherwise(when(col(\"TDC_CHANNEL\")<=63,3).\\\n",
    "                                          otherwise(when(col(\"TDC_CHANNEL\")<128,4))\n",
    "                                )).\\\n",
    "                                select([ col('TDC_CHANNEL'), col('ORBIT_CNT'),\n",
    "                                col('BX_COUNTER'),col('TDC_MEAS'),\n",
    "                                col('CHAMBER')])\n",
    "```\n",
    "\n",
    "#### Histograms\n",
    "We avoid the ```rdd.histogram``` method, as working with spark dataframes is generally faster. We stick to an approach which allows for higher task parallelization:\n",
    "\n",
    "```python\n",
    "def histogram_a(df,min_v,max_v,inc,key):\n",
    "    '''This function return the bins and counts for the first type of requested histogram'''\n",
    "    hist_bins = np.arange(min_v,max_v,inc)\n",
    "    hist = df\\\n",
    "        .filter((min_v<=F.col(key)) & (F.col(key)<=max_v))\\\n",
    "        .withColumn('BIN', F.floor((F.col(key)-min_v)/inc))\\\n",
    "        .groupBy('CHAMBER','BIN')\\\n",
    "        .count().select('CHAMBER','BIN', col('count').alias('COUNT'))\n",
    "    return (hist_bins, hist)\n",
    "```\n",
    "Assuming all the histogram bins to be evenly spaced in order to use spark dataframe functions to calculate the bin to which the output count belongs.\n",
    "\n",
    "A second, more tricky histogram structure was achieved by means of the following:\n",
    "```python\n",
    "def histogram_b(df,min_v,max_v,inc,key_1,key_2):\n",
    "    '''This function return the bins and counts for the second type of requested histogram'''\n",
    "    hist_bins = np.arange(min_v,max_v,inc)\n",
    "    hist = df\\\n",
    "        .groupBy('CHAMBER',key_1)\\\n",
    "        .agg(F.countDistinct(key_2).alias('ACTIVE'))\\\n",
    "        .filter((min_v<=F.col(key_1))&(F.col(key_1)<=max_v))\\\n",
    "        .withColumn('BIN',F.floor((F.col(key_1)-min_v)/inc))\\\n",
    "        .groupBy('CHAMBER','BIN')\\\n",
    "        .agg(F.sum('ACTIVE').alias('COUNT'))\n",
    "    return(hist_bins, hist)\n",
    "```\n",
    "in which an aggregate function is executed in order to add the distinct counts of key_2 for each bin of key_1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbc49cb",
   "metadata": {},
   "source": [
    "```python\n",
    "def scintillator_data(df):\n",
    "    '''Define a dataframe containing the relevant information for \n",
    "    the scintillator analysis''' \n",
    "    \n",
    "    #First we filter the events encoding the passage time,\n",
    "    #then we add the PASSAGE time for each event \n",
    "    #Finally if we have two scilantor hits within the same orbit we keep \n",
    "    #the one with the smaller time\n",
    "    return(df.filter((col(\"CHAMBER\").isNull()) & (col(\"FPGA\") == 1)).\\\n",
    "                          withColumn(\"PASSAGETIME\", 25 * (col(\"ORBIT_CNT\") * 3564 + col(\"BX_COUNTER\") + col(\"TDC_MEAS\")/30)).\\\n",
    "                          drop(\"TDC_CHANNEL\").drop(\"BX_COUNTER\").\\\n",
    "                          drop(\"TDC_MEAS\").drop(\"CHAMBER\").\\\n",
    "                          groupBy(\"ORBIT_CNT\").min(\"PASSAGETIME\").\\\n",
    "                          withColumnRenamed(\"ORBIT_CNT\",\"ORBIT_CNT_sci\").\\\n",
    "                          withColumnRenamed(\"min(PASSAGETIME)\",\"PASSAGETIME\")\n",
    "          )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4089dfec",
   "metadata": {},
   "source": [
    "```python\n",
    "def computations(df, epoch, log):\n",
    "    '''This is the main function of the code, it requires a dataframe as input. The dataframe is analysed\n",
    "       and the results are published in the kafka topic \"results\" '''\n",
    "    main_df = chamber_assignment(df)\n",
    "\n",
    "    scintillator_df = scintillator_data(main_df)\n",
    "    \n",
    "    ### Drop the columns with null values from main_df\n",
    "    hit_df = main_df.na.drop(subset=[\"CHAMBER\"])\n",
    "    \n",
    "    ## TOTAL NUMBER OF PROCESSED HITS\n",
    "    total_hits = hit_df.count()\n",
    "    if not total_hits: return\n",
    "\n",
    "    ## TOTAL NUMBER OF PROCESSED HITS PER CHAMBER\n",
    "    chamber_hits = hit_df\\\n",
    "        .groupBy('CHAMBER').count()\\\n",
    "        .select(col('CHAMBER'),col('count').alias('COUNT'))\n",
    "    \n",
    "    ## ACTIVE TDC_CHANNEL PER CHAMBER\n",
    "    min_v_1 = 0\n",
    "    max_v_1 = 127\n",
    "    inc_1 = 5\n",
    "    hist_1_bins, hist_1 = histogram_a(hit_df,min_v_1,max_v_1,inc_1, 'TDC_CHANNEL')\n",
    "    \n",
    "    ## ACTIVE TDC_CHANNEL PER CHAMBER PER ORBIT_CNT\n",
    "    min_v_2 = 6.e5 #main_df.agg(F.min(F.col('ORBIT_CNT')).alias('min')).collect()[-1].min\n",
    "    max_v_2 = 1.e7 #main_df.agg(F.max(F.col('ORBIT_CNT')).alias('max')).collect()[-1].max\n",
    "    inc_2 = 0.5e6\n",
    "    hist_2_bins, hist_2 = histogram_b(hit_df,min_v_2,max_v_2,inc_2, 'ORBIT_CNT', 'TDC_CHANNEL')\n",
    "    \n",
    "    \n",
    "    ### keep only the hits with a scintillator signal within the same orbit\n",
    "    chamber_sci = hit_df.join(scintillator_df,main_df.ORBIT_CNT ==  scintillator_df.ORBIT_CNT_sci,\"inner\")\n",
    "\n",
    "    ## ADD TIME CORRECTION BY CHAMBER\n",
    "    chamber_sci = chamber_sci.withColumn('TIME_OFFSET',when(col(\"CHAMBER\") == 1, 93.9).\\\n",
    "                                                       when(col(\"CHAMBER\") == 2, 101.4).\\\n",
    "                                                       when(col(\"CHAMBER\") == 3, 95.5).\\\n",
    "                                                       when(col(\"CHAMBER\") == 4, 92.4))\n",
    "\n",
    "    ### Add the ABSSOLUTETIME and DRIFTIME\n",
    "    chamber_sci = chamber_sci.withColumn(\"ABSOLUTETIME\",\n",
    "                             25 * (col(\"ORBIT_CNT\") * 3564 + col(\"BX_COUNTER\") + col(\"TDC_MEAS\")/30)).\\\n",
    "                              withColumn(\"DRIFTIME\",col(\"ABSOLUTETIME\")-col(\"PASSAGETIME\") + col(\"TIME_OFFSET\"))\n",
    "   \n",
    "\n",
    "    ## ACTIVE TDC_CHANNEL PER CHAMBER WITHIN SCINTILLATOR SIGNAL\n",
    "    min_v_3 = 0\n",
    "    max_v_3= 127\n",
    "    inc_3 = 5\n",
    "    hist_3_bins, hist_3 = histogram_a(chamber_sci,min_v_3,max_v_3,inc_3, 'TDC_CHANNEL')\n",
    "    \n",
    "\n",
    "    ## HISTOGRAM OF DRIFTIME, PER CHAMBER\n",
    "    min_v_4 = 0\n",
    "    max_v_4= 1000\n",
    "    inc_4 = 10\n",
    "    hist_4_bins, hist_4 = histogram_a(chamber_sci,min_v_4,max_v_4,inc_4, 'DRIFTIME')\n",
    "\n",
    "\n",
    "    # PREPARE THE RESULTS\n",
    "    _chamber_hits = {row.CHAMBER: int(row.COUNT) for row in chamber_hits.collect()}\n",
    "    _hist_1_dict = prepare_results(hist_1,hist_1_bins)\n",
    "    _hist_2_dict = prepare_results(hist_2,hist_2_bins)\n",
    "    _hist_3_dict = prepare_results(hist_3,hist_3_bins)\n",
    "    _hist_4_dict = prepare_results(hist_4,hist_4_bins)\n",
    "    \n",
    "    default = lambda bins: {'Bins': list(bins), 'Counts' : [0]*(len(bins)-1)}\n",
    "    \n",
    "    results = {f'Chamber_{i}': {\n",
    "        'Count': _chamber_hits.get(i, 0),\n",
    "        'Hist_1': _hist_1_dict.get(i, default(hist_1_bins)),\n",
    "        'Hist_2': _hist_2_dict.get(i, default(hist_2_bins)),\n",
    "        'Hist_3': _hist_3_dict.get(i, default(hist_3_bins)),\n",
    "        'Hist_4': _hist_4_dict.get(i, default(hist_4_bins))} for i in range(1,5)}\n",
    "\n",
    "    results.update({\n",
    "        'Index': time.time(),\n",
    "        'Total Count': int(total_hits)\n",
    "    })\n",
    "\n",
    "    log(results)\n",
    "    return\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0901dc",
   "metadata": {},
   "source": [
    "## Process visualization:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2095a26f",
   "metadata": {},
   "source": [
    "### Consumer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4e9477",
   "metadata": {},
   "source": [
    "```python\n",
    "KAFKA_BOOTSTRAP_SERVERS = 'slave01:9092'\n",
    "consumer = KafkaConsumer(bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,\n",
    "                         consumer_timeout_ms=100000)\n",
    "\n",
    "consumer.subscribe('results')\n",
    "for message in consumer:\n",
    "    message = ast.literal_eval(message.value.decode(\"utf-8\"))\n",
    "    \n",
    "    print('Timestamp:                  ',message['Index'])\n",
    "    print('Total Counts of Events:     ',message['Total Count'])\n",
    "    \n",
    "    for chamber in [\"Chamber_1\", \"Chamber_2\", \"Chamber_3\", \"Chamber_4\"]:\n",
    "        print(' +',chamber,' Total Counts: ', message[chamber][\"Count\"])\n",
    "        \n",
    "    print('------------------')\n",
    "    \n",
    "    save_obj(message,'message') # Save current instance to a file for the Dashboard\n",
    "```\n",
    "<br><br>\n",
    "\n",
    "```\n",
    "Timestamp:                   1630939581.8853412\n",
    "Total Counts of Events:      4768\n",
    " + Chamber_1  Total Counts:  726\n",
    " + Chamber_2  Total Counts:  1652\n",
    " + Chamber_3  Total Counts:  1428\n",
    " + Chamber_4  Total Counts:  962\n",
    "------------------\n",
    "Timestamp:                   1630939587.3595812\n",
    "Total Counts of Events:      4495\n",
    " + Chamber_1  Total Counts:  717\n",
    " + Chamber_2  Total Counts:  1624\n",
    " + Chamber_3  Total Counts:  1139\n",
    " + Chamber_4  Total Counts:  1015\n",
    "------------------\n",
    "Timestamp:                   1630939593.0362525\n",
    "Total Counts of Events:      4113\n",
    " + Chamber_1  Total Counts:  754\n",
    " + Chamber_2  Total Counts:  1442\n",
    " + Chamber_3  Total Counts:  1003\n",
    " + Chamber_4  Total Counts:  914\n",
    "------------------\n",
    "Timestamp:                   1630939598.2794814\n",
    "Total Counts of Events:      4470\n",
    " + Chamber_1  Total Counts:  761\n",
    " + Chamber_2  Total Counts:  1656\n",
    " + Chamber_3  Total Counts:  1014\n",
    " + Chamber_4  Total Counts:  1039\n",
    "------------------\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c39f00c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Live Plotting:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6553c8ac",
   "metadata": {},
   "source": [
    "To create a live webpage dashboard we used [Plotly Dash](https://github.com/plotly/dash) a Python library built on top of Plotly to create Analytical Web Apps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244f5a71",
   "metadata": {},
   "source": [
    "The information reported in the Dashboard are the following:\n",
    "\n",
    "**PLOTS**\n",
    "1. total number of processed hits, post-clensing (PLOT AND TABLE)\n",
    "2. total number of processed hits, post-clensing, per chamber (TABLE)\n",
    "3. histogram of the counts of active TDC_CHANNEL, per chamber (HISTOGRAM 1)\n",
    "4. histogram of the total number of active TDC_CHANNEL in each ORBIT_CNT, per chamber (HISTOGRAM 2)\n",
    "\n",
    "**EXTRA**\n",
    "1. histogram of the counts of active TDC_CHANNEL, per chamber, ONLY for those orbits with at least one scintillator signal in it (EXTRA 1)\n",
    "2. histogram of the DRIFTIME, per chamber (EXTRA 2 AND EXTRA 2 (cumulative))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb33c3bd",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/dashboard.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56b50c6",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6a71b1",
   "metadata": {},
   "source": [
    "We tested various configurations analyzing the data from [spark://master:4040]()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb031c2",
   "metadata": {},
   "source": [
    "### Horizontal scalability:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528b36fb",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;***Whole cluster***\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;***3 workers*** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfc6d85",
   "metadata": {},
   "source": [
    "<tr>\n",
    "    <td> <img src=\"./imgs/horiz/horizwhole.jpg\"/> </td>\n",
    "    <td> <img src=\"./imgs/horiz/horiz3a.jpg\"/> </td>\n",
    "</tr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffc57a0",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;***3 workers***\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;***2 workers*** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322451d3",
   "metadata": {},
   "source": [
    "<tr>\n",
    "    <td> <img src=\"./imgs/horiz/horiz3b.jpg\"/> </td>\n",
    "    <td> <img src=\"./imgs/horiz/horiz2.jpg\"/> </td>\n",
    "</tr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4da35f",
   "metadata": {},
   "source": [
    "### Scalability with ammount of data:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0747a9df",
   "metadata": {},
   "source": [
    "***Processing Time:***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c097749a",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;***15 seconds***\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;***50 seconds*** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f48ce2b",
   "metadata": {},
   "source": [
    "<tr>\n",
    "    <td> <img src=\"./imgs/datascalability/15.jpg\"/> </td>\n",
    "    <td> <img src=\"./imgs/datascalability/50a.jpg\"/> </td>\n",
    "</tr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6229af17",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;***50 seconds***\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;***120 seconds*** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0dad14",
   "metadata": {},
   "source": [
    "<tr>\n",
    "    <td> <img src=\"./imgs/datascalability/50b.jpg\"/> </td>\n",
    "    <td> <img src=\"./imgs/datascalability/120.jpg\"/> </td>\n",
    "</tr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1889849d",
   "metadata": {},
   "source": [
    "## Backup Information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc14722",
   "metadata": {},
   "source": [
    "### Data-cleansing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c18843a",
   "metadata": {},
   "source": [
    "Data-cleansing : $$\\text{HEAD} == 2 $$\n",
    "Other entries provide ancillary information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1d1e65",
   "metadata": {},
   "source": [
    "### Chamber mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342a011a",
   "metadata": {},
   "source": [
    "• Chamber 0 → (FPGA = 0) AND (TDC_CHANNEL in [0-63])\\\n",
    "• Chamber 1 → (FPGA = 0) AND (TDC_CHANNEL in [64-127])\\\n",
    "• Chamber 2 → (FPGA = 1) AND (TDC_CHANNEL in [0-63])\\\n",
    "• Chamber 3 → (FPGA = 1) AND (TDC_CHANNEL in [64-127])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae31d75",
   "metadata": {},
   "source": [
    "### Driftime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1d91bc",
   "metadata": {},
   "source": [
    "#### Absolute time\n",
    "For each hit we can associate an absolute time:\n",
    "\n",
    "$$t_{TDC\\space hit} = 25 ∗ ( ORBIT\\_CNT ∗ 3564 + BX\\_COUNTER + TDC\\_MEAS /30)\\quad [ns]$$\n",
    "\n",
    "\n",
    "#### Passage of a muon time\n",
    "The passage time of any muon is provided by an external scintillator signal which correspond to the following selection:\n",
    "\n",
    "$$\\text{(FPGA == 1) AND (TDC_CHANNEL == 128)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44e2c85",
   "metadata": {},
   "source": [
    "#### Scintillator time offset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc32a5d",
   "metadata": {},
   "source": [
    "```python\n",
    "# scintillator time offset by Chamber\n",
    "time_offset_by_chamber = {\n",
    "0: 95.0 - 1.1, # Ch 0\n",
    "1: 95.0 + 6.4, # Ch 1\n",
    "2: 95.0 + 0.5, # Ch 2\n",
    "3: 95.0 - 2.6, # Ch 3\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60f54dd",
   "metadata": {},
   "source": [
    "#### Driftime\n",
    "For those hits with a scintillator signal within the same orbit, a DRIFTIME can be defined, corresponding to the ABSOLUTETIME difference between each hit and the scintillator (from the same orbit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792aeb16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "849df438",
   "metadata": {},
   "source": [
    "### Dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626a7fa0",
   "metadata": {},
   "source": [
    "#### Callbacks\n",
    "The dashboard file reads periodically from a file located in ```./board/message.pkl``` that contains the last instance produced by the topic_results (in consumer).\n",
    "\n",
    "Whenever a variable gets updated during the reading of the file, the appropriate update function for the figures gets called and updates them.\\\n",
    "The update functions come with a ```callback decorator```:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795071b6",
   "metadata": {},
   "source": [
    "Example of callback functions: \n",
    "\n",
    "```python\n",
    "@app.callback(Output('hist1-1','figure'),\n",
    "             [Input('graph-update', 'n_intervals')])\n",
    "def updateHist1(n):\n",
    "     return hist_getdata(1,1)\n",
    "    \n",
    "@app.callback(Output('hist1-2','figure'),\n",
    "             [Input('graph-update', 'n_intervals')])\n",
    "def updateHist2(n):\n",
    "     return hist_getdata(1,2)\n",
    "    \n",
    "@app.callback(Output('hist1-3','figure'),\n",
    "             [Input('graph-update', 'n_intervals')])\n",
    "def updateHist3(n):\n",
    "     return hist_getdata(1,3)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a726d0e5",
   "metadata": {},
   "source": [
    "Structure of the callbacks:\n",
    "<img src=\"./imgs/dashboard_callbacks.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fa87f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
